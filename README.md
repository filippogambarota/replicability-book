
<!-- README.md is generated from README.Rmd. Please edit that file -->

# Contents

## Chapter 1 - what is a replication

- theoretical introduction to the concept of replication
- papers presented in the summer school
- important references
- replicability crisis

#### References

- Machery, E. (2020). What is a replication? Philosophy of Science,
  87(4), 545–567.
  <https://www.cambridge.org/core/journals/philosophy-of-science/article/what-is-a-replication/F41B751ECC31462CBBD46711BC733AEE>
- Miller, J. (2009). What is the probability of replicating a
  statistically significant effect? Psychonomic Bulletin & Review,
  16(4), 617–640. <https://doi.org/10.3758/PBR.16.4.617>

## Chapter 2 - replicability and reproducibility

- replicability vs reproducibility
- reproducibility tools
- suggested references and tools

## Chapter 3 - formalizing and measuring replication

- the meta-analysis model for uniformity and notation
- problem of measuring replication
- simulation introduction

#### References

- Hedges, L. V., & Schauer, J. M. (2019). Statistical analyses for
  studying replication: Meta-analytic perspectives. Psychological
  Methods, 24(5), 557–570. <https://doi.org/10.1037/met0000189>
- Schauer, J. M. (2022). Replicability and Meta-Analysis. In W.
  O’Donohue, A. Masuda, & S. Lilienfeld (Eds.), Avoiding Questionable
  Research Practices in Applied Psychology (pp. 301–342). Springer
  International Publishing.
  <https://doi.org/10.1007/978-3-031-04968-2_14>

## Chapter 4 - frequentists and bayesian methods

- review of the methods with the R application
- pro and cons of method with the power consideration of Hedges
- r packages with all the function for the replications
- review of existing R packages

#### References

- Camerer, C. F., Dreber, A., Holzmeister, F., Ho, T.-H., Huber, J.,
  Johannesson, M., Kirchler, M., Nave, G., Nosek, B. A., Pfeiffer, T.,
  Altmejd, A., Buttrick, N., Chan, T., Chen, Y., Forsell, E., Gampa, A.,
  Heikensten, E., Hummer, L., Imai, T., … Wu, H. (2018). Evaluating the
  replicability of social science experiments in Nature and Science
  between 2010 and 2015. Nature Human Behaviour, 2(9), 637–644.
  <https://doi.org/10.1038/s41562-018-0399-z>
- Errington, T. M., Mathur, M., Soderberg, C. K., Denis, A., Perfito,
  N., Iorns, E., & Nosek, B. A. (2021). Investigating the replicability
  of preclinical cancer biology. eLife, 10.
  <https://doi.org/10.7554/eLife.71601>
- Hedges, L. V., & Schauer, J. M. (2019a). Consistency of effects is
  important in replication: Rejoinder to Mathur and VanderWeele (2019).
  Psychological Methods, 24(5), 576–577.
  <https://doi.org/10.1037/met0000237>
- Hedges, L. V., & Schauer, J. M. (2019b). More Than One Replication
  Study Is Needed for Unambiguous Tests of Replication. Journal of
  Educational and Behavioral Statistics: A Quarterly Publication
  Sponsored by the American Educational Research Association and the
  American Statistical Association, 44(5), 543–570.
  <https://doi.org/10.3102/1076998619852953>
- Ly, A., Etz, A., Marsman, M., & Wagenmakers, E.-J. (2019). Replication
  Bayes factors from evidence updating. Behavior Research Methods,
  51(6), 2498–2508. <https://doi.org/10.3758/s13428-018-1092-x>
- Mathur, M. B., & VanderWeele, T. J. (2019). Challenges and suggestions
  for defining replication “success” when effects may be heterogeneous:
  Comment on Hedges and Schauer (2019). Psychological Methods, 24(5),
  571–575. <https://doi.org/10.1037/met0000223>
- Mathur, M. B., & VanderWeele, T. J. (2020). New statistical metrics
  for multisite replication projects. Journal of the Royal Statistical
  Society. Series A, (Statistics in Society), 183(3), 1145–1166.
  <https://doi.org/10.1111/rssa.12572>
- Schauer, J. M., & Hedges, L. V. (2021). Reconsidering statistical
  methods for assessing replication. Psychological Methods, 26(1),
  127–139. <https://doi.org/10.1037/met0000302>
- Schauer, Jacob M. (2023). On the Accuracy of Replication Failure
  Rates. Multivariate Behavioral Research, 58(3), 598–615.
  <https://doi.org/10.1080/00273171.2022.2066500>
- Schauer, Jacob M., Fitzgerald, K. G., Peko-Spicer, S., Whalen, M. C.
  R., Zejnullahi, R., & Hedges, L. V. (2021). An evaluation of
  statistical methods for aggregate patterns of replication failure. The
  Annals of Applied Statistics, 15(1), 208–229.
  <https://doi.org/10.1214/20-AOAS1387>
- Schauer, Jacob M., & Hedges, L. V. (2020). Assessing heterogeneity and
  power in replications of psychological experiments. Psychological
  Bulletin, 146(8), 701–719. <https://doi.org/10.1037/bul0000232>
- Simonsohn, U. (2015). Small telescopes: detectability and the
  evaluation of replication results. Psychological Science, 26(5),
  559–569. <https://doi.org/10.1177/0956797614567341>
- Spence, J. R., & Stanley, D. J. (2016). Prediction interval: What to
  expect when you’re expecting … A replication. PloS One, 11(9),
  e0162874. <https://doi.org/10.1371/journal.pone.0162874>
- Valentine, J. C., Biglan, A., Boruch, R. F., Castro, F. G.,
  Collins, L. M., Flay, B. R., Kellam, S., Mościcki, E. K., &
  Schinke, S. P. (2011). Replication in prevention science. Prevention
  Science: The Official Journal of the Society for Prevention Research,
  12(2), 103–117. <https://doi.org/10.1007/s11121-011-0217-6>
- Nosek, B. A., Hardwicke, T. E., Moshontz, H., Allard, A., Corker, K.
  S., Dreber, A., Fidler, F., Hilgard, J., Kline Struhl, M., Nuijten, M.
  B., Rohrer, J. M., Romero, F., Scheel, A. M., Scherer, L. D.,
  Schönbrodt, F. D., & Vazire, S. (2022). Replicability, robustness, and
  reproducibility in psychological science. Annual Review of Psychology,
  73(1), 719–748. <https://doi.org/10.1146/annurev-psych-020821-114157>
- Fletcher, S. C. (2021). How (not) to measure replication. European
  Journal for Philosophy of Science, 11(2), 57.
  <https://doi.org/10.1007/s13194-021-00377-2>

## Chapter 5 - meta-analysis and publication bias

<!-- TODO check if putting this -->

- quick intro to meta-analysis (for the publication bias)
- publication bias as an important replication problem
- statistical method for the publication bias assessment and correction
- p curve and z curve

#### References

- Bartoš, F., & Schimmack, U. (2022). Z-curve 2.0: Estimating
  replication rates and discovery rates. Meta-Psychology.
  <https://conferences.lnu.se/index.php/metapsychology/article/view/2720>
- Simonsohn, U., Nelson, L. D., & Simmons, J. (2013). P-curve: A key to
  the file drawer. Journal of Experimental Psychology. General,
  143(2), 534. <https://doi.org/10.1037/a0033242>
- Simonsohn, Uri, Nelson, L. D., & Simmons, J. P. (2014). P-curve and
  effect size: Correcting for publication bias using only significant
  results: Correcting for publication bias using only significant
  results. Perspectives on Psychological Science: A Journal of the
  Association for Psychological Science, 9(6), 666–681.
  <https://doi.org/10.1177/1745691614553988>

## Chapter 6 - improving the studies

<!-- TODO check if putting this -->

- multiverse analysis, exploratory and inference
- vibration analysis
- specification curve
- pima

#### References

- Patel, C. J., Burford, B., & Ioannidis, J. P. A. (2015). Assessment of
  vibration of effects due to model specification can demonstrate the
  instability of observational associations. Journal of Clinical
  Epidemiology, 68(9), 1046–1058.
  <https://doi.org/10.1016/j.jclinepi.2015.05.029>
- Girardi, P., Vesely, A., Lakens, D., Altoè, G., Pastore, M., Calcagnì,
  A., & Finos, L. (2024). Post-selection Inference in Multiverse
  Analysis (PIMA): An Inferential Framework Based on the Sign Flipping
  Score Test. Psychometrika.
  <https://doi.org/10.1007/s11336-024-09973-6>
- Simonsohn, U., Simmons, J. P., & Nelson, L. D. (2020). Specification
  curve analysis. Nature Human Behaviour, 4(11), 1208–1214.
  <https://doi.org/10.1038/s41562-020-0912-z>

## Chapter 7 - planning replication studies

- hedges model and problems

#### References

- Hedges, L. V., & Schauer, J. M. (2021). The Design of Replication
  Studies. Journal of the Royal Statistical Society. Series A, , 184(3),
  868–886. <https://doi.org/10.1111/rssa.12688>

``` r
system("sh quarto-scripts/safe-todoget.sh")
todo <- readLines("todoget.md")
cat(todo, sep = "\n")
```

# Todoget Report

last update: 2024-12-19 (11:53)

## index.qmd

- Line 4: TODO expand this section a little bit
- Line 8: TODO describe book contents and chapters here
- Line 12: The book is written in [Quarto](https://quarto.org/). All
  materials and source code GitHub. The main language used in the
  statistical part of the book is [R](https://www.r-project.org/). In
  addition, we wrote several functions used in the examples collected
  into an R package TODO put R package name used through the book. You
  are free to fork, share and reuse contents. In addition, we consider
  the book as a shared resources thus if you have some suggestions,
  additions or any way to extend or improve the content you can submit a
  pull request on Github or contact the author (see the
  @sec-contributing section).
- Line 18: TODO study a little bit how the PR method could work

## chapters/chapter3.qmd

- Line 167: TODO: review this formula too lenghty
- Line 238: TODO: review and smooth this section
- Line 255: TODO see if the remainder of the section can be shortened,
  it seems a bit repetitive
- Line 347: TODO: review and smooth this section
- Line 408: The selection process implicit in publication decisions
  increases the likelihood of observing extreme and statistically
  significant results as the original studies. The replications studies
  (without assuming heterogeneity) have usually higher sample sizes
  bringing estimated effects toward the true (and generally lower) mean
  effect. TODO describe better the figure and the simulation The
  @fig-hes shows a simulation where extreme values are selected from a
  population of effects centered on zero for an original experiment with
  a small sample size. Then, using the same effects we simulated other
  experiments with increasing sample size as usually done in replication
  projects. Clearly the effects shrink toward the mean and in this case
  are no longer significant. @Perugini2014-bx proposed a method to
  adjust the estimation of the original effect size when estimating the
  probability of replication. TODO check perugini work .
- Line 623: TODO check if pawel is actually estimating the probability
  of individual replication
- Line 630: The actual model will be discussed in Chapter x. But the
  crucial point is that when formalizing a replication is important to
  take into account all the sources of information as well as applyning
  an appropriate weight according to degree of heterogeneity between the
  original study and the replication study. The model has been applied
  to the data from the Psychological replication project TODO ref here
  suggesting better performance when including heterogeneity and the
  shrinkage factor. We adapted the @Pawel2020-cm model creating a more
  general version that can be easily declined for simulating and
  analyzing data from replication studies. The model is presented in
  @fig-general-model.
- Line 646: TODO: describe standardized vs unstandardized
- Line 654: TODO: describe and
- Line 765: TODO expand more here from pawel and maybe Ulrich, R., &
  Miller, J. (2020). Questionable research practices may have little
  effect on replicability. eLife, 9, e58237.
  <https://doi.org/10.7554/eLife.58237>
- Line 767: TODO: model in stan
- Line 771: TODO small section about the slides deck 3 of the last
  summer school with the meta-analysis model based on the hierarchical
  model above

## chapters/chapter4.qmd

- Line 0: TODO: table with method (rows), (colums)properties, id exact
  or extension, bayesian or not, open source code, citation
- Line 11: TODO check the final table and the overall structure of the
  paper because it’s great also the references are amazing
- Line 32: TODO the idea here is also to keep track of the methods
  providing a clear overview
- Line 103: In light of the formalization presented in Chapter XXX and
  XXX we can formulate a general method to simulate data from the
  generative model ( TODO insert citation to figures lavagna e pavel ).
- Line 105: TODO introduce here the notation for effect size and
  sampling variance

## chapters/glossary.qmd

- Line 0: TODO put better this glossary maybe using the
  <https://debruine.github.io/quarto-glossary/> package

## chapters/changelog.qmd

- Line 0: TODO adapt a changelog strategy as lakens
  <https://lakens.github.io/statistical_inferences/changelog.html>

## notes/multilevel-model.md

- Line 0: A natural extension of the previous model where is by
  including another hierarchical layer where $\mu_{\theta}$ effects are
  sampled from another distribution. This distribution can be considered
  as the probability distribution of true effects across a research
  field. Also the heterogeneity values can be considered as sampled from
  a probability distribution. In the @fig- we presented the model
  assuming to have two effects in a given research area. Given a certain
  ROPE TODO check if rope is defined we have a proportion of true and
  false effects ($\omega$) for each different effect across a research
  field.

## extra/miller2009.html

- Line 241: // TODO: Check when it could be a function
  (<https://github.com/zenorocha/clipboard.js/issues/860>)
- Line 352: // TODO in 1.5, we should make sure this works without a
  callout special case
