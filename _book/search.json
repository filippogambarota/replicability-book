[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "The three Rs of trustworthy science",
    "section": "",
    "text": "Welcome\nThe aim of the book is to provide un updated (and continuously evolving) overview of the replication in science from a theoretical and statistical point of view. The book is not supposed to be exaustive but providing a general framework for learning and teaching the problem of replication in the era of the reproducibility and replicability crisis.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#contents",
    "href": "index.html#contents",
    "title": "The three Rs of trustworthy science",
    "section": "Contents",
    "text": "Contents",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#book-structure",
    "href": "index.html#book-structure",
    "title": "The three Rs of trustworthy science",
    "section": "Book structure",
    "text": "Book structure\nThe book is written in Quarto. All materials and source code GitHub. The main language used in the statistical part of the book is R. In addition, we wrote several functions used in the examples collected into an R package  used through the book. You are free to fork, share and reuse contents. In addition, we consider the book as a shared resources thus if you have some suggestions, additions or any way to extend or improve the content you can submit a pull request on Github or contact the author (see the Contributing section).",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#sec-contributing",
    "href": "index.html#sec-contributing",
    "title": "The three Rs of trustworthy science",
    "section": "Contributing",
    "text": "Contributing\nFor contributing submit a PR on Github or write an email to Filippo Gambarota (filippo.gambarota@unipd.it).",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "chapters/chapter1.html",
    "href": "chapters/chapter1.html",
    "title": "1  What is a replication",
    "section": "",
    "text": "Key Readings\nKey readings:\nOptional readings:",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>What is a replication</span>"
    ]
  },
  {
    "objectID": "chapters/chapter1.html#key-readings",
    "href": "chapters/chapter1.html#key-readings",
    "title": "1  What is a replication",
    "section": "",
    "text": "Machery (2020): What is a replication?\nNosek & Errington (2020): What is replication?\n\n\n\nRosenthal (1990): Replication in behavioral research. Especially for the concepts of precision and the idea of weighting replication studies according to qualitative and quantitative factors.\nNational Academies of Sciences, Engineering, and Medicine et al. (2019) National Academies report on Reproducibility and Replicability in Science",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>What is a replication</span>"
    ]
  },
  {
    "objectID": "chapters/chapter1.html#replication-robustness-and-reproducibility",
    "href": "chapters/chapter1.html#replication-robustness-and-reproducibility",
    "title": "1  What is a replication",
    "section": "1.1 Replication, Robustness and Reproducibility",
    "text": "1.1 Replication, Robustness and Reproducibility\nThis book is concerned with what we call the three “R”’s of trustworthy science: Replication, Robustness and Reproducibility. We begin by sketching a definition of each. This first Chapter then elaborates on the definition of replicability more specifically.\nIn 2019 the USA’s National Academies published a report on Reproducibility and Replicability in Science (National Academies of Sciences, Engineering, and Medicine et al., 2019), which we strongly recommend as a complementary reading to this document. They propose to define Replicability as “obtaining consistent results across studies aimed at answering the same scientific question, each of which has obtained its own data”. This will be our first “R”. The NAS definition is also endorsed by the American Statistical Association (Broman et al., 2017) and generally adopted in statistics (for example (Heller et al., 2014) add a couple of review papers).\nThe second “R” is Reproducibility, defined by NAS as “obtaining consistent results using the same input data, computational steps, methods, and code, and conditions of analysis” (National Academies of Sciences, Engineering, and Medicine et al., 2019). Here the experimental design, data and methodologies of analysis are all fixed. A reproducible research paper should provide enough information to obtain the results originally reported, starting from the same raw data. Raw data are data in a form as close as possible to what was generated by the original experimental source. Defining precisely what raw data is can be complex both conceptually and practically, but a digression on this would take us too far astray. Nowadays it is becoming more common to supplement a paper with analysis code, raw data (if possible) and details or materials (e.g., experimental stimuli, biological reagents, questionnaires) about the experimental setup. Reproducibility can be considered as the most fundamental pre-requisite of replication in science. Chapter XXX will provide a more detailed explanation of the concept with an overview of tools for doing reproducible science.\nOur third “R” also involves keeping the raw data fixed, and focuses on changing parts of the analysis method. This is different from the previous two, as it investigates the robustness of the results to the many judgment calls that typically need to be made in the implementation of a statistical analysis. Chapter XXX will present some methods for investigating robustness. For example, multiverse analysis (e.g., Steegen et al., 2016) is a way to systematically sample different plausible analytical strategies and see their impact on the final conclusions.\nUsage of the first two “R” terms is often inconsistent; and, it is reversed in computer science. (Barba, 2018; Goodman et al., 2016; Kenett & Shmueli, 2015) review these terminologies and their usage.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>What is a replication</span>"
    ]
  },
  {
    "objectID": "chapters/chapter1.html#defining-replication",
    "href": "chapters/chapter1.html#defining-replication",
    "title": "1  What is a replication",
    "section": "1.2 Defining Replication",
    "text": "1.2 Defining Replication\n\n1.2.1 Challenges\nWhen studying, planning or conducting studies about replication in science, it is essential to start from a clear definition and formalization of replication. First we need to define when an experiment or study constitutes a replication attempt of another.\nA gray area often arises where scientists attempt to replicate the results of a study using a similar but not identical scientific methodology. If they obtain a different result, should they conclude that the new study did not replicate the original? To answer, the replication needs to be evaluated not only in terms of its result; but, also with regard to the study design — for example, with regard to whether experimental factors, approaches and conditions are the same or different. In this first chapter we discuss the design of a replication study from both an empirical and a philosophical perspective.\nThe statistical analysis and the evaluation of whether replication was successful will be covered in Chapter XXX, where we will discover that there can be several different definitions of what a successful replication is. Anderson & Kelley (2024) reviewed more than 20 different replication criteria and Heyard et al. (2024) identified more than 50 published statistical approaches to assess a replication experiment.\nWe begin by reviewing concepts that in our view are general enough to be applied to multiple research areas and translated into an empirically testable statistical framework. The bibliography for this chapter contains a selected list of theoretical papers on replication.\n\n\n1.2.2 Cronbach’s essential components of an experiment\nWe start by defining the essential components of an experiment as proposed by Cronbach & Shapiro (1982)1 His framework is concisely referred to as UTOS because it consisted of these four components:\n\n\n\n\n\n\nUTOS Framework\n\n\n\n\nUnits: the elements of the population from which the study sample has been drawn; the term population is used here in its statistical sense: a real or hypothetical universe of units (not necessarily persons) to which the inferential conclusions of the study should apply;\nTreatments: the independent variables, for example experimental interventions, or population subgroups of interest;\nOutcomes: the dependent variables on which the study is focused\nSetting: the remainder of factors that can affect the experimental outcome and its relation to treatments. For example the factors defining the “environment” where a psychological experiment takes place (lab vs online experiment, cultural aspects, etc.)\n\n\n\nSome initial comments:\n\nU: this component positions us unequivocally in the territory of statistical inference, where we posit a population of reference consisting of distinct units, and we are interested in generalizing from an observed sample to the entire population. Naturally not all science aligns with this paradigm. Before reading further, try to think about important scientific studies you may have come across recently that do not fit with this paradigm.\nT & O: these components implicitly assume that the goal of the study was to elucidate how, in the population of interest, the outcomes relate to the independent variables, perhaps causally, if the experiment is controlled. Again, this is an important subset of science, but is by no means exhaustive. Clustering, discovery of latent dimensions in high dimensional data, segmentation of images, and a number of other exceptions come to mind. Add your own. Also, important science can be simply exploratory and descriptive, but replicability questions are relevant there as well.\nS: By changing settings “minimally” you get “essentially” the same experiments. For example you may change the temperature in the lab for an experiment where temperature is a negligible factor. On the other hand, if you change the temperature in a chemistry experiment where a certain reaction is very sensitive to temperature, you may get a completely different experiment and arguably a different study: skating on ice vs water, so to speak. So it takes a little sprinkle of common sense for this to be a useful definition. Be alert.\n\n\n\n1.2.3 Machery’s definition of a replication experiment\nMachery (2020) proposed a simple and flexible perspective on replication called the resampling account.\nConsider an original experiment in the UTOS framework, and imagine we can sharply specify fixed and random factors in the following sense: a random factor can be thought of as sampled from the population of interest, while a fixed factor is not sampled, but rather determined by design. For example, participants (Units) can be assumed to be random when the study samples a group of participants according to a well defined sampling scheme. In contrast, an experimental manipulation is generally fixed. Random sampling affords to option to use statistical inference to learn parameters of the population of interest (see also Yarkoni, 2020 for the idea of generalizability). Stricltly speaking, inference using a fixed factor is limited only to the tested condition. Defining fixed and random factors in real settings can be complex. Challenges and approaches are extensively debated in the literature (e.g., Clark & Linzer, 2015).\nReturning to replication, according to Machery, a replication experiment is an experiment created by sampling from the random factors (e.g., new group of participants), and keeping the non-random factors fixed (e.g., same type of drug or treatment). In this definition, population parameters of interest are identical between an original study and its replication. We will return on this point in Chapter XXX.\nA replication experiment is meant to assess what Machery calls reliability. Machery (2020) uses the term reliability as in measurement theory (Tal, 2020) where a reliable instrument (in this case the experiment is seen as the instrument) produces consistent 2 results across different replications of the measurement. When replicating a whole experiment, say sampling a new sample of participants, we are similarly interested in evaluating whether the experimental setup is reliable in the sense of producing consistent results.\nAs an illustration of the measurement metaphor, imagine a treatment for depression that has been evaluated in an original study. The researchers developed a protocol, defined the concept of depression, administered the treatment and chose some depression self-report measures, finding a decreasing level of depression compared to the control group. Another group of researchers decided to replicate this original experiment. They adopted the same protocol, definitions and measures but collected another sample of participants from the same population. The results of the replication experiment, for example the magnitude of the effect of the treatment on the depression level as defined by the protocol, may or may not be close to the original. If they are close, this provides evidence in favor of the reliability of the original experiment, as defined above (Nosek & Errington, 2017). Here, we are not questioning the definition or the measures used in the study.\nTo recap the progress made so far, if you can frame a study as UTOS, and clearly separate the random and fixed factors, you know how to design a replication study!",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>What is a replication</span>"
    ]
  },
  {
    "objectID": "chapters/chapter1.html#validity-and-extensions",
    "href": "chapters/chapter1.html#validity-and-extensions",
    "title": "1  What is a replication",
    "section": "1.3 Validity and Extensions",
    "text": "1.3 Validity and Extensions\n\n1.3.1 Conceptual Replication\nDoes is make sense to compare experiments when fixed factors have changed? In many cases it does, and this type of analysis still falls under the purview of replication, broadly construed. Readers familiar with the replication literature may have noticed that we did not use concepts such as direct or conceptual replication yet. Briefly, a direct replication is usually defined as an experiment that tries to recreate the original experiment as closely as possible. While this objective can be achieved in some fields, it is typically empirically challenging, due to uncontrollable factors (Nosek & Errington, 2017, 2020). Examples include replicating effects that are strongly culturally dependent, or experiments that used a completely outdated technology for the experimental setup.\nIn contrast, conceptual replication is a broad term defining a replication experiment with similar aims and methodology but with important differences (e.g., Crandall & Sherman, 2016). In Machery’s terms, a conceptual replication changes some of the fixed factors. In the depression example above, a conceptual replication might be a second experiment using another theoretical definition or measure of depression. Here, the goal is still within the broad umbrella of understanding the effect of the treatment on depression, but the experiment is not a strict replication in the Machery sense, because the outcome measurement methodology has changed. For this reason Machery (2020) uses the term extension. Extensions may include a methodological change but also a more profound philosophical change, for example using a scale motivated by a different theoretical framework for defining depression. An extension of an experiment is not assessing the reliability, but rather the validity of the original experiment and in a broader sense the theory underlying the original experiment. Similarly to reliability, the term validity is used with measurement theory in mind. The validity of a measurement involves correspondence with the true phenomenon being assessed (for example, the actual weight of an object being weighed).\nIn the depression example, if the researchers want to see whether the treatment is effective also using other measures of depression they are extending the original results; and, thereby, testing the validity of the result about the treatment.\n\n\n1.3.2 Precision of Replication\nIn practice we can often design a variety of replication / extension experiments, with varying degrees of similarity with the original one. Rosenthal (1990) defines the important concept of precision as the degree of similarity between the replication and the original experiment. The most precise is a Machery replication. Generally, very precise experiments lack external validity (they are not able to extend) but they can directly speak to the reliability of the original experimental setup and results. Less precise experiments will provide weaker evidence regarding the original experiment, shifting the focus to our confidence in the underlying theory. For example, we may say that the treatment also reduces physiological indexes of depression or is also effective in a different population of patients.\n\n\n1.3.3 Replication vs Extension: different scientific value?\nWe have seen that a precise replication experiment is an attempt to ask the same inferential question in as simlar a way as possible. Variations in setting and fixed factors can generate experiments that probe the trustworthiness of the conclusions further. These slight variations can be successfully analyzed together with replicability in mind, for example in a meta-analysis (see Chapter XXX), even though they do not constitute a strict replication by Machery’s exacting standards.\nAs settings are varied further and the replication becomes less precise, we eventually find ourselves having extended the experimental paradigm beyond the initial goals and populations. When is this line crossed? The answer to this question is dependent on the context; and, reasonable scientists may disagree about it. An important debate in the literature regards the value of a replication, as compared to an extension, of an experiment. Crandall & Sherman (2016) considers extensions as more valuable while Simons (2014) argues in favor of focusing on replications of experiments. Clearly both have a place in science. If the main aim is to test the reliability of the original experiment one should reduce the heterogeneity (i.e., increase the precision) as much as possible. On the other hand, if the aim is to explore different instances of the same effect or theory, one should extend the experimental setup.\nIn the remainder of the discussion we will use precise replication to denote a replication experiment in the strict sense of Machery, or a sufficiently close approximation, while we use the word extension to describe replication settings where the precision is lower. The appropriate definition of a successful replication, and the proper choice of statistical methodlogy for analyzing data may vary greatly, depending on where the follow-up study falls on this precision continuum.\n\n\n1.3.4 Contextual dependency\nContextual dependency (Gollwitzer & Schwabe, 2022; Inbar, 2016; Van Bavel et al., 2016) further develops the Machery (2020) framework by considering specific weights assigned to each UTOS element. Without going into details, the impact of the different UTOS elements on the actual experiment could differ according to the type of research question. For example, consider a social psychology experiment originally conducted on Western American participants and later extended to European or Asian participants. If the psychological construct of interest is strongly affected by cultural variations, the impact of changing the ethnicity may be considerable. In contrast, a lab-based experiment about low-level perceptual effects may be more portable across culturally diverse populations. Our expectations about the replication/extension outcome need to be calibrated with respect to the specific research question at hand, and the impact of each experimental element.\n\n\n1.3.5 Investigators as a Factor\nAs a final note, one important but often underestimated problem concerns the investigators conducting the replication study or studies. They can legitimately be considered part of the setting in some cases; but, they can be seen as random or irrelevant in others. Rosenthal (1990) clearly described the problem of correlated replicators and proposed different weighting approaches, based on the design of the replication study, for replication/extension studies conducted from authors that are orthogonal in terms of background theory and methods. Authors sharing a theory or a strict network of co-authorships are likely to create experiments and data that are more similar compared to independent authors. A replication or extension study conducted by an independent, and possibly skeptical, researcher should have a greater impact on increasing or decreasing confidence in the original findings, as compared to a follow-up study conducted by the original author.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>What is a replication</span>"
    ]
  },
  {
    "objectID": "chapters/chapter1.html#references",
    "href": "chapters/chapter1.html#references",
    "title": "1  What is a replication",
    "section": "References",
    "text": "References\n\n\n\n\nAnderson, S. F., & Kelley, K. (2024). Sample size planning for replication studies: The devil is in the design. Psychological Methods, 29, 844–867. https://doi.org/10.1037/met0000520\n\n\nBarba, L. A. (2018). Terminologies for reproducible research. arXiv [Cs.DL], arXiv:1802.03311. http://arxiv.org/abs/1802.03311\n\n\nBroman, K., Cetinkaya-Rundel, M., Nussbaum, A., Paciorek, C., Peng, R., Turek, D., & Wickham, H. (2017, January 18). Recommendations to funding agencies for supporting reproducible research. https://www.amstat.org/asa/files/pdfs/POL-ReproducibleResearchRecommendations.pdf\n\n\nClark, T. S., & Linzer, D. A. (2015). Should i use fixed or random effects? Political Science Research and Methods, 3, 399–408. https://doi.org/10.1017/psrm.2014.32\n\n\nCrandall, C. S., & Sherman, J. W. (2016). On the scientific superiority of conceptual replications for scientific progress. Journal of Experimental Social Psychology, 66, 93–99. https://doi.org/10.1016/j.jesp.2015.10.002\n\n\nCronbach, L. J., & Shapiro, K. (1982). Designing evaluations of educational and social programs. https://eduq.info/xmlui/handle/11515/9106\n\n\nGollwitzer, M., & Schwabe, J. (2022). Context dependency as a predictor of replicability. Review of General Psychology: Journal of Division 1, of the American Psychological Association, 26, 241–249. https://doi.org/10.1177/10892680211015635\n\n\nGoodman, S. N., Fanelli, D., & Ioannidis, J. P. A. (2016). What does research reproducibility mean? Science Translational Medicine, 8, 341ps12. https://doi.org/10.1126/scitranslmed.aaf5027\n\n\nHeller, R., Bogomolov, M., & Benjamini, Y. (2014). Deciding whether follow-up studies have replicated findings in a preliminary large-scale omics study. Proceedings of the National Academy of Sciences of the United States of America, 111, 16262–16267. https://doi.org/10.1073/pnas.1314814111\n\n\nHeyard, R., Pawel, S., Frese, J., Voelkl, B., Würbel, H., McCann, S., Held, L., Wever, K. E., Hartmann, H., Townsin, L., & Zellers, S. (2024). A scoping review on metrics to quantify reproducibility: A multitude of questions leads to a multitude of metrics. https://scholar.google.com/citations?view_op=view_citation&hl=en&citation_for_view=JAb7P1QAAAAJ:ldfaerwXgEUC\n\n\nInbar, Y. (2016). Association between contextual dependence and replicability in psychology may be spurious. Proceedings of the National Academy of Sciences of the United States of America, 113, E4933–4. https://doi.org/10.1073/pnas.1608676113\n\n\nKenett, R. S., & Shmueli, G. (2015). Clarifying the terminology that describes scientific reproducibility. Nature Methods, 12, 699. https://doi.org/10.1038/nmeth.3489\n\n\nMachery, E. (2020). What is a replication? Philosophy of Science, 87, 545–567. https://doi.org/10.1086/709701\n\n\nNational Academies of Sciences, Engineering, and Medicine, Policy and Global Affairs, Committee on Science, Engineering, Medicine, and Public Policy, Board on Research Data and Information, Division on Engineering and Physical Sciences, Committee on Applied and Theoretical Statistics, Board on Mathematical Sciences and Analytics, Division on Earth and Life Studies, Nuclear and Radiation Studies Board, & Division of Behavioral and Social Sciences and Education. (2019). Reproducibility and replicability in science. National Academies Press. https://doi.org/10.17226/25303\n\n\nNosek, B. A., & Errington, T. M. (2017). Making sense of replications. eLife, 6, e23383. https://doi.org/10.7554/eLife.23383\n\n\nNosek, B. A., & Errington, T. M. (2020). What is replication? PLoS Biology, 18, e3000691. https://doi.org/10.1371/journal.pbio.3000691\n\n\nParmigiani, G. (2023). Defining replicability of prediction rules. Statistical Science: A Review Journal of the Institute of Mathematical Statistics, 38. https://doi.org/10.1214/23-sts891\n\n\nRosenthal, R. (1990). Replication in behavioral research. Journal of Social Behavior and Personality. https://search.proquest.com/openview/5ccc3a267139968c18c0d7ce1a1c09f6/1?pq-origsite=gscholar&cbl=1819046\n\n\nSimons, D. J. (2014). The value of direct replication. Perspectives on Psychological Science: A Journal of the Association for Psychological Science, 9, 76–80. https://doi.org/10.1177/1745691613514755\n\n\nSteegen, S., Tuerlinckx, F., Gelman, A., & Vanpaemel, W. (2016). Increasing transparency through a multiverse analysis. Perspectives on Psychological Science: A Journal of the Association for Psychological Science, 11, 702–712. https://doi.org/10.1177/1745691616658637\n\n\nTal, E. (2020). Measurement in science. Metaphysics Research Lab, Stanford University. https://plato.stanford.edu/archives/fall2020/entries/measurement-science/\n\n\nVan Bavel, J. J., Mende-Siedlecki, P., Brady, W. J., & Reinero, D. A. (2016). Contextual sensitivity in scientific reproducibility. Proceedings of the National Academy of Sciences of the United States of America, 113, 6454–6459. https://doi.org/10.1073/pnas.1521897113\n\n\nYarkoni, T. (2020). The generalizability crisis. The Behavioral and Brain Sciences, 45, e1. https://doi.org/10.1017/S0140525X20001685",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>What is a replication</span>"
    ]
  },
  {
    "objectID": "chapters/chapter1.html#footnotes",
    "href": "chapters/chapter1.html#footnotes",
    "title": "1  What is a replication",
    "section": "",
    "text": "This framework has been proposed for social science and psychology but can be easily applied to different fields. For example see Parmigiani (2023) for a more detailed taxonomy applied to biomedical studies.↩︎\ne.g. close in some quantiative metric if they are real numbers↩︎",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>What is a replication</span>"
    ]
  },
  {
    "objectID": "chapters/chapter2.html",
    "href": "chapters/chapter2.html",
    "title": "\n2  Replicability and reproducibility\n",
    "section": "",
    "text": "3 Doing research is hard…",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Replicability and reproducibility</span>"
    ]
  },
  {
    "objectID": "chapters/chapter2.html#doing-research-is-hard-1",
    "href": "chapters/chapter2.html#doing-research-is-hard-1",
    "title": "\n2  Replicability and reproducibility\n",
    "section": "\n3.1 Doing research is hard…",
    "text": "3.1 Doing research is hard…\n. . .\n\nyou have to read papers, textbooks, slides and track information\n\n. . .\n\nyou have to plan your experiment or research\n\n. . .\n\nyou have to collect, organize and manage the research data\n\n\n. . .\n\nyou have to analyze data, create figures and tables\n\n\n. . .\n\nyou have to write reports, papers, slides, etc.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Replicability and reproducibility</span>"
    ]
  },
  {
    "objectID": "chapters/chapter2.html#doing-research-is-hard-2",
    "href": "chapters/chapter2.html#doing-research-is-hard-2",
    "title": "\n2  Replicability and reproducibility\n",
    "section": "\n3.2 Doing research is hard…",
    "text": "3.2 Doing research is hard…",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Replicability and reproducibility</span>"
    ]
  },
  {
    "objectID": "chapters/chapter2.html#doing-reproducible-research-is-even-harder",
    "href": "chapters/chapter2.html#doing-reproducible-research-is-even-harder",
    "title": "\n2  Replicability and reproducibility\n",
    "section": "\n3.3 Doing reproducible research is even harder… 😱",
    "text": "3.3 Doing reproducible research is even harder… 😱\n. . .\n\norganize data in a sharable and comprehensible format\n\n. . .\n\nchoose a future-proof place to share data along the research paper\n\n. . .\n\nanalyze data using a reproducible framework: code, programming language, scripting\n\n. . .\n\nreport data (papers, slides, etc.) using a reproducible framework",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Replicability and reproducibility</span>"
    ]
  },
  {
    "objectID": "chapters/chapter2.html#reproducibility-starter-pack-construction_worker",
    "href": "chapters/chapter2.html#reproducibility-starter-pack-construction_worker",
    "title": "\n2  Replicability and reproducibility\n",
    "section": "\n4.1 Reproducibility starter pack :construction_worker:",
    "text": "4.1 Reproducibility starter pack :construction_worker:\n. . .\n\n\n4.1.1 A general purpose (or flexible enough) programming language such as  or \n\n\n. . .\n\n\n4.1.2 A literate programming framework to integrate code and text\n\n. . .\n\n\n4.1.3 A version control system to track projects\n\n. . .\n\n\n4.1.4 An online repository to store the project and sharing with others",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Replicability and reproducibility</span>"
    ]
  },
  {
    "objectID": "chapters/chapter2.html#r",
    "href": "chapters/chapter2.html#r",
    "title": "\n2  Replicability and reproducibility\n",
    "section": "\n5.1 R",
    "text": "5.1 R\n\nR is a free software environment for statistical computing and graphics.\n\n\n(TBH) Is not a proper general purpose programming language (such as C++ or Python).\nNew extensions (packages) allow R to do pretty everything (file manager, image processing, webscraping, etc.)\nIt is free and open-source\nThe community is extremely active and keep growing",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Replicability and reproducibility</span>"
    ]
  },
  {
    "objectID": "chapters/chapter2.html#r---cran",
    "href": "chapters/chapter2.html#r---cran",
    "title": "\n2  Replicability and reproducibility\n",
    "section": "\n5.2 R - CRAN",
    "text": "5.2 R - CRAN\nThe CRAN is the repository where package developers upload their packages and other users can install them.\n\n\n\n\n\n\n. . .\nAs the saying goes: if something exist, there is for sure an R package for doing it! :smile:",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Replicability and reproducibility</span>"
    ]
  },
  {
    "objectID": "chapters/chapter2.html#r---pypl-index",
    "href": "chapters/chapter2.html#r---pypl-index",
    "title": "\n2  Replicability and reproducibility\n",
    "section": "\n5.3 R - PYPL Index",
    "text": "5.3 R - PYPL Index\n\n\nSource: https://pypl.github.io/PYPL.html\n\n\nRank\nLanguage\nShare\n1-year.trend\n\n\n\n1\nPython\n28.04%\n0.30%\n\n\n2\nJava\n15.78%\n-1.30%\n\n\n3\nJavaScript\n9.27%\n-0.20%\n\n\n4\nC#\n6.77%\n-0.20%\n\n\n5\nC/C++\n6.59%\n0.40%\n\n\n6\nPHP\n5.01%\n-0.40%\n\n\n7\nR\n4.35%\n0.00%\n\n\n8\nTypeScript\n3.09%\n0.30%\n\n\n9\nSwift\n2.54%\n0.50%\n\n\n10\nObjective-C\n2.15%\n0.10%\n\n\n11\nRust\n2.14%\n0.50%",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Replicability and reproducibility</span>"
    ]
  },
  {
    "objectID": "chapters/chapter2.html#r---pypl-index-1",
    "href": "chapters/chapter2.html#r---pypl-index-1",
    "title": "\n2  Replicability and reproducibility\n",
    "section": "\n5.4 R - PYPL Index",
    "text": "5.4 R - PYPL Index\nThe popularity is on a different scale compared to Python but still increasing:\n\n\nSource: https://pypl.github.io/PYPL.html",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Replicability and reproducibility</span>"
    ]
  },
  {
    "objectID": "chapters/chapter2.html#r-or-python",
    "href": "chapters/chapter2.html#r-or-python",
    "title": "\n2  Replicability and reproducibility\n",
    "section": "\n5.5 R or Python?",
    "text": "5.5 R or Python?\n\nPython is a good alternative. Personally, I use and enjoy python but I do most of my work in R.\nPython is a very general-purpose language more powerful for general tasks.\nI find python very useful for programming cognitive experiments, image processing, automatizing tasks and interacting with the operating system\nR is still a little bit superior in terms of data manipulation and visualization. Python is faster and more powerful for complex machine learning.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Replicability and reproducibility</span>"
    ]
  },
  {
    "objectID": "chapters/chapter2.html#modern-r",
    "href": "chapters/chapter2.html#modern-r",
    "title": "\n2  Replicability and reproducibility\n",
    "section": "\n5.6 Modern R",
    "text": "5.6 Modern R\n. . .\n\nFor purist programmers, R is somehow weird: arrays starts with 1, object-oriented programming is hidden, a lot of built-in vectorized functions, etc. See The R Inferno book for an overview.\n\n. . .\n\nDespite the weirdness, R have the majority of common programming paradigms (functions, conditionals, iterations, etc.).\n\n. . .\n\nThe scope of this week is not providing an introduction to R but I would put the focus on two topics for a modern usage of R:\n\nfunctional programming\nthe tidy approach",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Replicability and reproducibility</span>"
    ]
  },
  {
    "objectID": "chapters/chapter2.html#functional-programming",
    "href": "chapters/chapter2.html#functional-programming",
    "title": "\n2  Replicability and reproducibility\n",
    "section": "\n5.7 Functional Programming",
    "text": "5.7 Functional Programming\n\nIn computer science, functional programming is a programming paradigm where programs are constructed by applying and composing functions.\n\nDespite R can be used both with an imperative and object-oriented approach, the functional side is quite powerful.\nActually, functional programming is quite complex. Here we refer to breaking down our code into small functions. These functions can be function from packages, custom or anonymous functions.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Replicability and reproducibility</span>"
    ]
  },
  {
    "objectID": "chapters/chapter2.html#functional-programming-example",
    "href": "chapters/chapter2.html#functional-programming-example",
    "title": "\n2  Replicability and reproducibility\n",
    "section": "\n5.8 Functional Programming, example…",
    "text": "5.8 Functional Programming, example…\nWe have a dataset (mtcars) and we want to calculate the mean, median, standard deviation, minimum and maximum of each column and store the result in a table.\n\nhead(mtcars)\n\n                   mpg cyl disp  hp drat    wt  qsec vs am gear carb\nMazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\nValiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1\n\nstr(mtcars)\n\n'data.frame':   32 obs. of  11 variables:\n $ mpg : num  21 21 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 ...\n $ cyl : num  6 6 4 6 8 6 8 4 4 6 ...\n $ disp: num  160 160 108 258 360 ...\n $ hp  : num  110 110 93 110 175 105 245 62 95 123 ...\n $ drat: num  3.9 3.9 3.85 3.08 3.15 2.76 3.21 3.69 3.92 3.92 ...\n $ wt  : num  2.62 2.88 2.32 3.21 3.44 ...\n $ qsec: num  16.5 17 18.6 19.4 17 ...\n $ vs  : num  0 0 1 1 0 1 0 1 1 1 ...\n $ am  : num  1 1 1 0 0 0 0 0 0 0 ...\n $ gear: num  4 4 4 3 3 3 3 4 4 4 ...\n $ carb: num  4 4 1 1 2 1 4 2 2 4 ...",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Replicability and reproducibility</span>"
    ]
  },
  {
    "objectID": "chapters/chapter2.html#functional-programming-1",
    "href": "chapters/chapter2.html#functional-programming-1",
    "title": "\n2  Replicability and reproducibility\n",
    "section": "\n5.9 Functional Programming",
    "text": "5.9 Functional Programming\nThe standard (~imperative) option is using a for loop, iterating through columns, calculate the values and store into another data structure.\n\nncols &lt;- ncol(mtcars)\nmeans &lt;- medians &lt;- mins &lt;- maxs &lt;- rep(0, ncols)\n\nfor(i in 1:ncols){\n  means[i] &lt;- mean(mtcars[[i]])\n  medians[i] &lt;- mean(mtcars[[i]])\n  mins[i] &lt;- mean(mtcars[[i]])\n  maxs[i] &lt;- mean(mtcars[[i]])\n}\n\nresults &lt;- data.frame(means, medians, mins, maxs)\nresults$col &lt;- names(mtcars)\n\nresults\n\n        means    medians       mins       maxs  col\n1   20.090625  20.090625  20.090625  20.090625  mpg\n2    6.187500   6.187500   6.187500   6.187500  cyl\n3  230.721875 230.721875 230.721875 230.721875 disp\n4  146.687500 146.687500 146.687500 146.687500   hp\n5    3.596563   3.596563   3.596563   3.596563 drat\n6    3.217250   3.217250   3.217250   3.217250   wt\n7   17.848750  17.848750  17.848750  17.848750 qsec\n8    0.437500   0.437500   0.437500   0.437500   vs\n9    0.406250   0.406250   0.406250   0.406250   am\n10   3.687500   3.687500   3.687500   3.687500 gear\n11   2.812500   2.812500   2.812500   2.812500 carb",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Replicability and reproducibility</span>"
    ]
  },
  {
    "objectID": "chapters/chapter2.html#functional-programming-2",
    "href": "chapters/chapter2.html#functional-programming-2",
    "title": "\n2  Replicability and reproducibility\n",
    "section": "\n5.10 Functional Programming",
    "text": "5.10 Functional Programming\nWe can decompose (and symplify the problem) by writing a function and looping through columns.\n. . .\n\nsumm &lt;- function(x){\n  data.frame(means = mean(x), medians = median(x), mins = min(x), maxs = max(x))\n}\nncols &lt;- ncol(mtcars)\ndfs &lt;- vector(mode = \"list\", length = ncols)\n\nfor(i in 1:ncols){\n  dfs[[i]] &lt;- summ(mtcars[[i]])\n}\n\nresults &lt;- do.call(rbind, dfs)\nresults\n\n        means medians   mins    maxs\n1   20.090625  19.200 10.400  33.900\n2    6.187500   6.000  4.000   8.000\n3  230.721875 196.300 71.100 472.000\n4  146.687500 123.000 52.000 335.000\n5    3.596563   3.695  2.760   4.930\n6    3.217250   3.325  1.513   5.424\n7   17.848750  17.710 14.500  22.900\n8    0.437500   0.000  0.000   1.000\n9    0.406250   0.000  0.000   1.000\n10   3.687500   4.000  3.000   5.000\n11   2.812500   2.000  1.000   8.000",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Replicability and reproducibility</span>"
    ]
  },
  {
    "objectID": "chapters/chapter2.html#functional-programming-3",
    "href": "chapters/chapter2.html#functional-programming-3",
    "title": "\n2  Replicability and reproducibility\n",
    "section": "\n5.11 Functional Programming",
    "text": "5.11 Functional Programming\n. . .\nWe can be even more minimalistic by removing the for loop and using the *apply family that provide a series of compact iterative method.\n. . .\n\nresults &lt;- lapply(mtcars, summ)\nresults &lt;- do.call(rbind, results)\nresults\n\n          means medians   mins    maxs\nmpg   20.090625  19.200 10.400  33.900\ncyl    6.187500   6.000  4.000   8.000\ndisp 230.721875 196.300 71.100 472.000\nhp   146.687500 123.000 52.000 335.000\ndrat   3.596563   3.695  2.760   4.930\nwt     3.217250   3.325  1.513   5.424\nqsec  17.848750  17.710 14.500  22.900\nvs     0.437500   0.000  0.000   1.000\nam     0.406250   0.000  0.000   1.000\ngear   3.687500   4.000  3.000   5.000\ncarb   2.812500   2.000  1.000   8.000",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Replicability and reproducibility</span>"
    ]
  },
  {
    "objectID": "chapters/chapter2.html#functional-programming-apply",
    "href": "chapters/chapter2.html#functional-programming-apply",
    "title": "\n2  Replicability and reproducibility\n",
    "section": "\n5.12 Functional Programming, *apply\n",
    "text": "5.12 Functional Programming, *apply\n\n. . .\nThe *apply family is one of the best tool in R. The idea is pretty simple: apply a function to each element of a list.\n. . .\nThe powerful side is that in R everything can be considered as a list. A vector is a list of single elements, a dataframe is a list of columns etc.\n. . .\nInternally, R is still using a for loop but the verbose part (preallocation, choosing the iterator, indexing) is encapsulated into the *apply function.\n. . .\n\nmeans &lt;- rep(0, ncol(mtcars))\nfor(i in 1:length(means)){\n  means[i] &lt;- mean(mtcars[[i]])\n}\n\n# the same with sapply\nmeans &lt;- sapply(mtcars, mean)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Replicability and reproducibility</span>"
    ]
  },
  {
    "objectID": "chapters/chapter2.html#for-loops-are-bad",
    "href": "chapters/chapter2.html#for-loops-are-bad",
    "title": "\n2  Replicability and reproducibility\n",
    "section": "\n5.13 for loops are bad?",
    "text": "5.13 for loops are bad?\nfor loops are the core of each operation in R (and in every programming language). For complex operation thery are more readable and effective compared to *apply. In R we need extra care for writing efficent for loops.\nExtremely slow, no preallocation:\n\nres &lt;- c()\nfor(i in 1:1000){\n  # do something\n  res[i] &lt;- x\n}\n\nVery fast, no difference compared to *apply\n\nres &lt;- rep(0, 1000)\nfor(i in 1:length(res)){\n  # do something\n  res[i] &lt;- x\n}",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Replicability and reproducibility</span>"
    ]
  },
  {
    "objectID": "chapters/chapter2.html#why-functional-programming",
    "href": "chapters/chapter2.html#why-functional-programming",
    "title": "\n2  Replicability and reproducibility\n",
    "section": "\n5.14 Why functional programming?",
    "text": "5.14 Why functional programming?\n. . .\n\nWe can write less and reusable code that can be shared\n\n. . .\n\nThe scripts are more compact, less errors prone and more flexible (imagine that you want to improve the summ function, you only need to change it once instead of touching the for loop)\n\n. . .\n\nFunctions can be easily and consistently documented (see roxygen documentation) improving the reproducibility and clarity of the code",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Replicability and reproducibility</span>"
    ]
  },
  {
    "objectID": "chapters/chapter2.html#more-about-functional-programming-in-r",
    "href": "chapters/chapter2.html#more-about-functional-programming-in-r",
    "title": "\n2  Replicability and reproducibility\n",
    "section": "\n5.15 More about functional programming in R",
    "text": "5.15 More about functional programming in R\n\nAdvanced R by Hadley Wickham, section on Functional Programming (https://adv-r.hadley.nz/fp.html)\nHands-On Programming with R by Garrett Grolemund https://rstudio-education.github.io/hopr/\n\nHadley Wickham: The Joy of Functional Programming (for Data Science)\n\nBruno Rodrigues Youtube Channel",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Replicability and reproducibility</span>"
    ]
  },
  {
    "objectID": "chapters/chapter2.html#the-tidy-approach",
    "href": "chapters/chapter2.html#the-tidy-approach",
    "title": "\n2  Replicability and reproducibility\n",
    "section": "\n5.16 The Tidy approach",
    "text": "5.16 The Tidy approach\nThe tidyverse is a series of high-quality R packages to do modern data science:\n\ndata manipulation (dplyr, tidyr)\nplotting (ggplot2)\nreporting (rmarkdown)\nstring manipulation (stringr)\nfunctionals (purrr)\n…",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Replicability and reproducibility</span>"
    ]
  },
  {
    "objectID": "chapters/chapter2.html#the-tidy-approach---pipes",
    "href": "chapters/chapter2.html#the-tidy-approach---pipes",
    "title": "\n2  Replicability and reproducibility\n",
    "section": "\n5.17 The Tidy approach - Pipes",
    "text": "5.17 The Tidy approach - Pipes\nOne of the great improvement from the tidyverse is the usage of the pipe %&gt;% now introduced in base R as |&gt;. You will se these symbols a lot when looking at modern R code.\n. . .\nThe idea is very simple, the standard pattern to apply a function is function(argument). The pipe can reverse the pattern as argument |&gt; function(). Normally when we apply multiple functions progressively the pattern is this:\n. . .\n\nx &lt;- rnorm(100)\nx &lt;- round(x, 3)\nx &lt;- abs(x)\nx &lt;- as.character(x)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Replicability and reproducibility</span>"
    ]
  },
  {
    "objectID": "chapters/chapter2.html#the-tidy-approach---pipes-1",
    "href": "chapters/chapter2.html#the-tidy-approach---pipes-1",
    "title": "\n2  Replicability and reproducibility\n",
    "section": "\n5.18 The Tidy approach - Pipes",
    "text": "5.18 The Tidy approach - Pipes\nWhen using the pipe, we remove the redundand assignment &lt;- pattern:\n\nx &lt;- rnorm(100)\nx |&gt;\n  round(3) |&gt;\n  abs() |&gt;\n  as.character()\n\nThe pipe can be read as “from x apply round, then abs, etc.”. The first argument of the piped function is assumed to be the result of the previus call.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Replicability and reproducibility</span>"
    ]
  },
  {
    "objectID": "chapters/chapter2.html#more-about-the-tidy-approach",
    "href": "chapters/chapter2.html#more-about-the-tidy-approach",
    "title": "\n2  Replicability and reproducibility\n",
    "section": "\n5.19 More about the Tidy approach",
    "text": "5.19 More about the Tidy approach\nThe tidy approach contains tons of functions and packages. The overall philosopgy can be deepen in the R for Data Science book.\n\n\nhttps://r4ds.hadley.nz/",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Replicability and reproducibility</span>"
    ]
  },
  {
    "objectID": "chapters/chapter2.html#ggplot2",
    "href": "chapters/chapter2.html#ggplot2",
    "title": "\n2  Replicability and reproducibility\n",
    "section": "\n5.20 ggplot2",
    "text": "5.20 ggplot2\n\n\nCode\nResult\nBase R Code\nBase R Result\n\n\n\nOnly an honour mention to ggplot2 https://ggplot2-book.org/ (part of the tidyverse) that is an amazing package for data visualization following the piping and tidy approach. Is the implementation of the grammar of graphics idea.\n\nlibrary(tidyverse)\niris |&gt;\n  mutate(wi = runif(n())) |&gt;\n  ggplot(aes(x = Sepal.Length, y = Petal.Width, color = Species)) +\n  geom_point(aes(size = wi)) +\n  geom_smooth(method = \"lm\", se = FALSE)\n  guides(size = \"none\") +\n  theme_minimal(15)\n\n\n\n\n\n\n\n\n\n\n\n\n\nMore verbose, more hard coding, more steps and intermidiate objects.\n\niris_l &lt;- split(iris, iris$Species)\nlms &lt;- lapply(iris_l, function(x) lm(Petal.Width ~ Sepal.Length, data = x))\n\nplot(iris$Sepal.Length, iris$Petal.Width, col = as.numeric(iris$Species), pch = 19)\nabline(lms[[1]], col = 1, lwd = 2)\nabline(lms[[2]], col = 2, lwd = 2)\nabline(lms[[3]], col = 3, lwd = 2)\nlegend(\"topleft\", legend = levels(iris$Species), fill = 1:3)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Replicability and reproducibility</span>"
    ]
  },
  {
    "objectID": "chapters/chapter2.html#ggplot2-1",
    "href": "chapters/chapter2.html#ggplot2-1",
    "title": "\n2  Replicability and reproducibility\n",
    "section": "\n5.21 ggplot2",
    "text": "5.21 ggplot2\nThe ggplot2 book https://ggplot2-book.org/ is a great resource to produce high-quality, publication ready plots. Clearly, the advantage of producing the figures entirely writing code are immense in terms of reusability and reproducibility.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Replicability and reproducibility</span>"
    ]
  },
  {
    "objectID": "chapters/chapter2.html#literate-programming-1",
    "href": "chapters/chapter2.html#literate-programming-1",
    "title": "\n2  Replicability and reproducibility\n",
    "section": "\n6.1 Literate Programming1\n",
    "text": "6.1 Literate Programming1\n\n\nDonald Knuth first defined literate programming as a script, notebook, or computational document that contains an explanation of the program logic in a natural language, interspersed with snippets of macros and source code, which can be compiled and rerun\n\nFor example jupyter notebooks, R Markdown and now Quarto are literate programming frameworks to integrate code and text.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Replicability and reproducibility</span>"
    ]
  },
  {
    "objectID": "chapters/chapter2.html#literate-programming-the-markup-language",
    "href": "chapters/chapter2.html#literate-programming-the-markup-language",
    "title": "\n2  Replicability and reproducibility\n",
    "section": "\n6.2 Literate Programming, the markup language",
    "text": "6.2 Literate Programming, the markup language\nBeyond the coding part, the markup language is the core element of a literate programming framework. The idea of a markup language is separating the result from what you actually write. Some examples are:\n\nLaTeX\nHTML\nMarkdown\nXML\n…",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Replicability and reproducibility</span>"
    ]
  },
  {
    "objectID": "chapters/chapter2.html#latex",
    "href": "chapters/chapter2.html#latex",
    "title": "\n2  Replicability and reproducibility\n",
    "section": "\n6.3 LaTeX 2\n",
    "text": "6.3 LaTeX 2",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Replicability and reproducibility</span>"
    ]
  },
  {
    "objectID": "chapters/chapter2.html#html",
    "href": "chapters/chapter2.html#html",
    "title": "\n2  Replicability and reproducibility\n",
    "section": "\n6.4 HTML",
    "text": "6.4 HTML\n&lt;!DOCTYPE html&gt;\n&lt;html&gt;\n&lt;body&gt;\n\n&lt;h1&gt;My First Heading&lt;/h1&gt;\n\nLorem Ipsum è un testo segnaposto utilizzato nel settore della tipografia e della stampa. Lorem Ipsum è considerato il testo segnaposto standard sin dal sedicesimo secolo, quando un anonimo tipografo prese una cassetta di caratteri e li assemblò per preparare un testo campione. È sopravvissuto non solo a più di cinque secoli, ma anche al passaggio alla videoimpaginazione, pervenendoci sostanzialmente inalterato. Fu reso popolare, negli anni ’60, con la diffusione dei fogli di caratteri trasferibili “Letraset”, che contenevano passaggi del Lorem Ipsum, e più recentemente da software di impaginazione come Aldus PageMaker, che includeva versioni del Lorem Ipsum.\n\n&lt;h2&gt;My Second Heading&lt;/h2&gt;\n\nLorem Ipsum è un testo segnaposto utilizzato nel settore della tipografia e della stampa. \n\nLorem Ipsum è considerato il testo segnaposto standard sin dal sedicesimo secolo, quando un anonimo \n\ntipografo prese una cassetta di caratteri e li assemblò per preparare un testo campione. \n\nÈ sopravvissuto non solo a più di cinque secoli, ma anche al passaggio alla videoimpaginazione, pervenendoci sostanzialmente inalterato. \n\nFu reso popolare, negli anni ’60, con la diffusione dei \n\nfogli di caratteri trasferibili “Letraset”, che contenevano passaggi del Lorem Ipsum\n\npiù recentemente da software di impaginazione come Aldus PageMaker, che includeva versioni del Lorem Ipsum.\n\n&lt;/body&gt;\n&lt;/html&gt;",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Replicability and reproducibility</span>"
    ]
  },
  {
    "objectID": "chapters/chapter2.html#markdown",
    "href": "chapters/chapter2.html#markdown",
    "title": "\n2  Replicability and reproducibility\n",
    "section": "\n6.5 Markdown3\n",
    "text": "6.5 Markdown3",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Replicability and reproducibility</span>"
    ]
  },
  {
    "objectID": "chapters/chapter2.html#markdown-1",
    "href": "chapters/chapter2.html#markdown-1",
    "title": "\n2  Replicability and reproducibility\n",
    "section": "\n6.6 Markdown",
    "text": "6.6 Markdown\nMarkdown is one of the most popular markup languages for several reasons:\n\neasy to write and read compared to Latex and HTML\neasy to convert from Markdown to basically every other format using pandoc\n\neasy to implement new features",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Replicability and reproducibility</span>"
    ]
  },
  {
    "objectID": "chapters/chapter2.html#markdown-source-code",
    "href": "chapters/chapter2.html#markdown-source-code",
    "title": "\n2  Replicability and reproducibility\n",
    "section": "\n6.7 Markdown (source code)",
    "text": "6.7 Markdown (source code)\n## Markdown\n\nMarkdown is one of the most popular markup languages for several reasons:\n\n- easy to write and read compared to Latex and HTML\n- easy to convert from Markdown to basically every other format using `pandoc`\n- easy to implement new features\n. . .\nAlso the source code can be used, compared to Latex or HTML, to take notes and read. Latex and HTML need to be compiled otherwise they are very hard to read.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Replicability and reproducibility</span>"
    ]
  },
  {
    "objectID": "chapters/chapter2.html#whats-wrong-about-microsoft-word",
    "href": "chapters/chapter2.html#whats-wrong-about-microsoft-word",
    "title": "\n2  Replicability and reproducibility\n",
    "section": "\n6.8 What’s wrong about Microsoft Word?",
    "text": "6.8 What’s wrong about Microsoft Word?\nMS Word is a WYSIWYG (what you see is what you get editor) that force users to think about formatting, numbering, etc. Markup languages receive the content (plain text) and the rules and creates the final document.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Replicability and reproducibility</span>"
    ]
  },
  {
    "objectID": "chapters/chapter2.html#whats-wrong-about-microsoft-word-1",
    "href": "chapters/chapter2.html#whats-wrong-about-microsoft-word-1",
    "title": "\n2  Replicability and reproducibility\n",
    "section": "\n6.9 What’s wrong about Microsoft Word?",
    "text": "6.9 What’s wrong about Microsoft Word?\nBeyond the pure writing process, there are other aspects related to research data.\n\nwriting math formulas\nreporting statistics in the text\nproducing tables\nproducing plots\n\nIn MS Word (or similar) we need to produce everything outside and then manually put figures and tables.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Replicability and reproducibility</span>"
    ]
  },
  {
    "objectID": "chapters/chapter2.html#the-solution-quarto",
    "href": "chapters/chapter2.html#the-solution-quarto",
    "title": "\n2  Replicability and reproducibility\n",
    "section": "\n6.10 The solution… Quarto",
    "text": "6.10 The solution… Quarto\nQuarto (https://quarto.org/) is the evolution of R Markdown that integrate a programming language with the Markdown markup language. It is very simple but quite powerful.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Replicability and reproducibility</span>"
    ]
  },
  {
    "objectID": "chapters/chapter2.html#basic-markdown",
    "href": "chapters/chapter2.html#basic-markdown",
    "title": "\n2  Replicability and reproducibility\n",
    "section": "\n6.11 Basic Markdown",
    "text": "6.11 Basic Markdown\nMarkdown can be learned in minutes. You can go to the following link https://quarto.org/docs/authoring/markdown-basics.html and try to understand the syntax.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Replicability and reproducibility</span>"
    ]
  },
  {
    "objectID": "chapters/chapter2.html#more-about-quarto-and-r-markdown",
    "href": "chapters/chapter2.html#more-about-quarto-and-r-markdown",
    "title": "\n2  Replicability and reproducibility\n",
    "section": "\n7.1 More about Quarto and R Markdown",
    "text": "7.1 More about Quarto and R Markdown\nThe topic is extremely vast. You can do everything in Quarto, a website, thesis, your CV, etc.\n\nYihui Xie - R Markdown Cookbook https://bookdown.org/yihui/rmarkdown-cookbook/\n\nYihui Xie - R Markdown: The Definitive Guide https://bookdown.org/yihui/rmarkdown/\n\nQuarto documentation https://quarto.org/docs/guide/",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Replicability and reproducibility</span>"
    ]
  },
  {
    "objectID": "chapters/chapter2.html#git-and-github-1",
    "href": "chapters/chapter2.html#git-and-github-1",
    "title": "\n2  Replicability and reproducibility\n",
    "section": "\n8.1 Git and Github",
    "text": "8.1 Git and Github\nThe basic idea is to track changes within a folder, assign a message and eventually a tag to a specific version obtaining a version hystory. The version history is completely navigable, you can go back to a previous version of the code.\n. . .\nThe are advanced features like branches for creating an independent version of the project to test new features and then merge into the main streamline.\n. . .\nThe entire (local) Git project can be hosted on Github to improve collaboration. Other people or collaborators can clone the repository and push their changes to the project.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Replicability and reproducibility</span>"
    ]
  },
  {
    "objectID": "chapters/chapter2.html#veeeery-basic-git-workflow-1",
    "href": "chapters/chapter2.html#veeeery-basic-git-workflow-1",
    "title": "\n2  Replicability and reproducibility\n",
    "section": "\n9.1 Veeeery basic Git workflow",
    "text": "9.1 Veeeery basic Git workflow\n\nAfter installing Git, you can start a new repository opening a terminal on a folder and typing git init. The folder is now a git project you can notice by the hidden .git folder.\n\ncd ~/some/folder\ngit init\n\nThen you can add files to the staging area. Basically these files are ready to be committed i.e. “written” in the Git history.\n\ngit add file1.txt\n# git add . # add everyting\n\nFinally you can commit the modified version of the file using git commit -m message\n\n\ngit commit -m \"my first amazing commit\"\n\nyou can see the Git hystory with all your commits:\n\ngit log",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Replicability and reproducibility</span>"
    ]
  },
  {
    "objectID": "chapters/chapter2.html#github",
    "href": "chapters/chapter2.html#github",
    "title": "\n2  Replicability and reproducibility\n",
    "section": "\n9.2 Github",
    "text": "9.2 Github\nImagine to put everyting into a server with nice viewing options and advanced features. Github is just an hosting service for your git folder.\nYou can create an empty repository on Github named git-test. Now my repo has the path git@github.com:filippogambarota/git-test.git.\ngit remote add origin git@github.com:filippogambarota/git-test.git\ngit push\nNow our local repository is linked with the remote repository. Every time we do git push our local commits will be uploaded.\nIf you worked on the repository from another machine or a colleague add some changes, you can do git pull and your local machine will be updated.\n\nThe repository git-test is online and can be seen here filippogambarota/git-test.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Replicability and reproducibility</span>"
    ]
  },
  {
    "objectID": "chapters/chapter2.html#github-1",
    "href": "chapters/chapter2.html#github-1",
    "title": "\n2  Replicability and reproducibility\n",
    "section": "\n9.3 Github",
    "text": "9.3 Github\nAn now let’s see on Github the result:",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Replicability and reproducibility</span>"
    ]
  },
  {
    "objectID": "chapters/chapter2.html#more-about-git-and-github",
    "href": "chapters/chapter2.html#more-about-git-and-github",
    "title": "\n2  Replicability and reproducibility\n",
    "section": "\n9.4 More about Git and Github",
    "text": "9.4 More about Git and Github\nThere are a lot of resources online:\n\nThe Open Science Manual - Zandonella and Massidda - Git and Github chapters.\nhttps://agripongit.vincenttunru.com/\nhttps://git-scm.com/docs/gittutorial",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Replicability and reproducibility</span>"
    ]
  },
  {
    "objectID": "chapters/chapter2.html#open-science-framework-1",
    "href": "chapters/chapter2.html#open-science-framework-1",
    "title": "\n2  Replicability and reproducibility\n",
    "section": "\n10.1 Open Science Framework",
    "text": "10.1 Open Science Framework\n\nOSF is a free, open platform to support your research and enable collaboration.\n\nIs a great tool to upload and share materials with others and collaborate on a project. Similarly to Github you can track the changes made to a project.\nThe great addition is having a DOI thus the project is persistently online and can be cited.\nIt is now common practice to create a OSF project supporting a research paper and put the link within the paper containing supplementary materials, raw data, scripts etc.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Replicability and reproducibility</span>"
    ]
  },
  {
    "objectID": "chapters/chapter2.html#open-science-framework-2",
    "href": "chapters/chapter2.html#open-science-framework-2",
    "title": "\n2  Replicability and reproducibility\n",
    "section": "\n10.2 Open Science Framework",
    "text": "10.2 Open Science Framework\nIt’s very easy to create a new project, then you simply need to add files and share it.\n\nThe project can be accessed here (depending on the visibility) https://osf.io/yf9tg/.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Replicability and reproducibility</span>"
    ]
  },
  {
    "objectID": "chapters/chapter2.html#open-science-framework-3",
    "href": "chapters/chapter2.html#open-science-framework-3",
    "title": "\n2  Replicability and reproducibility\n",
    "section": "\n10.3 Open Science Framework",
    "text": "10.3 Open Science Framework\n\n10.3.1 OSF and Github\nAn interesting feature is linking a Github repository to OSF. Now all changes made on Github (easier to manage) are mirrored into OSF. You can easily work in Github for the coding part and use OSF to upload other data or information and to assign a DOI to the project.\n\n10.3.2 Preprints\nOSF is also linked to a popular service for preprints called PsyArXiv https://psyarxiv.com/ thus you can link a preprint to an OSF project.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Replicability and reproducibility</span>"
    ]
  },
  {
    "objectID": "chapters/chapter2.html#more-on-osf",
    "href": "chapters/chapter2.html#more-on-osf",
    "title": "\n2  Replicability and reproducibility\n",
    "section": "\n10.4 More on OSF",
    "text": "10.4 More on OSF\n\nhttps://help.osf.io/article/342-getting-started-on-the-osf\nhttps://arca-dpss.github.io/manual-open-science/osf-chapter.html",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Replicability and reproducibility</span>"
    ]
  },
  {
    "objectID": "chapters/chapter2.html#more-on-reproducibility",
    "href": "chapters/chapter2.html#more-on-reproducibility",
    "title": "\n2  Replicability and reproducibility\n",
    "section": "\n10.5 More on reproducibility",
    "text": "10.5 More on reproducibility\nIn general, I highly suggest the online book The Open Science Manual https://arca-dpss.github.io/manual-open-science/ written by my friend Claudio Zandonella and Davide Massidda where these and other topics are explained in details:",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Replicability and reproducibility</span>"
    ]
  },
  {
    "objectID": "chapters/chapter2.html#footnotes",
    "href": "chapters/chapter2.html#footnotes",
    "title": "\n2  Replicability and reproducibility\n",
    "section": "",
    "text": "https://en.wikipedia.org/wiki/Literate_programming↩︎\nhttps://latexbase.com/↩︎\nhttps://markdownlivepreview.com/↩︎",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Replicability and reproducibility</span>"
    ]
  },
  {
    "objectID": "chapters/chapter3.html",
    "href": "chapters/chapter3.html",
    "title": "3  Probability of Replication",
    "section": "",
    "text": "Key Readings\nKey readings:\nOptional readings:",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Probability of Replication</span>"
    ]
  },
  {
    "objectID": "chapters/chapter3.html#key-readings",
    "href": "chapters/chapter3.html#key-readings",
    "title": "3  Probability of Replication",
    "section": "",
    "text": "Miller (2009) What is the probability of replicating a statistically significant effect?\n\n\nPawel & Held (2020) Probabilistic forecasting of replication studies\n\n\n\n\n\nUlrich & Miller (2020): Questionable research practices may have little effect on replicability. A more extended work based on the Miller (2009) model estimating the impact of questionable research practices",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Probability of Replication</span>"
    ]
  },
  {
    "objectID": "chapters/chapter3.html#formalization",
    "href": "chapters/chapter3.html#formalization",
    "title": "3  Probability of Replication",
    "section": "\n1 Formalization",
    "text": "1 Formalization\n\n1.1 The Miller (2009) definitions\nMiller (2009) proposed one of the earliest probabilistic frameworks for estimating the probability of replication. He defines two different senses of replication probabilities:\n\naggregate replication probability (ARP): the probability of researchers working on a certain research field finding a statistically significant result in a replication study given a significant original study.\nindividual replication probability (IRP): the probability of finding a significant effect on a replication study when replicating a specific original study.\n\nIn this chapter we will take off from Miller’s definitions and provide a formal framework to answer key questions about replication probabilities.\n\n\n\n\n\n\nKey questions\n\n\n\n\nWhat is the probability of replicating an observed effect in a precise replication experiment?\nWhat is the probability of replicating experiments of a certain type within a research field?\nHow is the probability of replication affected by the experimental design (e.g., sample size, etc.) and by the effect size?\nIs the probability of replication affected by questionable research practices or publication bias?\n\n\n\n\n1.2 Notation\nWe begin by providing a consistent notation and terminology, and a formal framework for the statistical models and analysis examples throughout the book. The notation is based on the replication model by Pawel & Held (2020) and the meta-analytical notation by Hedges & Schauer (2021), Schauer & Hedges (2021), and Hedges & Schauer (2019).\nWe use \\(x\\)’s to denote predictors, treatments, and fixed factors, and \\(y\\)’s to denote outcomes. The subscript \\(i\\) denotes individuals within a study, while \\(r\\) indexes studies. We use \\(r=0\\) to denote the original study, and \\(r = 1, 2, \\ldots, R\\) to denote the \\(R \\geq 1\\) replication studies.\nWe use the greek letter \\(\\theta\\)’s to define the true effects for the original and replication study(s). These are population parameters. In particular \\(\\theta_0\\) is the effect in original study and \\(\\theta_r\\) is the effect in the \\(r\\)-th study. In principle, \\(\\theta\\)’s can be vector valued, but for simplicity we will focus on a scalar effect. We will also use other greek letters for parameters specific to individual settings.\nThe one-to-one replication design is used when a single replication study is replicating an original experiment. On the other hand, a replication project with \\(R\\) replication studies is called a one-to-many design.\nIn Chapter 1 we covered the Machery definition of replication, in which only the randomly drawn subjects differ between the original study and the replications. In a replication of this type, the population of reference remains the same, an therefore \\(\\theta_0 = \\theta_1 = \\ldots = \\theta_R\\). This will be the case with Miller’s original, for example. Generally, in a highly precise replication, this will also be approximately correct.\nOn the other hand, if we consider a group of extension experiments, or even more broadly examine an entire field of research, we typically alter fixed factors, and thus consider many different, albeit related, populations. In this case \\(\\theta\\)’s are no longer identical. Variation between \\(\\theta\\)’s is an important aspect of replicability analysis. We will return to it in several places. For now, consider that this variation depends on the unverse of studies of interest. For example, a group of extension experiments for a specific treatment / outcome combination will generally have smaller variability than a group considering the effect of the same treatment on a variety of outcomes.\n\n1.3 Two-Level Models\nOne way to think about studying the variation of the \\(\\theta\\)’s is to frame the analysis in a two-level model. Within a study, we imagine that subjects are drawn from a population, with each population associated with a specific study design / fixed factors. In turn, then, study designs themselves are seen as draws from a hypothetically vast collection of possible designs. This allows to describe the variation of \\(\\theta\\)’s probabilistically. In many examples we will use a Normal distribution to describe the resulting population of effect sizes. This normal distribution will have mean \\(\\mu_{\\theta}\\) and variance \\(\\tau^2\\).\nReplication is often framed in terms of tests of hypothesis. Parenthetically, several fields of study are making organized attempts at moving scientific reporting away from tests of hypotheses (Harrington et al., 2019). Evidential summaries from hypothesis tests, such as p-values, systematically confound sample size and effect size (Wasserstein & Lazar, 2016). Better reporting focuses on quantitative effect sizes. Nonetheless, replicability analysis requires explicit consideration of significance, as this remains a factor in determining publication.\nHere we will define the null hypothesis to be that “the effect is too small to be of scientific interest”. Formally, the null hypothesis is true when \\(|\\theta| &lt; h_0\\) where \\(h_0\\) is specified by the analyst and defines a so-called region of practical equivalence or ROPE (Kruschke, 2015; Kruschke & Liddell, 2018)1. In a precise replication, \\(\\theta\\) does not change across studies and thus the null hypothesis is either true or not in every study. In contrast, if we consider a group of extension experiments or a whole field, \\(\\theta\\)’s vary and the null hypothesis may hold in some studies and not others. We will use \\(\\pi\\) to denote the proportion of non-null effect in a population of studies.\nMiller (2009) noted that \\(\\pi\\) is an unknown quantity that is difficult to estimate from the literature. In Psychology, Wilson & Wixted (2018) tried to estimate \\(\\pi\\) based on the Collaboration & Open Science Collaboration (2015) large scale replication project finding different rates for social and cognitive psychology. In a similar vein, Jager & Leek (2014) tried to estimate the false discovery rate across the top medical journals.\nThe more precise the replication, the smaller will \\(\\tau\\) be. As \\(\\tau\\) goes to zero, all \\(\\theta\\)’s will concentrate around \\(\\mu_{\\theta}\\) and \\(\\pi\\) will approach either 0 or 1 depending on whether \\(\\mu_{\\theta}\\) is in the region of practical equivalence.\nFigure 3.1 depicts this simple generating model. The idea is that true effects in a given research area \\(\\theta_r\\) comes from a normal distribution with mean \\(\\mu_\\theta\\) and standard deviation \\(\\tau\\). Within this research area, we have a certain proportion of true \\(\\pi\\) and false \\(1 - \\pi\\) effects. In case of false effects, we can imagine a ROPE in which the effects are assumed to be equal to zero. Then these true effects \\(\\theta_r\\) can be the target of an original study \\(\\hat \\theta_0\\) and one or more replication studies \\(\\hat \\theta_r\\) (\\(r = (1, \\ldots, R)\\)). If the study is a precise replication study the heterogeneity is expected to be close to zero while in an extension experiment we are expecting more heterogeneity. To simplify, the expected heterogeneity of a very precise replication is zero and all studies are essentially estimating the same original true effect \\(\\theta_0\\). In cases where the heterogeneity is different from zero (i.e., an extension) each replication study is associated with a different underlying true effect \\(\\theta_r\\). The degree of heterogeneity is related to the degree of precision, viz., the number and relevance of changes to fixed factors. This model can be considered similar to a random-effects meta-analysis (described in Chapter XXX). The model can also be used to simulate replication studies as will be shown in Chapter XXX.\n\n\n\n\n\n\nFigure 1: two level generating model\n\n\n\n1.4 Multi-level Model\nFigure 3.1 provides an example of a generating model for a specific research area in the literature, e.g., the distribution of true effects related to psychological treatments for depression. Some treatments in some conditions will have a higher/lower effect and in other conditions they will have a null effect. Extending the same reasoning to multiple effects within a broader literature, we can include another hierarchical layer in the model. Figure 3.2 depicts this extended model. To simplify we can have the treatments for depression and anxiety. These research areas have specific average effects \\(\\mu_\\theta^1, \\mu_\\theta^2\\), heterogeneity \\(\\tau^1, \\tau^2\\) and proportions of true \\(\\pi^1, \\pi^2\\) and false \\(1 - \\pi^2, 1 - \\pi^2\\) effects. The combinatons of the two research areas define the broader literature defined as a mixture of the two distributions. Each component (i.e., anxiety and depression distributions) is associated with a weight \\(\\omega^1, \\omega^2\\) representing the importance (e.g., prevalence of studies) in the final mixture distribution. As a final note, instead of considering \\(\\mu_\\theta\\) and \\(\\tau\\) as fixed, we can sample these values from two distributions generating a more realistic research area contanining a larger set of true effects. In terms of replication, we can apply the same logic of Figure 3.1 where the degree of precision determines the expected heterogeneity in the replication/extension study.\n\n\n\n\n\nFigure 2: Mixture generating model\n\n\n\n1.5 Simple Null Hypotheses\nMiller (2009) and many other works in the literature are concerned with simple null hypotheses. In our notation the null hypothesis is true when \\(|\\theta| = 0\\). The logic of section Section 3.1.3 can be cloned for this case as well, as illustrated in Figure Figure 3.3. Now the distribution of \\(\\theta\\) has two components: a continuous component describing the variation of the effect assuming the effect is not zero, and a discrete component at exactly zero. The probability of drawing a nonzero effect will again be \\(\\pi\\), which is also the area under the bell curve in the figure. Thus the point mass at zero is \\(1-\\pi\\). In terms of replication, we can apply the same logic of Figure 3.1 and Figure 3.2 where the degree of precision determines the expected heterogeneity in the replication/extension study.\n\n\n\n\n\nFigure 3: Point null generating model",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Probability of Replication</span>"
    ]
  },
  {
    "objectID": "chapters/chapter3.html#probability-of-replication-in-exact-replication-experiments",
    "href": "chapters/chapter3.html#probability-of-replication-in-exact-replication-experiments",
    "title": "3  Probability of Replication",
    "section": "\n2 Probability of Replication in Exact Replication Experiments",
    "text": "2 Probability of Replication in Exact Replication Experiments\n\n2.1 Two-Sample Design Review\nWe will often use a two-sample design to make the discussion more concrete. Assuming the homogeneity of variances among the two groups, the \\(t\\) statistic is calculated as follows:\n\\[\nt = \\frac{\\overline{y}_1 - \\overline{y}_2}{SE_{\\overline{y}_1 - \\overline{y}_2}}\n\\tag{1}\\]\nwhere subscripts denote the two groups. The standard error at the denominator is: \\[\n\\mbox{SE}_{\\overline{y}_1 - \\overline{y}_2} = s_p \\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}\n\\tag{2}\\]\nwhere \\(s_p\\) is the pooled standard deviation: \\[\ns_p = \\sqrt{\\frac{s^2_{y_1}(n_1 - 1) + s^2_{y_1}{(n_2 - 1)}}{n_1 + n_2 - 2}}\n\\tag{3}\\]\nIn the case of a simple null hypothesis, the p-value can be calculated from a \\(t\\) distribution with \\(\\nu = n_1 + n_2 - 2\\) degrees of freedom.\nMore generally, when the true effect size is not zero, the \\(t\\) statistic will follow a non-central Student’s \\(t\\) distribution with \\(\\nu\\) degrees of freedom and non-centrality parameter \\(\\lambda\\), calculated as follows: \\[\n\\lambda = \\delta \\sqrt{\\frac{n_1 n_2}{n_1 + n_2}}\n\\tag{4}\\]\nwhere \\(\\delta\\) is the true effect size — the true difference between means divided by the true group specific standard error.\nIf \\(F_{t_\\nu}\\) is the cumulative distribution function of the non-central Student’s \\(t\\) distribution, the statistical power is calculated as:\n\\[\n1 - \\beta = 1 - F_{t_\\nu} \\left(t_c, \\lambda \\right) + F_{t_\\nu} \\left(-t_{c}, \\lambda \\right)\n\\tag{5}\\]\nWith this setup, we can easily implement these equations in R, thus creating a flexible set of functions to reproduce and extend the examples from Miller (2009).\n\npower.t &lt;- function(d, n1, n2 = NULL, alpha = 0.05){\n    if(is.null(n2)) n2 &lt;- n1\n    df  &lt;- n1 + n2 - 2\n    ncp &lt;- d * sqrt((n1 * n2) / (n1 + n2))\n    tc  &lt;- abs(qt(alpha/2, df = df))\n    pt(-tc, df = df, ncp = ncp) + pt(tc, df = df, ncp = ncp, lower.tail = FALSE)\n}\n\n\npower.t(d = 0.5, n1 = 30)\n## [1] 0.4778965\npower.t(d = 0.5, n1 = 100)\n## [1] 0.9404272\npower.t(d = 0.1, n1 = 200)\n## [1] 0.1694809\n\nWhen the null hypothesis is a set (section Section 3.1.3) the p-value is calculated conditional on the least favorable case, which occurs when \\(\\theta = h_0\\) or \\(\\theta = - h_0\\), so the noncentral \\(t\\) is used to compute both the p-value and the power.\nAnother possibility is to use the full ROPE for the statistical test within the so-called equivalence testing framework (Lakens et al., 2018; see also Goeman et al., 2010 for the three-sided hypothesis testing).\n\n2.2 The Calculus of Replication Probabilities\nConsider the case of strict replication, so the null hypothesis is true or false in both studies at the same time.\nTo prepare for Miller (2009), we look at replication probability by focusing on replicating significance. Given that the original study was significant (that is that \\(|t_0| &gt; t_{c_0}\\)), what is the probability that the second study will also be significant and the effect will be in the same direction as the original study? Formally we express significance in the original study as \\(|t_0| &gt; t_{c_0}\\), significance in the replication study as \\(|t_1| &gt; t_{c_1}\\), and concordance of the direction of the effect as \\(t_0 \\cdot t_1 &gt; 0\\).\n\\[\np_{rep} = \\Pr(|t_1| &gt; t_{c_1} \\cap t_0 \\cdot t_1 &gt;0 \\boldsymbol{\\mid} |t_0| &gt; t_{c_0})\n\\tag{6}\\]\nUsing the laws of conditional and total probability in turn, We can express this as\n\\[\n\\begin{split}\np_{rep} &= \\Pr(|t_1| \\geq t_{c_1} \\boldsymbol{\\mid} |t_0| \\geq t_{c_0}) \\\\[10pt]\n&= \\frac{\\Pr(|t_1| \\geq t_{c_1} \\cap t_0 \\cdot t_1 &gt;0 \\cap |t_0| \\geq t_{c_0})}{\\Pr(|t_0| \\geq t_{c_0})} \\\\[10pt]\n\\end{split}\n\\tag{7}\\]\nConsiderering separately the numerator and the denominator, and using the law of total probability, we have:\n\\[\n\\begin{split}\n\\mbox{numerator} =& \\Pr(H_1) \\cdot \\Pr(|t_1| \\geq t_{c1} \\cap t_0 \\cdot t_1 &gt;0 \\cap |t_0| \\geq t_{c_0} \\mid H_1) + \\\\\n& \\Pr(H_0) \\cdot \\Pr(|t_1| \\geq t_{c1} \\cap t_0 \\cdot t_1 &gt;0 \\cap |t_0| \\geq t_{c0} \\mid H_0) \\\\[10pt]\n\\mbox{denominator} =& \\Pr(H_1) \\cdot \\Pr(|t_0| \\geq t_{c_0} \\mid H_1) + \\\\\n& \\Pr(H_0) \\cdot \\Pr(|t_0| \\geq t_{c_1} \\mid H_0).\n\\end{split}\n\\tag{8}\\]\nThese expressions are a bit cumbersome, but they allow us to express the probability of replication as a useful function of power and significance level.\n\\[\n\\Pr (|t_0| \\geq t_{c_0} | H_0 ) = \\alpha_0, \\quad \\mbox{and} \\quad \\Pr (|t_0| \\geq t_{c_0} \\cap t_0 \\cdot t_1 &gt;0 | H_0 ) = \\alpha_1 / 2\n\\tag{9}\\]\nwhere \\(\\alpha_r\\) is the the study-specific significance level. The division by two is because half of the false positive rejections are expected to occur in the discordant direction under the null. Also\n\\[\n\\Pr(|t_0| \\geq t_{c_0} | H_1) = 1-\\beta_0 \\quad \\mbox{and} \\quad \\Pr(|t_1| \\geq t_{c_1} \\cap t_0 \\cdot t_1 &gt;0 | H_1) \\approx 1-\\beta_1\n\\tag{10}\\]\nwhere \\(1-\\beta_r\\) is the study specific power. Power is simple to compute when the alternative is also a single point, but it is more complex when, as in section Section 3.1.3, we consider a distribution of non-null effects. For now, think about \\(1-\\beta_r\\) as an average power. We will return on this point when we discuss hierarchical models.\nThe approximation sign \\(\\approx\\) refers to the fact that we are ignoring the probability that the replication study will generate a significant and discordant effect if the true effect is not null and is indeed in the direction identified in the original study. This probability will be small when the sample size is large or the effect size is large, but can be nontrivial in small studies with small effects.\nRewriting and using independence of the two studies:\n\\[\n\\begin{split}\np_{rep} & \\approx\n\\frac{\\Pr(H_1) \\cdot (1-\\beta_0)(1-\\beta_1) + (1- \\Pr(H_1)) \\cdot \\alpha_0 \\alpha_1 / 2}{\\Pr(H_1) \\cdot (1-\\beta_0) + (1-\\Pr(H_1)) \\cdot \\alpha_0} \\\\[10pt]\n\\end{split}\n\\tag{11}\\]\nThe key quantity that remains to understand is \\(\\Pr(H_1)\\), the probability that the effect is not null. The interpretation of this probability depends on the context as well as the analysts’ approach. In section Section 3.2.3 we consider Miller’s version, where \\(\\Pr(H_1)\\) is essentially \\(\\pi\\) from Section Section 3.1.3. Later we will cover an alternative Bayesian interpretation where \\(\\Pr(H_1)\\) is specified based on expert knowledge existing prior to the original experiment.\nWe take two quick digressions before going back to Miller. First, if \\(R&gt;1\\) we can extend this reasoning to more general expressions. For example if we want to estimate the probability of having successful replications in \\(R\\) experiments we can consider \\[\n\\Pr \\left( \\bigcap_{r=1}^R |t_r| &gt; t_{c_r} \\boldsymbol{\\mid} |t_0| &gt; t_{c_0} \\right).\n\\tag{12}\\]\nSecond, if you are not familiar with the logic behind Bayes’s rule, we will walk you through it in the remainder of this section. The probability of the event A happening given (\\(|\\)) that B happened is:\n\\[\n\\Pr(A|B) = \\frac{\\Pr(A \\cap B)}{\\Pr(B)}\n\\tag{13}\\]\nWhere \\(\\Pr(A \\cap B)\\) is the joint probability that can be calculated as:\n\\[\n\\Pr(A \\cap B) = \\Pr(B|A) \\cdot \\Pr(A)\n\\tag{14}\\]\nThen the full equation also known as Bayes rule is:\n\\[\n\\Pr(A|B) = \\frac{\\Pr(B|A) \\cdot \\Pr(A)}{\\Pr(B)}\n\\tag{15}\\]\n\n2.3 Miller’s Aggregate Replication Probability (ARP)\n\nTo recap, Miller (2009) is about replication with the highest precision, because he considers no differences in populations between the experiments. He focuses on tests of hypotheses, and considers replication of experiments with significant testing results, as we did above. He also assumes that power and significance level are the same in the original and replication experiment. He also evaluates power at a single point at a time, effectively assuming that the statistical problem is testing a simple null hypothesis against a simple alternative.\nWith regards to \\(\\Pr( H_1)\\) he considers a scheme similar to that of section Section 3.1.3. A study design is drawn randomly from the aggregate describing a scientific field of interest, and then replicated with high precision. In what proportion of cases does this exercise lead to a replication? For this calculation \\(\\Pr( H_1)\\) is the same as \\(\\pi\\) in Figure 3.1. Miller calls \\(\\pi\\) the strength of a research area. This parameter has also been considered by other authors when evaluating the probability of replication within a research field (Ioannidis, 2005; Wilson & Wixted, 2018).\nIn Miller’s setting, the only relevant parameters in the replication model are the type-2 error rate \\(\\beta\\) (or equivalently the statistical power \\(1 - \\beta\\)), the type-1 error rate \\(\\alpha\\) and the proportion of true hypotheses \\(\\pi\\) in a certain research field.\nUsing formula Equation 3.11, the probability of replicating an effect in a given research area (ARP), conditional on a significant original experiment, is \\[\np_{ARP} \\approx \\frac\n{\\pi \\cdot (1 - \\beta)^2 + (1 - \\pi) \\cdot \\alpha^2 / 2}\n{\\pi \\cdot (1 - \\beta)   + (1 - \\pi) \\cdot \\alpha}\n\\tag{16}\\]\n\nThe denominator is the probability of a significant result in the original study, broken down into the probability of a false positive \\((1 - \\pi) \\alpha\\) plus the probability of a true positive \\(\\pi (1 - \\beta)\\). Now, let’s consider the numerator. This is the probability that both the original and replication studies are significant and also concordant in the sign of their effects.\nA good way to understand the process is by formalizing inference using a contingency table as done in Table 3.1.\n\n\nTable 1: Inferential process formalized using a contingency table\n\n\n\n\n\n\n\n\nDecision on \\(H_0\\)\n\n\n\n\n\n\n\n\n\n\n\nFalse\n\n\nTrue\n\n\n\n\n\n\n\n\n\\(\\boldsymbol{H_0}\\)\n\n\nFalse\n\n\nTrue Positive (\\(1 - \\beta\\))\n\n\nFalse Negative (\\(\\beta\\))\n\n\n\\(\\pi\\)\n\n\n\n\n\\(\\boldsymbol{H_0}\\)\n\n\nTrue\n\n\nFalse Positive (\\(\\alpha\\))\n\n\nTrue Negative (\\(1 - \\alpha\\))\n\n\n\\(1 - \\pi\\)\n\n\n\n\n\n\n\nFor example, the probability that something is significant is the sum of true positives (i.e., the statistical power) and false positives (i.e., type-1 errors) weighted by the prevalence \\(\\pi\\). Thus the numerator \\(\\Pr(|t_1| &gt; t_{c1} \\cap |t_0| &gt; t_{c0})\\) can be written as the sum between:\n\nOriginal study being significant \\((1 - \\beta) \\cdot \\pi + \\alpha \\cdot (1 - \\pi)\\)\n\nReplication study being significant \\((1 - \\beta) + \\frac{\\alpha}{2}\\)\n\n\nWe have \\(\\frac{\\alpha}{2}\\) in the second expression because we assume that replication success happens when the effect is in the same direction, i.e., we are ignoring false positives with opposite sign.\nAn interesting use of Equation 3.11 and Equation 3.16 is that we can explore the probability of replication in different scenarios by manipulating the parameters defined above. Ulrich & Miller (2020) improved and extended Miller (2009), by estimating the impact of questionable research practices on the probability of replication. They found that the most important parameter negatively impacting replication rates is \\(\\pi\\), with questionable research practices having only a minor impact.\n\n2.4 R Code for the Aggregate replication probability (ARP)\nWe implemented the Miller (2009) equations in R (see also ?arp):\n\narp &lt;- function(R, pi, power, alpha) {\n\n    if(any(R &lt; 2)) stop(\"number of studies should be equal or greater than 2\")\n\n    if(length(power) != 1 & length(power) != R){\n      stop(\"length of power vector should be 1 or R\")\n    }\n\n    if(length(power) == 1) power &lt;- rep(power, R)\n\n    num &lt;- pi * prod(power) + (1 - pi) * alpha * (alpha/2)^(R - 1)\n    den &lt;- pi * power[1] + (1 - pi) * alpha\n    num/den\n}\n\nr is the number of replication studies, pi is \\(\\pi\\), power is \\(1 - \\beta\\) and alpha is \\(\\alpha\\).\nThe Figure 3.4 depicts how the ARP changes as we vary the theory strength \\(\\pi\\) and the power (assumed to be the same for the original and replication experiment).\n\n\n\n\n\n\n\nFigure 4\n\n\n\n\nThe main takeaways from Figure 3.4 are:\n\nfor medium (and probably plausible) levels of theory strength \\(\\pi\\), the replication probabilities are relatively low — even when the power is high\nwhen \\(R &gt; 1\\), the replication probabilities are very low — in all conditions\n\n2.5 Miller’s Individual replication probability (IRP)\n\nIn contrast to the ARP, which considers a randomly drawn original study from a field, the individual replication probability (IRP) focuses on a specific original study. Miller’s approach is ask what is the probability of a significant and concordant test result in the replication study, pretending that the estimated effect in the original study is correct. The reduces to calculating the power of the replication study conditional at observed effect, that is \\(p_{IRP} = 1 - \\beta(\\hat\\theta_0)\\)\nUsing the Equations from Section 3.2.1, we can reproduce and extend the Miller (2009) figures. Basically, we estimate the IRP assuming that the power (thus the effect size and sample size) of the replication study is the same as that of the original study. The key function is p2d, which returns the power given the effect size for the point estimate and the lower/upper limit of the effect size confidence interval2.\n\np2d &lt;- function(p, n, alpha = 0.05){\n    df &lt;- n*2 - 2\n    to &lt;- abs(qt(p/2, df))\n    se &lt;- sqrt(1/n + 1/n)\n    cit &lt;- effectsize:::.get_ncp_t(to, df)\n    lb.t &lt;- cit[1]\n    ub.t &lt;- cit[2]\n    d &lt;- to * se\n    lb &lt;- lb.t * se\n    ub &lt;- ub.t * se\n    # power\n    d.power &lt;- power.t(d = d, n1 = n, alpha = alpha)\n    lb.power &lt;- power.t(d = lb, n1 = n, alpha = alpha)\n    ub.power &lt;- power.t(d = ub, n1 = n, alpha = alpha)\n    list(t = to, d = d,\n         se = se, df = df,\n         ci.lb = lb, ci.ub = ub,\n         d.power = d.power,\n         lb.power = lb.power,\n         ub.power = ub.power,\n         p = p)\n}\n\n\n\n\n\n\n\n\nFigure 5: fig-miller-irp\n\n\n\n\nThe most important takeaway from Figure 3.5 is that the statistical significance of the original experiment alone \\(p_0 \\leq \\alpha\\) is not enough to reliably estimate the probability of replicating the effect. Only when the p-value is very small is the range of expected power relatively narrow. For significant but higher p-values there is a lot of uncertainty vis-á-vis the IRP. In this context, Perugini et al. (2014) have recommended against the use of the point estimate of an effect size to plan a new study (e.g., a replication). A more conservative approach would be to use another value (e.g., the lower bound) of the \\((1 - \\alpha) \\cdot 100\\) confidence interval. This conservative approach can mitigate the likely overestimation of the original study, thus providing a more realistic estimation of the probability of replication.\n\n2.6 Bayesian Individual Replication Probability\nMiller’s IRP has two main limitations: first, the effect size estimated in the original study is subject to sampling variability. This is not considered by Miller. In reality, the effect size could be larger or smaller. Second, in scientific areas where finding large effects is more challenging, the true effect size is more likely to be smaller, relative to those estimated for research areas in which large effects abound. So considering the study in isolation is a limitation. Even at the individual level, we should expect a higher probability of replication for research areas where there are more true effects to be found. Some of these limitations can be mitigated by Bayesian replication probabilities. If you are not familiar with the logic behind Bayesian statistics, Lindley (1972) provides an excellent introduction.\nFor a simple example, consider specifying \\(\\Pr(H_1)\\) for the individual experiment at hand to reflect knowledge exisitng up to the point of the original experiment, but not including the original experiment itself. In the depression treatment example, this may have to to with the strength of the hypothesis, the plausibility of treatment examined, prior experiments in animals if the original experiment is in humans etc. Then one can apply the calculus of Section Section 3.2.2 (and specifically Equation 3.16) to evaluate the individual replication probability. Consideration of the sampling variability in the effect size may be incorporated in the specific way one calculates the average power. Consideration of the strength of the field of study can be embedded in the prior, either informally or through the use of hierarchical models as seen later.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Probability of Replication</span>"
    ]
  },
  {
    "objectID": "chapters/chapter3.html#selection-and-regression-to-the-mean",
    "href": "chapters/chapter3.html#selection-and-regression-to-the-mean",
    "title": "3  Probability of Replication",
    "section": "\n3 Selection and Regression to the Mean",
    "text": "3 Selection and Regression to the Mean\nThe original effect size estimation could be biased (usually inflated) for several reasons (e.g., publication bias) affecting the actual probability of replication. For example, let’s assume that original studies are published only when they are significant. In addition, let’s assume we are considering a research area with medium strength and with average medium-small effect sizes (a very likely scenario). Finally, let’s assume we are working with sample sizes that are relatively low. Such a research area will tend to be low-powered; and, significant effects will either be inflated (in the best case) or false positives (in the worst case).\n\nWhen conducting replication studies of these kinds of original studies, we may find significant but substantially smaller estimated effects (or even non significant effects.) For example, the large scale project Collaboration & Open Science Collaboration (2015) reported that — regardless of significance level — replicated effects were substantially smaller than original effects. Regression to the mean is a good explanation of this phenomenon, since selection of extreme results will produce on average lower estimations (i.e., shrinkage) in later studies. In addition, if the replication study design (i.e., sample size) is based on the original effect size estimation, the inflation can greatly reduce the power. This is related to the concept of the type-M (M for magnitude) error (Gelman & Carlin, 2014), which quantifies the exent of inflation given the true effect and the sample size.\nIn Code 3.1, there is a small simulation showing how to estimate the type-M error for a two-sample design.\n\n\n\nCode 1: Type-M error simulation for a two-samples design\n\nd &lt;- 0.3\nn &lt;- 50\nnsim &lt;- 1e4\n\ndi &lt;- pi &lt;- rep(NA, nsim)\n\nfor(i in 1:nsim){\n    g0 &lt;- rnorm(n, 0, 1)\n    g1 &lt;- rnorm(n, d, 1)\n    tt &lt;- t.test(g1, g0)\n    di[i] &lt;- tt$statistic * tt$stderr\n    pi[i] &lt;- tt$p.value\n}\n\nsignificant &lt;- pi &lt;= 0.05\nmean(abs(di[significant])) / d\n\n\n\n\n[1] 1.748238\n\n\nThe type-M error increases as the sample size and/or the true effect size decreases. The Figure 3.6 shows the type-M error for different conditions.\n\nCodetypeM &lt;- function(d, n, B = 1e3){\n    di &lt;- pi &lt;- rep(NA, B)\n    for(i in 1:nsim){\n        g0 &lt;- rnorm(n, 0, 1)\n        g1 &lt;- rnorm(n, d, 1)\n        tt &lt;- t.test(g1, g0)\n        di[i] &lt;- tt$statistic * tt$stderr\n        pi[i] &lt;- tt$p.value\n    }\n    significant &lt;- pi &lt;= 0.05\n    mean(abs(di[significant])) / d\n}\n\nsim &lt;- expand.grid(\n    n = seq(10, 100, 10),\n    d = c(0.1, 0.3, 0.5, 1),\n    M = NA\n)\n\n\nfor(i in 1:nrow(sim)){\n    sim$M[i] &lt;- typeM(sim$d[i], sim$n[i])\n}\n\n\n\n\n\n\n\n\n\nFigure 6: fig-typem\n\n\n\n\nFigure 3.7 shows the impact of regression to the mean. We simulated two scenarios with a selection for extreme (i.e., significant) values, where the true average effect is zero. In the top panel, we included some heterogeneity — there is a distribution of true effects where the average size is zero. The selection occurs in a hypothetical original study with a sample size of 25. Then, we simulated two other studies with sample sizes 100 and 500, respectively, sampling from the same distribution and with the same true effect. The crucial lesson of Figure 3.7 is that selection of extreme studies will result in replication studies with lower power and smaller estimated effects. This is especially true if the replication study is based on the estimated effect of the original, low-powered study.\n\n\n\n\n\n\n\nFigure 7: Regression to the mean. Triangles are significant studies at \\(\\alpha = 0.05\\) and dots are the non-significant ones. The highlighted effect (in red) is a randomly selected effect that was significant with \\(n = 25\\) participants (original study \\(\\theta_0\\)). We simulated \\(k = 50\\) true effects \\(\\theta_k\\) (in blue) from a Gaussian distribution with mean \\(\\mu_{\\theta = 0}\\) and \\(\\tau^2 = 0.05\\). Then we simulated the first experiment as a comparison of two independent groups, with effect size \\(\\theta_k\\). Then we used the same \\(\\theta_k\\) to simulate another experiment with \\(n = 100\\) and \\(n = 500\\) participants. Clearly, as \\(n\\) increases, the \\(\\theta_k\\) estimation is more precise, with a shrinkage towards the average effect \\(\\mu_\\theta\\).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Probability of Replication</span>"
    ]
  },
  {
    "objectID": "chapters/chapter3.html#probability-of-replication-in-extension-experiments",
    "href": "chapters/chapter3.html#probability-of-replication-in-extension-experiments",
    "title": "3  Probability of Replication",
    "section": "\n4 Probability of Replication in Extension Experiments",
    "text": "4 Probability of Replication in Extension Experiments\n\n4.1 Hierarchical Models\nPawel & Held (2020) generalize the methodology of Miller (2009). Miller’s model neglected the possibility of heterogeneity among true effects, assuming that the power (and thus the effect size) is the same when \\(H_0\\) is false. A more sophisticated way to predict the probability of estimating a certain finding is to use the information of the original study and replication study (one-to-one replication) within a hierarchical bayesian model. Here are two main features of this more general modelling approach:\n\nincludes the uncertainty of both studies and eventually the between-study heterogeneity \\(\\tau\\). This allows the replication study to be truly different from the original study (e.g., it allows us to take the effects of publication bias into account).\nincludes a shrinkage factor, balancing the strength of the original study and the between-study heterogeneity, which allows us to account for regression to the mean and effect size inflation of the original study.\n\nWe adapted the Pawel & Held (2020) model, creating a more general version that can be easily adapted to simulating and analyzing data from replication studies as well as predicting the probability of replication given the model parameters. The generalized version of the model is presented in Figure 3.8. (The actual model will be implemented in Chapter XXX.)\nIn the simple case of a two-group study, where \\(x_{ir}\\) denotes whether individual \\(i\\) is in group one, the distribution of the data in study \\(r\\) is as follows:\n\\[\ny_{ir}|x_{ir} \\sim \\mathcal{N}(\\mu_r + x_{ir}\\theta_r, \\sigma_r)\n\\]\nIn this case, \\(\\theta_r\\) can be intepreted as the mean difference between the two groups.\nAt the effect size level, for example, we may have:\n\\[\n\\theta_r \\sim \\mathcal{N}(\\mu_\\theta, \\tau)\n\\]\n\nFigure 3.8 depicts the hierarchical model. \\(\\mu_\\theta\\) is the average true effect and \\(\\tau\\) is the heterogeneity thus the standard deviation of the true effects. This is very similar to the generating model presented in Figure 3.1 and can be considered as a standard two-level model (or a random-effects meta-analysis). Both \\(\\mu_\\theta\\) and \\(\\tau\\) can be associated with a prior distribution. In this case \\(\\boldsymbol{\\zeta_\\theta}\\) and \\(\\boldsymbol{\\zeta_\\tau}\\) are vectors of parameters of the prior distributions. A common choice would be a normal distribution for \\(\\mu_\\theta\\) and an half-cauchy distribution for \\(\\tau\\) (Williams et al., 2018). At the study level, \\(\\theta_r\\) is the study-specific slope (i.e., the difference between the groups), \\(\\mu_t\\) is the intercept (i.e., expected value for the reference group). \\(y_{nr}\\) and \\(x_{nr}\\) are the response values and the predictors (respectively) for the individual \\(n\\) of the study \\(r\\).\n\n\n\n\n\n\n\n\nFigure 8: Hierarchical baysian model for estimating the replication outcome.\n\n\n\n\nThe model can be easily implemented in R using the probabilistic programming language STAN (Stan Development Team, 2021) or, under some conditions, using the analytical form as used in Pawel & Held (2020).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Probability of Replication</span>"
    ]
  },
  {
    "objectID": "chapters/chapter3.html#references",
    "href": "chapters/chapter3.html#references",
    "title": "3  Probability of Replication",
    "section": "\n5 References",
    "text": "5 References\n\n\n\n\nCollaboration, O. S., & Open Science Collaboration. (2015). Estimating the reproducibility of psychological science. In Science (Vol. 349). https://doi.org/10.1126/science.aac4716\n\n\nGelman, A., & Carlin, J. (2014). Beyond power calculations. In Perspectives on Psychological Science (Vol. 9, pp. 641–651). https://doi.org/10.1177/1745691614551642\n\n\nGoeman, J. J., Solari, A., & Stijnen, T. (2010). Three-sided hypothesis testing: Simultaneous testing of superiority, equivalence and inferiority. Statistics in Medicine, 29, 2117–2125. https://doi.org/10.1002/sim.4002\n\n\nHarrington, D., D’Agostino, R. B., Sr, Gatsonis, C., Hogan, J. W., Hunter, D. J., Normand, S.-L. T., Drazen, J. M., & Hamel, M. B. (2019). New guidelines for statistical reporting in the journal. The New England Journal of Medicine, 381, 285–286. https://doi.org/10.1056/NEJMe1906559\n\n\nHedges, L. V., & Schauer, J. M. (2019). Statistical analyses for studying replication: Meta-analytic perspectives. Psychological Methods, 24, 557–570. https://doi.org/10.1037/met0000189\n\n\nHedges, L. V., & Schauer, J. M. (2021). The design of replication studies. Journal of the Royal Statistical Society. Series A, (Statistics in Society), 184, 868–886. https://doi.org/10.1111/rssa.12688\n\n\nIoannidis, J. P. A. (2005). Why most published research findings are false. PLoS Medicine, 2, e124. https://doi.org/10.1371/journal.pmed.0020124\n\n\nJager, L. R., & Leek, J. T. (2014). An estimate of the science-wise false discovery rate and application to the top medical literature. Biostatistics, 15, 1–12. https://academic.oup.com/biostatistics/article-abstract/15/1/1/244509\n\n\nKruschke, J. K. (2015). Bayesian approaches to testing a point (“null”) hypothesis. In Doing bayesian data analysis (pp. 335–358). Elsevier. https://doi.org/10.1016/b978-0-12-405888-0.00012-x\n\n\nKruschke, J. K., & Liddell, T. M. (2018). The bayesian new statistics: Hypothesis testing, estimation, meta-analysis, and power analysis from a bayesian perspective. Psychonomic Bulletin & Review, 25, 178–206. https://doi.org/10.3758/s13423-016-1221-4\n\n\nLakens, D., Scheel, A. M., & Isager, P. M. (2018). Equivalence testing for psychological research: A tutorial. Advances in Methods and Practices in Psychological Science, 1, 259–269. https://doi.org/10.1177/2515245918770963\n\n\nLindley, D. V. (1972). Bayesian statistics a review. In Bayesian statistics (pp. 1–74). https://doi.org/10.1137/1.9781611970654.ch1\n\n\nMiller, J. (2009). What is the probability of replicating a statistically significant effect? Psychonomic Bulletin & Review, 16, 617–640. https://doi.org/10.3758/PBR.16.4.617\n\n\nPawel, S., & Held, L. (2020). Probabilistic forecasting of replication studies. PloS One, 15, e0231416. https://doi.org/10.1371/journal.pone.0231416\n\n\nPerugini, M., Gallucci, M., & Costantini, G. (2014). Safeguard power as a protection against imprecise power estimates. Perspectives on Psychological Science: A Journal of the Association for Psychological Science, 9, 319–332. https://doi.org/10.1177/1745691614528519\n\n\nSchauer, J. M., & Hedges, L. V. (2021). Reconsidering statistical methods for assessing replication. Psychological Methods, 26, 127–139. https://doi.org/10.1037/met0000302\n\n\nStan Development Team. (2021). Stan: A c++ library for probability and sampling, version 2.26.0. https://mc-stan.org/\n\n\nSteiger, J. H. (2004). Beyond the f test: Effect size confidence intervals and tests of close fit in the analysis of variance and contrast analysis. Psychological Methods, 9, 164–182. https://doi.org/10.1037/1082-989X.9.2.164\n\n\nUlrich, R., & Miller, J. (2020). Questionable research practices may have little effect on replicability. eLife, 9, e58237. https://doi.org/10.7554/eLife.58237\n\n\nWasserstein, R. L., & Lazar, N. A. (2016). The ASA statement on p-values: Context, process, and purpose. The American Statistician, 70, 129–133. https://doi.org/10.1080/00031305.2016.1154108\n\n\nWilliams, D. R., Rast, P., & Bürkner, P.-. C. (2018). Bayesian meta-analysis with weakly informative prior distributions. PsyArXiv. https://doi.org/10.31234/osf.io/7tbrm\n\n\nWilson, B. M., & Wixted, J. T. (2018). The prior odds of testing a true effect in cognitive and social psychology. Advances in Methods and Practices in Psychological Science, 1, 186–197. https://doi.org/10.1177/2515245918767122",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Probability of Replication</span>"
    ]
  },
  {
    "objectID": "chapters/chapter3.html#footnotes",
    "href": "chapters/chapter3.html#footnotes",
    "title": "3  Probability of Replication",
    "section": "",
    "text": "To note that the term ROPE refer to the Bayesian version of the equivalence region in the frequentist equivalence testing approach (Lakens et al., 2018). Despite using the Bayesian term, we are just referring to an area where the effect is neglegible and equivalent to a point-null value (e.g, zero).↩︎\nTo calculate the confidence interval of the effect size we used the so-called pivot method (Steiger, 2004) implemented into the effectsize:::.get_ncp_t() function↩︎",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Probability of Replication</span>"
    ]
  },
  {
    "objectID": "chapters/chapter4.html",
    "href": "chapters/chapter4.html",
    "title": "\n4  Statistical methods for replication assessment\n",
    "section": "",
    "text": "4.1 Introduction\nAnderson & Kelley (2024) suggests that also the perspective of supporting the null hypothesis should be taken into account especially when we are really skeptical about the initial results. We want to replicate an effect to see the reliability, maybe better estimating the true effect with a larger sample (taking into account the inflation of the original effect) but we could also think that the original effect is a false positive and we want to do the replication to see if the null hypothesis is supported. The bayes factor approach by Ly et al. (2019) Verhagen & Wagenmakers (2014) is exactly about this. also equivalence testing is useful\nIn the previous chapters we introduced replication from a statistical and theoretical point of view. We purposely omitted defining the outcome of a replication study to dedicate an entire chapter about this part. Similarly to the multiple definitions problem there are several statistical measures for the replication assessment. There are also multiple ways to propose a classification but we could indentify:\nIn addition to the multitude of methods, the field of replication measures is relatively new and keep growing proposing new metrics, simulation studies, and comparisons between methods.\nA review work could be in principle the best option but this is not feasible and also useful for a very active research area. There is an amazing project proposed by Heyard et al. (2024) where an extensive systematic review is supported by an online and keep up-to-date database with all replication measures organized and classified according to a common methodology. The authors reviewed roughly 50 different measures. The database is avaliable at the following link http://rachelheyard.com/reproducibility_metrics/.\nWe decided to make a selection of methods from the database according to the following criteria:\nAn important point is related to the choice of the method when evaluating a replication result. The Heyard et al. (2024) work shows the amount of different methods and definitions of replication success without formally comparing them. This is not a problematic aspect because in practice, large scale replication projects such as the  used different methods for the same dataset. In this context is important clearly use the method, and the related replication definition, closest to our aim.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Statistical methods for replication assessment</span>"
    ]
  },
  {
    "objectID": "chapters/chapter4.html#introduction",
    "href": "chapters/chapter4.html#introduction",
    "title": "\n4  Statistical methods for replication assessment\n",
    "section": "",
    "text": "for similar methods we keep the most recent and general version. For example, if a method \\(x\\) originally developed for one-to-one replication designs has been extended to cover also one-to-many designs we will present the latter.\nwe select both bayesian and frequentist methods\nwe selected methods more related to inference and also methods more focused on estimation of the effect size\nwe selected methods that are not linked to a specific research area or topic. For example in the database there are methods for voxels in fMRI research or methods for metrology.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Statistical methods for replication assessment</span>"
    ]
  },
  {
    "objectID": "chapters/chapter4.html#methods",
    "href": "chapters/chapter4.html#methods",
    "title": "\n4  Statistical methods for replication assessment\n",
    "section": "\n4.2 Methods",
    "text": "4.2 Methods\n\nPrediction interval: replication effect in original 95% prediction interval\nProportion of population effects agreeing in direction with the original\nBayes Factor: Independent Jeffreys-Zellner-Siow BF test (default BF)\nBayes Factor: Equality-of-effect-size BF test\nBayes Factor: Fixed-effect meta-analysis BF Test (Meta-analytic BF) [this probably just a bayesian meta-analysis]\nBayesian Evidence Synthesis (variant: Meta-Analysis Model-based Assessment of replicability (MAMBA))\nBayesian mixture model for reproducibility rate\nConfidence interval: original effect in replication 95% CI (Coverage)\nConfidence interval: replication effect in original 95%CI (Capture probability)\nContinuously cumulating meta-analytic approach\nCorrespondence test\nCredibility analysis (Reverse-Bayes, probability of credibility, probability of replicating an effect)\nDesign analysis [type-m/s]\nEquivalence testing (TOST (two one-sided tests))\nLikelihood-based approach for reproducibility (Likelihood-ratio) [similar to bf?]\nMinimum effect testing [similar to small telescope?]\nP interval\nPrediction interval: replication effect in original 95% prediction interval\nReplication Bayes factor [already included]\nSceptical \\(p\\)-value (versions: nominal sceptical \\(p\\)-value, golden sceptical \\(p\\)-value, controlled sceptical \\(p\\)-value)\nSceptical Bayes Factor (Reverse-Bayes)\nSmall Telescopes\nSnapshot hybrid (Bayesian meta-analysis)\nZ-curve (Exact replication rate, p-curves)\nConsistency of original with replications, \\(P_{\\mbox{orig}}\\)\n\nI squared - \\(I^2\\) (Estimation of effect variance) [this can be togheter with Q and meta-analysis]\nProportion of population effects agreeing in direction with the original, \\(\\hat{P}_{&gt;0}\\)\n\nBland-Altman Plot (Agreement measures)\nCorrelation between effects\nDifference in effect size (Q-statistic, (meta-analytic) Q-test, difference test, Tukey’s post-hoc honest significant difference test)\nExternally standardized residuals [idea di calcolare tipo m su effetto stimato]\nMeta-analysis\nSignificance criterion (vote counting, two-trials rule, regulatory agreement)",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Statistical methods for replication assessment</span>"
    ]
  },
  {
    "objectID": "chapters/chapter4.html#worth-mentioning",
    "href": "chapters/chapter4.html#worth-mentioning",
    "title": "\n4  Statistical methods for replication assessment\n",
    "section": "\n4.3 Worth mentioning",
    "text": "4.3 Worth mentioning\n\nRepliCATS [subjective elicitation]\nText-based machine learning model to estimate reproducibility",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Statistical methods for replication assessment</span>"
    ]
  },
  {
    "objectID": "chapters/chapter4.html#for-me-to-check",
    "href": "chapters/chapter4.html#for-me-to-check",
    "title": "\n4  Statistical methods for replication assessment\n",
    "section": "\n4.4 for me to check",
    "text": "4.4 for me to check\n\nCausal replication framework",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Statistical methods for replication assessment</span>"
    ]
  },
  {
    "objectID": "chapters/chapter4.html#simulating-data",
    "href": "chapters/chapter4.html#simulating-data",
    "title": "\n4  Statistical methods for replication assessment\n",
    "section": "\n4.4 Simulating data",
    "text": "4.4 Simulating data\nWhile real-world examples are important, to understand the replication methods from a statistical point of view, simulating simplified examples is a good strategies. Furthermore, simulating data is nowadays considered an important tool to teach and understand statistical methods . In additions, Monte Carlo simulations are necessary to estimate statistical properties (e.g., statistical power or type-1 error rate) of complex models.\nFor the simulated examples we can define the following simulation approach:\n\nprimary studies always compare two independent groups on a certain response variable\nwithin a study, the two groups are assumed to comes from two normal distributions with unit variance. For one group the mean is centered on zero and for the other group the mean is centered on the value representing the effect size for that specific study.\nthe sample size can vary between the two groups\n\nUsing the same notation as ?sec-prob-replication we can define:\n\n\n\\(y_0\\): as the reference group\n\n\\(y_1\\): as the treated group\n\nFor example we can simulate a single study:\n\nes &lt;- 0.3\nn0 &lt;- 30\nn1 &lt;- 30\n\ny0 &lt;- rnorm(n0, 0, 1)\ny1 &lt;- rnorm(n1, es, 1)\n\nmean(y1) - mean(y0) # this is the effect size\n## [1] 0.1424186\nvar(y1)/n1 + var(y0)/n0 # this is the sampling variability\n## [1] 0.07676008\n\n\n\n\n\n\n\n\n\nNow we can simply iterate the process for \\(R\\) studies to create a series of replications with a common true parameter \\(\\theta\\). We can also generate a more realistic set of sample sizes sampling from a probability distribution (e.g., Poisson). Then, if we want to include variability in the true effects as in the extension framework we can simply sample \\(k\\) \\(\\theta\\)’s from a normal distribution with variability \\(\\tau^2\\).\n\nk &lt;- 10\nmu &lt;- 0.3 # average true effect\ntau2 &lt;- 0.1 # heterogeneity\ntheta &lt;- rnorm(k, mu, sqrt(tau2)) # true effects for R studies\n\nyi &lt;- vi &lt;- rep(NA, k)\nn0 &lt;- n1 &lt;- 10 + rpois(k, 40 - 10)\n\nfor(i in 1:k){\n    y0 &lt;- rnorm(n0[i], 0, 1)\n    y1 &lt;- rnorm(n1[i], theta[i], 1)\n    yi[i] &lt;- mean(y1) - mean(y0)\n    vi[i] &lt;- var(y1)/n1[i] + var(y0)/n0[i]\n}\n\nsim &lt;- data.frame(id = 1:k, yi, vi, n0, n1)\nsim\n\n   id          yi         vi n0 n1\n1   1  0.91536765 0.05998698 33 33\n2   2  0.21592802 0.05646583 41 41\n3   3 -0.28591840 0.04279212 43 43\n4   4  0.17983651 0.04201626 49 49\n5   5  0.43823997 0.04569011 36 36\n6   6 -0.09135959 0.02806265 43 43\n7   7  0.17825811 0.03853985 44 44\n8   8  0.22267042 0.03863560 41 41\n9   9 -0.51402963 0.03605916 34 34\n10 10  0.25534246 0.05626168 36 36\n\n\nThen we can put everything in a function that can be used to simulate different scenarios.\nfilor::print_fun(c(funs$sim_study, funs$sim_studies))\n\n\n\n   id        yi         vi       sei n0 n1\n1   1 0.6161396 0.04730631 0.2175001 30 30\n2   2 0.4824897 0.08190062 0.2861828 30 30\n3   3 0.2956900 0.04957705 0.2226590 30 30\n4   4 0.5864874 0.04592896 0.2143104 30 30\n5   5 0.5801820 0.05745505 0.2396978 30 30\n6   6 0.1790572 0.06244904 0.2498981 30 30\n7   7 0.7640250 0.05946423 0.2438529 30 30\n8   8 0.7043039 0.06151998 0.2480322 30 30\n9   9 0.7016143 0.07994590 0.2827471 30 30\n10 10 0.5599986 0.09518197 0.3085158 30 30",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Statistical methods for replication assessment</span>"
    ]
  },
  {
    "objectID": "chapters/chapter4.html#some-random-concepts-1",
    "href": "chapters/chapter4.html#some-random-concepts-1",
    "title": "\n4  Statistical methods for replication assessment\n",
    "section": "\n6.1 Some (random) concepts",
    "text": "6.1 Some (random) concepts\n\nCredibility of scientific claims is established with evidence for their replicability using new data (Nosek & Errington, 2020)\n\n\nReplication is repeating a study’s procedure and observing whether the prior finding recurs (Jeffreys, 1973)\n\n\nReplication is a study for which any outcome would be considered diagnostic evidence about a claim from prior research (Nosek & Errington, 2020).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Statistical methods for replication assessment</span>"
    ]
  },
  {
    "objectID": "chapters/chapter4.html#difficulty-in-drawing-conclusions-from-replications",
    "href": "chapters/chapter4.html#difficulty-in-drawing-conclusions-from-replications",
    "title": "\n4  Statistical methods for replication assessment\n",
    "section": "\n6.2 Difficulty in drawing conclusions from replications",
    "text": "6.2 Difficulty in drawing conclusions from replications\nReplication is often intended as conditioned to the original result. The original result could be a false positive or a biased result. Also the replication attempt could be a false positive or a false negative (Nosek & Errington, 2020).\n. . .\n\nTo be a replication, two things must be true. Outcomes consistent with a prior claim would increase confidence in the claim, and outcomes inconsistent with a prior claim would decrease confidence in the claim (Nosek & Errington, 2020).\n\n. . .\nThis is somehow similar with a Bayesian reasoning where evidence about a phenomenon is updated after collecting more data.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Statistical methods for replication assessment</span>"
    ]
  },
  {
    "objectID": "chapters/chapter4.html#exact-and-conceptual-replications",
    "href": "chapters/chapter4.html#exact-and-conceptual-replications",
    "title": "\n4  Statistical methods for replication assessment\n",
    "section": "\n6.3 Exact and Conceptual replications",
    "text": "6.3 Exact and Conceptual replications\nExact replications are commonly considered as the gold-standard but in practice (especially in Social Sciences, Psychology, etc.) are rare.\nLet’s imagine, an original study \\(y_{or}\\) finding a result.\n\nReplication \\(y_{rep}\\) with the exact same method find the same result. Replication or not?\n\nReplication \\(y_{rep}\\) with a similar method find the same result. Replication or not?\n\nReplication \\(y_{rep}\\) with similar method did not find the same result. Replication or not?",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Statistical methods for replication assessment</span>"
    ]
  },
  {
    "objectID": "chapters/chapter4.html#direct-and-conceptual-replications-schmidt2009-mq",
    "href": "chapters/chapter4.html#direct-and-conceptual-replications-schmidt2009-mq",
    "title": "\n4  Statistical methods for replication assessment\n",
    "section": "\n6.4 Direct and Conceptual replications (S. Schmidt, 2009)\n",
    "text": "6.4 Direct and Conceptual replications (S. Schmidt, 2009)\n\nA direct replication is defined as the repetition of an experimental procedure.\nA conceptual replication is defined as testing the same hypothesis with different methods.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Statistical methods for replication assessment</span>"
    ]
  },
  {
    "objectID": "chapters/chapter4.html#exact-replications-are-often-impossible-schmidt2009-mq",
    "href": "chapters/chapter4.html#exact-replications-are-often-impossible-schmidt2009-mq",
    "title": "\n4  Statistical methods for replication assessment\n",
    "section": "\n6.5 Exact replications are (often) impossible (S. Schmidt, 2009)\n",
    "text": "6.5 Exact replications are (often) impossible (S. Schmidt, 2009)\n\nLet’s imagine an extreme example: testing the physiological reaction to arousing situation:\n\nThe original study: Experiment with prehistoric reacting to an arousing stimulus\nThe actual replication: It is possible to create the exact situation? Some phenomenon changes overtime, especially people-related phenomenon\n\nExact replication is often not feasible. Even using the same experimental setup (direct replication) does not assure that we are studying the same phenomenon.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Statistical methods for replication assessment</span>"
    ]
  },
  {
    "objectID": "chapters/chapter4.html#as-exact-as-possible",
    "href": "chapters/chapter4.html#as-exact-as-possible",
    "title": "\n4  Statistical methods for replication assessment\n",
    "section": "\n6.6 As Exact as possible…",
    "text": "6.6 As Exact as possible…\nEven when an experiment use almost the same setup of the original study there is a source of unknown uncertainty. Which is the impact of a slightly change in the experimental setup on the actual result?\n\nA study on the human visual system: presenting stimuli on different monitors –&gt; small change with a huge impact\nA study on consumer behavior: participant answering question using a smartphone or a computer –&gt; small but (maybe) irrelevant change\n\nHow to evaluate the actual impact?",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Statistical methods for replication assessment</span>"
    ]
  },
  {
    "objectID": "chapters/chapter4.html#what-we-are-not-going-to-do",
    "href": "chapters/chapter4.html#what-we-are-not-going-to-do",
    "title": "\n4  Statistical methods for replication assessment\n",
    "section": "\n7.1 What we are (not) going to do?",
    "text": "7.1 What we are (not) going to do?\n. . .\n\nI will not present a strictly theoretical and philosophical approach to replication (what is a replication?, what is the most appropriate definition?, etc.). But we can discuss it together :smile:!\n\n. . .\n\nAccording to the replication definitions and problems, we will explore some statistical methods to evaluate a replication success",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Statistical methods for replication assessment</span>"
    ]
  },
  {
    "objectID": "chapters/chapter4.html#overall-model-and-notation-1",
    "href": "chapters/chapter4.html#overall-model-and-notation-1",
    "title": "\n4  Statistical methods for replication assessment\n",
    "section": "\n8.1 Overall model and notation",
    "text": "8.1 Overall model and notation\nFor the purpose of notation and simplicity we can define a meta-analytical-based replication model (Hedges & Schauer, 2019c; Schauer, 2022; Schauer & Hedges, 2021)\n\\[\ny_i = \\mu_{\\theta} + \\delta_i + \\epsilon_i\n\\]\n\\[\n\\delta_i \\sim \\mathcal{N}(0, \\tau^2)\n\\]\n\\[\n\\epsilon_i \\sim \\mathcal{N}(0, \\sigma^2_i)\n\\]",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Statistical methods for replication assessment</span>"
    ]
  },
  {
    "objectID": "chapters/chapter4.html#overall-model-and-notation-2",
    "href": "chapters/chapter4.html#overall-model-and-notation-2",
    "title": "\n4  Statistical methods for replication assessment\n",
    "section": "\n8.2 Overall model and notation",
    "text": "8.2 Overall model and notation\n\nThus each study \\(i\\) out of the number of studies \\(k\\).\n\n\\(\\mu_{\\theta}\\) is the real average effect and \\(\\theta = \\mu_{\\theta} + \\delta_i\\) is the real effect of each study\n\n\\(\\tau^2\\) is the real variance among different studies. When \\(\\tau^2 = 0\\) there is no variability among studies\n\n\\(\\epsilon_i\\) are the sampling errors that depends on \\(\\sigma^2_i\\), the sampling variability of each study\nWe define \\(\\theta_{orig}\\) (or \\(\\theta_1\\)) as the original study and \\(\\theta_{rep_i}\\) (with \\(i\\) from 2 to \\(k\\)) as the replication studies",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Statistical methods for replication assessment</span>"
    ]
  },
  {
    "objectID": "chapters/chapter4.html#simulating-for-learning",
    "href": "chapters/chapter4.html#simulating-for-learning",
    "title": "\n4  Statistical methods for replication assessment\n",
    "section": "\n8.3 Simulating for learning",
    "text": "8.3 Simulating for learning\nFor the examples we are going to simulate studies. Each study comes from a two-groups comparison on a continous outcome:\n\\[\n\\Delta = \\overline{X_1} - \\overline{X_2}\n\\]\n\\[\nSE_{\\Delta} = \\sqrt{\\frac{s^2_1}{n_1} + \\frac{s^2_2}{n_2}}\n\\]\nWith \\(X_{1_j} \\sim \\mathcal{N}(0, 1)\\) and \\(X_{2_j} \\sim \\mathcal{N}(\\Delta, 1)\\)\ncontinue…",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Statistical methods for replication assessment</span>"
    ]
  },
  {
    "objectID": "chapters/chapter4.html#simulating-for-learning-1",
    "href": "chapters/chapter4.html#simulating-for-learning-1",
    "title": "\n4  Statistical methods for replication assessment\n",
    "section": "\n8.4 Simulating for learning",
    "text": "8.4 Simulating for learning\nThus our observed effect sizes \\(y_i\\) is sampled from: \\[\ny_i \\sim \\mathcal{N}(\\mu_\\theta, \\tau^2 + \\frac{1}{n_1} + \\frac{1}{n_2})\n\\]\nWhere \\(\\frac{1}{n_1} + \\frac{1}{n_2}\\) is the sampling variability (\\(\\sigma^2_i\\)).\nThe sampling variances are sampled from:\n\\[\n\\sigma_i^2 \\sim \\frac{\\chi^2_{n_1 + n_2 - 2}}{n_1 + n_2 - 2} (\\frac{1}{n_1} + \\frac{1}{n_2})\n\\]",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Statistical methods for replication assessment</span>"
    ]
  },
  {
    "objectID": "chapters/chapter4.html#simulating-for-learning-2",
    "href": "chapters/chapter4.html#simulating-for-learning-2",
    "title": "\n4  Statistical methods for replication assessment\n",
    "section": "\n8.5 Simulating for learning",
    "text": "8.5 Simulating for learning\nEverything is implemented into the sim_studies() function:\n\nsim_studies &lt;- function(k, theta, tau2, n0, n1, summary = FALSE){\n  yi &lt;- rnorm(k, theta, sqrt(tau2 + 1/n0 + 1/n1))\n  vi &lt;- (rchisq(k, n0 + n1 - 2) / (n0 + n1 - 2)) * (1/n0 + 1/n1)\n  out &lt;- data.frame(yi, vi, sei = sqrt(vi))\n  if(summary){\n    out &lt;- summary_es(out)\n  }\n  return(out)\n}",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Statistical methods for replication assessment</span>"
    ]
  },
  {
    "objectID": "chapters/chapter4.html#simulating-for-learning-an-example",
    "href": "chapters/chapter4.html#simulating-for-learning-an-example",
    "title": "\n4  Statistical methods for replication assessment\n",
    "section": "\n8.6 Simulating for learning, an example",
    "text": "8.6 Simulating for learning, an example\n\nsim_studies(k = 10, theta = 0.5, tau2 = 0.1, n0 = 30, n1 = 30)\n          yi         vi       sei\n1  0.6306046 0.07040783 0.2653447\n2  0.3117390 0.06975489 0.2641115\n3  1.3005623 0.06786796 0.2605148\n4  0.6860657 0.05444358 0.2333315\n5  0.5698951 0.07307152 0.2703174\n6  1.1003673 0.08245891 0.2871566\n7  1.4643882 0.05544487 0.2354673\n8  0.1718634 0.07772710 0.2787958\n9  0.3836479 0.06972331 0.2640517\n10 0.3877660 0.07604360 0.2757600",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Statistical methods for replication assessment</span>"
    ]
  },
  {
    "objectID": "chapters/chapter4.html#exact-vs-approximate-replication",
    "href": "chapters/chapter4.html#exact-vs-approximate-replication",
    "title": "\n4  Statistical methods for replication assessment\n",
    "section": "\n8.7 Exact vs Approximate replication",
    "text": "8.7 Exact vs Approximate replication\nThis distinction (see Brandt et al., 2014 for a different terminology) refers to parameters \\(\\theta_i\\). With exact are considering a case where:\n\\[\n\\theta_1 = \\theta_2 = \\theta_3, \\dots, \\theta_k\n\\]\nThus the true parameters of \\(k\\) replication studies are the same. Thus the variability among true effects \\(\\tau^2 = 0\\).\nSimilarly, due to (often not controllable) differences among experiments (i.e., lab, location, sample, etc.) we could expect a certain degree of variability \\(\\tau^2\\). In other terms \\(\\tau^2 &lt; \\tau^2_0\\) where \\(\\tau^2_0\\) is the maximum variability (that need to be defined). In this way studies are replicating:\n\\[\n\\theta_i \\sim \\mathcal{N}(\\mu_\\theta, \\tau^2_0)\n\\]",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Statistical methods for replication assessment</span>"
    ]
  },
  {
    "objectID": "chapters/chapter4.html#types-of-agreement",
    "href": "chapters/chapter4.html#types-of-agreement",
    "title": "\n4  Statistical methods for replication assessment\n",
    "section": "\n8.8 Types of agreement",
    "text": "8.8 Types of agreement\nCoarsely, we can define a replication success when two or more studies obtain the “same” result. The definion of sameness it is crucial:\n\nsame sign or direction: two studies (original and replication) evaluating the efficacy of a treatment have a positive effect \\(sign(\\theta_1) = sign(\\theta_2)\\) where \\(sign\\) is the sign function.\nsame magnitude: two studies (original and replication) evaluating the efficacy of a treatment have the same effect in terms \\(|\\theta_1 - \\theta_2| = 0\\) or similar up to a tolerance factor \\(|\\theta_1 - \\theta_2| &lt; \\gamma\\) where \\(\\gamma\\) is the maximum difference considered as null.\n\nThe different methods that we are going to see are focused on a specific type of aggreement. For example, we could consider \\(\\theta_1 = 3x\\) and \\(\\theta_2 = x\\) to have the same sign but the replication study is on a completely different scale. Is this considered a successful replication?",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Statistical methods for replication assessment</span>"
    ]
  },
  {
    "objectID": "chapters/chapter4.html#falsification-vs-consistency",
    "href": "chapters/chapter4.html#falsification-vs-consistency",
    "title": "\n4  Statistical methods for replication assessment\n",
    "section": "\n8.9 Falsification vs Consistency",
    "text": "8.9 Falsification vs Consistency\n. . .\nThis refers to how the replication setup is formulated. With \\(k = 2\\) studies where \\(k_1\\) is the original study and \\(k_2\\) is the replication we have a one-to-one setup. In this setup we compare the replication with the original and according to the chosen method and expectation we conclude if \\(k_1\\) has been replicated or not.\n. . .\nWhen \\(k &gt; 2\\) we could collapse the replication studies into a single value (e.g., using a meta-analysis method) and compare the results using a one-to-one or we can use a method for one-to-many designs.\n. . .\nRegardless the method, falsification approaches compared the original with the replicate(s) obtaining a yes-no answer or a continuous result. On the other side consistency methods are focused on evaluating the degree of similarity (i.e., consistency) among all studies.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Statistical methods for replication assessment</span>"
    ]
  },
  {
    "objectID": "chapters/chapter4.html#the-big-picture",
    "href": "chapters/chapter4.html#the-big-picture",
    "title": "\n4  Statistical methods for replication assessment\n",
    "section": "\n8.10 The big picture",
    "text": "8.10 The big picture",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Statistical methods for replication assessment</span>"
    ]
  },
  {
    "objectID": "chapters/chapter4.html#statistical-methods-disclaimer-schauer2021-ja",
    "href": "chapters/chapter4.html#statistical-methods-disclaimer-schauer2021-ja",
    "title": "\n4  Statistical methods for replication assessment\n",
    "section": "\n9.1 Statistical Methods, disclaimer (Schauer & Hedges, 2021)\n",
    "text": "9.1 Statistical Methods, disclaimer (Schauer & Hedges, 2021)\n\n\nThere are no unique methods to assess replication from a statistical point of view\nFor available statistical methods, statistical properties (e.g., type-1 error rate, power, bias, etc.) are not always known or extensively examined\nDifferent methods answers to the same question or to different replication definitions",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Statistical methods for replication assessment</span>"
    ]
  },
  {
    "objectID": "chapters/chapter4.html#vote-counting-based-on-significance-or-direction",
    "href": "chapters/chapter4.html#vote-counting-based-on-significance-or-direction",
    "title": "\n4  Statistical methods for replication assessment\n",
    "section": "\n4.5 Vote Counting based on significance or direction",
    "text": "4.5 Vote Counting based on significance or direction\nThe simplest method is called vote counting (Hedges & Olkin, 1980; Valentine et al., 2011). A replication attempt \\(\\theta_{rep}\\) is considered successful if the result has the same direction of the original study \\(\\theta_{orig}\\) and it is statistically significant i.e., \\(p_{\\theta_{rep}} \\leq \\alpha\\). Similarly we can count the number of replication with the same sign as the original study.\n\n\nEasy to understand, communicate and compute\n\n\n\n\nDid not consider the size of the effect\nDepends on the power of \\(\\theta_{rep}\\)\n\n\n\nLet’s simulate an exact replication:\n\n## original study\nn_orig &lt;- 30\ntheta_orig &lt;- theta_from_z(2, n_orig)\n\norig &lt;- data.frame(\n  yi = theta_orig,\n  vi = 4/(n_orig*2)\n)\n\norig$sei &lt;- sqrt(orig$vi)\norig &lt;- summary_es(orig)\n\norig\n\n         yi         vi       sei zi       pval      ci.lb    ci.ub\n1 0.5163978 0.06666667 0.2581989  2 0.04550026 0.01033725 1.022458\n\n## replications\n\nR &lt;- 10\nreps &lt;- sim_studies(R = R, \n                    mu = theta_orig, \n                    tau2 = 0, \n                    n_orig)\n\nreps &lt;- summary_es(reps)\n\nhead(reps)\n\n  id        yi         vi       sei n0 n1        zi         pval        ci.lb\n1  1 0.9919289 0.07862573 0.2804028 30 30 3.5375143 0.0004039124  0.442349502\n2  2 0.5309135 0.07153431 0.2674590 30 30 1.9850275 0.0471414070  0.006703475\n3  3 0.1814660 0.08390345 0.2896609 30 30 0.6264772 0.5310019605 -0.386258990\n4  4 0.6880203 0.06852721 0.2617770 30 30 2.6282684 0.0085820778  0.174946731\n5  5 0.4598906 0.07557917 0.2749167 30 30 1.6728363 0.0943595228 -0.078936178\n6  6 0.7422399 0.05782128 0.2404606 30 30 3.0867426 0.0020236275  0.270945815\n      ci.ub\n1 1.5415083\n2 1.0551234\n3 0.7491909\n4 1.2010938\n5 0.9987173\n6 1.2135339\n\n\nLet’s compute the proportions of replication studies are statistically significant:\n\nmean(reps$pval &lt;= 0.05)\n\n[1] 0.6\n\n\nLet’s compute the proportions of replication studies with the same sign as the original:\n\nmean(sign(orig$yi) == sign(reps$yi))\n\n[1] 1\n\n\nWe could also perform some statistical tests. See Bushman & Wang (2009) and Hedges & Olkin (1980) for vote-counting methods in meta-analysis.\nAn extreme example:\nLet’s imagine an original experiment with \\(n_{orig} = 30\\) and \\(\\hat \\theta_{orig} = 0.5\\) that is statistically significant \\(p \\approx 0.045\\). Now a direct replication (thus assuming \\(\\tau^2 = 0\\)) study with \\(n_{rep} = 350\\) found \\(\\hat \\theta_{rep_1} = 0.15\\), that is statistically significant \\(p\\approx 0.047\\).\n\n\n\n\n\n\n\n\nThe most problematic aspect of using only the information from the sign of the effect or the p value is completely losing the information about the size of the effect and the precision.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Statistical methods for replication assessment</span>"
    ]
  },
  {
    "objectID": "chapters/chapter4.html#example-with-simulated-data",
    "href": "chapters/chapter4.html#example-with-simulated-data",
    "title": "\n4  Statistical methods for replication assessment\n",
    "section": "\n10.2 Example with simulated data",
    "text": "10.2 Example with simulated data\nLet’s simulate an exact replication:\n\n## original study\nn_orig &lt;- 30\ntheta_orig &lt;- theta_from_z(2, n_orig)\n\norig &lt;- data.frame(\n  yi = theta_orig,\n  vi = 4/(n_orig*2)\n)\n\norig$sei &lt;- sqrt(orig$vi)\norig &lt;- summary_es(orig)\n\norig\n         yi         vi       sei zi       pval      ci.lb    ci.ub\n1 0.5163978 0.06666667 0.2581989  2 0.04550026 0.01033725 1.022458\n\n## replications\n\nk &lt;- 10\nreps &lt;- sim_studies(k = k, theta = theta_orig, tau2 = 0, n_orig, n_orig, summary = TRUE)\n\nhead(reps)\n         yi         vi       sei       zi         pval       ci.lb     ci.ub\n1 0.8797180 0.06597473 0.2568555 3.424953 0.0006149050  0.37629053 1.3831455\n2 0.7245546 0.04843434 0.2200780 3.292262 0.0009938480  0.29320963 1.1558996\n3 0.4111667 0.04870422 0.2206903 1.863094 0.0624491032 -0.02137836 0.8437118\n4 0.7085433 0.06511996 0.2551861 2.776574 0.0054935065  0.20838765 1.2086989\n5 0.8810186 0.05914530 0.2431981 3.622638 0.0002916136  0.40435914 1.3576780\n6 0.3968792 0.06680153 0.2584599 1.535554 0.1246477547 -0.10969292 0.9034514",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Statistical methods for replication assessment</span>"
    ]
  },
  {
    "objectID": "chapters/chapter4.html#example-with-simulated-data-1",
    "href": "chapters/chapter4.html#example-with-simulated-data-1",
    "title": "\n4  Statistical methods for replication assessment\n",
    "section": "\n10.3 Example with simulated data",
    "text": "10.3 Example with simulated data\nLet’s compute the proportions of replication studies are statistically significant:\n\nmean(reps$pval &lt;= 0.05)\n[1] 0.6\n\n\nLet’s compute the proportions of replication studies with the same sign as the original:\n\nmean(sign(orig$yi) == sign(reps$yi))\n[1] 1\n\n\nWe could also perform some statistical tests. See Bushman & Wang (2009) and Hedges & Olkin (1980) for vote-counting methods in meta-analysis.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Statistical methods for replication assessment</span>"
    ]
  },
  {
    "objectID": "chapters/chapter4.html#vote-counting-extreme-example",
    "href": "chapters/chapter4.html#vote-counting-extreme-example",
    "title": "\n4  Statistical methods for replication assessment\n",
    "section": "\n10.4 Vote Counting, extreme example",
    "text": "10.4 Vote Counting, extreme example\nLet’s imagine an original experiment with \\(n_{orig} = 30\\) and \\(\\hat \\theta_{orig} = 0.5\\) that is statistically significant \\(p \\approx 0.045\\). Now a direct replication (thus assuming \\(\\tau^2 = 0\\)) study with \\(n_{rep} = 350\\) found \\(\\hat \\theta_{rep_1} = 0.15\\), that is statistically significant \\(p\\approx 0.047\\).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Statistical methods for replication assessment</span>"
    ]
  },
  {
    "objectID": "chapters/chapter4.html#confidence-interval-replication-within-original",
    "href": "chapters/chapter4.html#confidence-interval-replication-within-original",
    "title": "\n4  Statistical methods for replication assessment\n",
    "section": "\n10.5 Confidence Interval, replication within original",
    "text": "10.5 Confidence Interval, replication within original\n\n\nTheory\nPlot\n\n\n\nAnother approach check if the replication attempt \\(\\theta_{rep}\\) is contained in the % confidence interval of the original study \\(\\theta_{orig}\\). Formally:\n\\[\n\\theta_{orig} - \\Phi(\\alpha/2) \\sqrt{\\sigma^2_{orig}} &lt; \\theta_{rep} &lt; \\theta_{orig} + \\Phi(\\alpha/2) \\sqrt{\\sigma^2_{orig}}\n\\]\nWhere \\(\\Phi\\) is the cumulative standard normal distribution, \\(\\alpha\\) is the type-1 error rate.\n\n\nTake into account the size of the effect and the precision of \\(\\theta_{orig}\\)\n\n\n\n\n\nThe original study is assumed to be a reliable estimation\nNo extension for many-to-one designs\nLow precise original studies lead to higher success rate\n\n\n\n\n\nse_orig &lt;- sqrt(4 / (2 * n_orig))\nci_orig &lt;- theta_orig + qnorm(c(0.025, 0.975)) * se_orig\ncurve(dnorm(x, theta_orig, se_orig), \n      -1, 2, \n      ylab = \"Density\", \n      xlab = latex2exp::TeX(\"$\\\\theta$\"))\nabline(v = ci_orig, lty = \"dashed\")\npoints(theta_orig, 0, pch = 19, cex = 2)\npoints(theta_rep, 0, pch = 19, cex = 2, col = \"firebrick\")\nlegend(\"topleft\", \n       legend = c(\"Original\", \"Replication\"), \n       fill = c(\"black\", \"firebrick\"))",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Statistical methods for replication assessment</span>"
    ]
  },
  {
    "objectID": "chapters/chapter4.html#confidence-interval-replication-within-original-1",
    "href": "chapters/chapter4.html#confidence-interval-replication-within-original-1",
    "title": "\n4  Statistical methods for replication assessment\n",
    "section": "\n10.6 Confidence Interval, replication within original",
    "text": "10.6 Confidence Interval, replication within original\nOne potential problem of this method regards that low precise original studies are “easier” to replicate due to larger confidence intervals.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Statistical methods for replication assessment</span>"
    ]
  },
  {
    "objectID": "chapters/chapter4.html#confidence-interval-original-within-replication",
    "href": "chapters/chapter4.html#confidence-interval-original-within-replication",
    "title": "\n4  Statistical methods for replication assessment\n",
    "section": "\n10.7 Confidence Interval, original within replication",
    "text": "10.7 Confidence Interval, original within replication\n\n\nTheory\nPlot\n\n\n\nThe same approach can be applied checking if the original effect size is contained within the replication confidence interval. Clearly these methods depends on the precision of studies. Formally:\n\\[\n\\theta_{rep} - \\Phi(\\alpha/2) \\sqrt{\\sigma_{rep}^2} &lt; \\theta_{orig} &lt; \\theta_{rep} + \\Phi(\\alpha/2) \\sqrt{\\sigma_{rep}^2}\n\\]\nThe method has the same pros and cons of the previous approach. One advantage is that usually replication studies are more precise (higher sample size) thus the parameter and the % CI is more reliable.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Statistical methods for replication assessment</span>"
    ]
  },
  {
    "objectID": "chapters/chapter4.html#prediction-interval-pi-what-to-expect-from-a-replication",
    "href": "chapters/chapter4.html#prediction-interval-pi-what-to-expect-from-a-replication",
    "title": "\n4  Statistical methods for replication assessment\n",
    "section": "\n10.8 Prediction interval (PI), what to expect from a replication",
    "text": "10.8 Prediction interval (PI), what to expect from a replication\nOne problem of the previous approaches is taking into account only the uncertainty of the original or the replication study. Patil et al. (2016) and Spence & Stanley (2016) proposed a method to take into account both sources of uncertainty.\nIf the original and replication studies comes from the same population, the sampling distribution of the difference is centered on 0 with a certain standard error \\(\\theta_{orig} - \\theta_{rep_0} \\sim \\mathcal{N}\\left( 0, \\sqrt{\\sigma^2_{\\hat \\theta_{orig} - \\hat \\theta_{rep}}} \\right)\\) (subscript \\(0\\) to indicate that is expected to be sampled from the same population as \\(\\theta_{orig}\\))\n\\[\n\\hat \\theta_{orig} \\pm z_{95\\%} \\sqrt{\\sigma^2_{\\theta_{orig} - \\theta_{rep}}}\n\\]\nIf factors other than standard error influence the replication result, \\(\\theta_{rep_0}\\) is not expected to be contained within the 95% prediction interval.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Statistical methods for replication assessment</span>"
    ]
  },
  {
    "objectID": "chapters/chapter4.html#prediction-interval-pi-what-to-expect-from-a-replication-1",
    "href": "chapters/chapter4.html#prediction-interval-pi-what-to-expect-from-a-replication-1",
    "title": "\n4  Statistical methods for replication assessment\n",
    "section": "\n10.9 Prediction interval (PI), what to expect from a replication",
    "text": "10.9 Prediction interval (PI), what to expect from a replication\nIn the case of a (un)standardized mean difference we can compute the prediction interval as:\n\\[\n\\sqrt{\\sigma^2_{\\epsilon_{\\hat \\theta_{orig} - \\hat \\theta_{rep_0}}}} = \\sqrt{\\left( \\frac{\\hat \\sigma^2_{o1}}{n_{o1}} +\\frac{\\hat \\sigma^2_{o2}}{n_{o2}}\\right) + \\left(\\frac{\\hat \\sigma^2_{o1}}{n_{r1}} + \\frac{\\hat\\sigma^2_{o2}}{n_{r2}}\\right)}\n\\]\nThe first term is just the standard error of the difference between the two groups in the original study and the second term is the standard error of the hypothetical replication study assuming the same standard deviation of the original but a different \\(n\\).\nIn this way we estimate an interval where, combining sampling variance from both studies and assuming that they comes from the same population, the replication should fall.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Statistical methods for replication assessment</span>"
    ]
  },
  {
    "objectID": "chapters/chapter4.html#prediction-interval-pi-what-to-expect-from-a-replication-2",
    "href": "chapters/chapter4.html#prediction-interval-pi-what-to-expect-from-a-replication-2",
    "title": "\n4  Statistical methods for replication assessment\n",
    "section": "\n10.10 Prediction interval (PI), what to expect from a replication",
    "text": "10.10 Prediction interval (PI), what to expect from a replication\n\n\nR Code\nPlot\nPros/Cons\n\n\n\n\nset.seed(2025)\n\no1 &lt;- rnorm(50, 0.5, 1) # group 1\no2 &lt;- rnorm(50, 0, 1) # group 2\nod &lt;- mean(o1) - mean(o2) # effect size\nse_o &lt;- sqrt(var(o1)/50 + var(o2)/50) # standard error of the difference\n\nn_r &lt;- 100 # sample size replication\n\nse_o_r &lt;- sqrt(se_o^2 + (var(o1)/100 + var(o2)/100))\n\nod + qnorm(c(0.025, 0.975)) * se_o_r\n[1] 0.325520 1.298378\n\n\n\n\n\nCodepar(mar = c(4, 4, 0.1, 0.1))  \ncurve(dnorm(x, od, se_o_r), od - se_o_r*4, od + se_o_r*4, lwd = 2, xlab = latex2exp::TeX(\"$\\\\theta$\"), ylab = \"Density\")\nabline(v = od + qnorm(c(0.025, 0.975)) * se_o_r, lty = \"dashed\")\n\n\n\n\n\n\n\n\n\n\n\nTake into account uncertainty of both studies\nWe can plan a replication using the standard deviation of the original study and the expected sample size\n\n\n\n\nLow precise original studies lead to wide PI. For a replication study is difficult to fall outside the PI\nMainly for one-to-one replications design",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Statistical methods for replication assessment</span>"
    ]
  },
  {
    "objectID": "chapters/chapter4.html#mathur-vanderweele--mathur2020-nw-p_orig",
    "href": "chapters/chapter4.html#mathur-vanderweele--mathur2020-nw-p_orig",
    "title": "\n4  Statistical methods for replication assessment\n",
    "section": "\n10.11 Mathur & VanderWeele (2020) \\(p_{orig}\\)\n",
    "text": "10.11 Mathur & VanderWeele (2020) \\(p_{orig}\\)\n\n\n\nTheory\nPros-cons\nR Code\nSimulation\n\n\n\nMathur & VanderWeele (2020) proposed a new method based on the prediction interval to calculate a p value \\(p_{orig}\\) representing the probability that \\(\\theta_{orig}\\) is consistent with the replications. This method is suited for many-to-one replication designs. Formally:\n\\[\nP_{orig} = 2 \\left[ 1 - \\Phi \\left( \\frac{|\\hat \\theta_{orig} - \\hat \\mu_{\\theta_{rep}}|}{\\sqrt{\\hat \\tau^2 + \\sigma^2_{orig} + \\hat{SE}^2_{\\hat \\mu_{\\theta_{rep}}}}} \\right) \\right]\n\\]\n\n\n\\(\\mu_{\\theta_{rep}}\\) is the pooled (i.e., meta-analytic) estimation of the \\(k\\) replications\n\n\\(\\tau^2\\) is the variance among replications\n\n\n\nIt is interpreted as the probability that \\(\\theta_{orig}\\) is equal or more extreme that what observed. A very low \\(p_{orig}\\) suggest that the original study is inconsistent with replications.\n\n\nSuited for many-to-one designs\nWe take into account all sources of uncertainty\nWe have a p-value\n\n\n\n\nThe code is implemented in the Replicate and MetaUtility R packages:\n\ntau2 &lt;- 0.05\ntheta_rep &lt;- 0.2\ntheta_orig &lt;- 0.7\n\nn_orig &lt;- 30\nn_rep &lt;- 100\nk &lt;- 20\n\nreplications &lt;- sim_studies(k, theta_rep, tau2, n_rep, n_rep)\noriginal &lt;- sim_studies(1, theta_orig, 0, n_orig, n_orig)\n\nfit_rep &lt;- metafor::rma(yi, vi, data = replications) # random-effects meta-analysis\n\nReplicate::p_orig(original$yi, original$vi, fit_rep$b[[1]], fit_rep$tau2, fit_rep$se^2)\n[1] 0.5563241\n\n\n\n\n\nCode# standard errors assuming same n and variance 1\nse_orig &lt;- sqrt(4/(n_orig * 2))\nse_rep &lt;- sqrt(4/(n_rep * 2))\nse_theta_rep &lt;- sqrt(1/((1/(se_rep^2 + tau2)) * k)) # standard error of the random-effects estimate\n\nsep &lt;- sqrt(tau2 + se_orig^2 + se_theta_rep^2) # z of p-orig denominator\n\ncurve(dnorm(x, theta_rep, sep), theta_rep - 4*sep, theta_rep + 4*sep, ylab = \"Density\", xlab = latex2exp::TeX(\"\\\\theta\"))\npoints(theta_orig, 0.02, pch = 19, cex = 2)\nabline(v = qnorm(c(0.025, 0.975), theta_rep, sep), lty = \"dashed\", col = \"firebrick\")",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Statistical methods for replication assessment</span>"
    ]
  },
  {
    "objectID": "chapters/chapter4.html#mathur-vanderweele--mathur2020-nw-hat-p_-0",
    "href": "chapters/chapter4.html#mathur-vanderweele--mathur2020-nw-hat-p_-0",
    "title": "\n4  Statistical methods for replication assessment\n",
    "section": "\n10.12 Mathur & VanderWeele (2020) \\(\\hat P_{> 0}\\)\n",
    "text": "10.12 Mathur & VanderWeele (2020) \\(\\hat P_{&gt; 0}\\)\n\n\n\nTheory\nR Code\nBootstrap Code\nBootstrap Results\n\n\n\nAnother related metric is the \\(\\hat P_{&gt; 0}\\), representing the proportion of replications following the same direction as the original effect. Before simply computing the proportions we need to adjust the estimated \\(\\theta_{rep_i}\\) with a shrinkage factor:\n\\[\n\\tilde{\\theta}_{rep_i} = (\\theta_{rep_i} - \\mu_{\\theta_{rep_i}}) \\sqrt{\\frac{\\hat \\tau^2}{\\hat \\tau^2 + v_{rep_i}}}\n\\]\nThis method is somehow similar to the vote counting but we are adjusting the effects taking into account \\(\\tau^2\\).\n\n\n\n# compute calibrated estimation for the replications\n# use restricted maximum likelihood to estimate tau2 under the hood\ntheta_sh &lt;- MetaUtility::calib_ests(replications$yi, replications$sei, method = \"REML\")\nmean(theta_sh &gt; 0)\n[1] 0.75\n\n\n\n\nThe authors suggest a bootstrapping approach for making inference on \\(\\hat P_{&gt; 0}\\)\n\nnboot &lt;- 1e4\ntheta_boot &lt;- matrix(0, nrow = nboot, ncol = k)\n\nfor(i in 1:nboot){\n  idx &lt;- sample(1:nrow(replications), nrow(replications), replace = TRUE)\n  replications_boot &lt;- replications[idx, ]\n  theta_cal &lt;- MetaUtility::calib_ests(replications_boot$yi, \n                                       replications_boot$sei, \n                                       method = \"REML\")\n  theta_boot[i, ] &lt;- theta_cal\n}\n\n# calculate\np_greater_boot &lt;- apply(theta_boot, 1, function(x) mean(x &gt; 0))",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Statistical methods for replication assessment</span>"
    ]
  },
  {
    "objectID": "chapters/chapter4.html#mathur-vanderweele--mathur2020-nw-hat-p_gtrless-q",
    "href": "chapters/chapter4.html#mathur-vanderweele--mathur2020-nw-hat-p_gtrless-q",
    "title": "\n4  Statistical methods for replication assessment\n",
    "section": "\n10.13 Mathur & VanderWeele (2020) \\(\\hat P_{\\gtrless q*}\\)\n",
    "text": "10.13 Mathur & VanderWeele (2020) \\(\\hat P_{\\gtrless q*}\\)\n\nInstead of using 0 as threshold, we can use meaningful effect size to be considered as low but different from 0. \\(\\hat P_{\\gtrless q*}\\) is the proportion of (calibrated) replications greater or lower than the \\(q*\\) value. This framework is similar to equivalence and minimum effect size testing (Lakens et al., 2018).\n\nq &lt;- 0.2 # minimum non zero effect\n\nfit &lt;- metafor::rma(yi, vi, data = replications)\n\n# see ?MetaUtility::prop_stronger\nMetaUtility::prop_stronger(q = q,\n                           M = fit$b[[1]],\n                           t2 = fit$tau2,\n                           tail = \"above\",\n                           estimate.method = \"calibrated\",\n                           ci.method = \"calibrated\",\n                           dat = replications,\n                           yi.name = \"yi\",\n                           vi.name = \"vi\")\n   est        se  lo   hi  bt.mn shapiro.pval\n1 0.35 0.1137901 0.1 0.55 0.3573    0.5224829",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Statistical methods for replication assessment</span>"
    ]
  },
  {
    "objectID": "chapters/chapter4.html#combining-original-and-replications",
    "href": "chapters/chapter4.html#combining-original-and-replications",
    "title": "\n4  Statistical methods for replication assessment\n",
    "section": "\n10.14 Combining original and replications",
    "text": "10.14 Combining original and replications\n\n\nTheory\nFixed-effects Model\nRandom-Effects model\nPooling replications\n\n\n\nAnother approach is to combine the original and replication results (both one-to-one and many-to-one) using a meta-analysis model. Then we can test if the pooled estimate is different from 0 or another meaningful value.\n\n\nUse all the available information, especially when fitting a random-effects model\nTake into account the precision by inverse-variance weighting\n\n\n\n\nDid not consider the publication bias\nFor one-to-one designs only a fixed-effects model can be used\n\n\n\n\n\n# fixed-effects\nfit_fixed &lt;- rma(yi, vi, method = \"FE\")\nsummary(fit_fixed)\n\nFixed-Effects Model (k = 20)\n\n  logLik  deviance       AIC       BIC      AICc   \n 20.7415   -0.0000  -39.4829  -38.4872  -39.2607   \n\nI^2 (total heterogeneity / total variability):   0.00%\nH^2 (total variability / sampling variability):  0.00\n\nTest for Heterogeneity:\nQ(df = 19) = 0.0000, p-val = 1.0000\n\nModel Results:\n\nestimate      se    zval    pval   ci.lb   ci.ub      \n  0.2000  0.0316  6.3246  &lt;.0001 signif. codes:&gt;\n\n\n\n\n# fixed-effects\nfit_random &lt;- rma(yi, vi, method = \"REML\")\nsummary(fit_random)\n\nRandom-Effects Model (k = 20; tau^2 estimator: REML)\n\n  logLik  deviance       AIC       BIC      AICc   \n 19.7044  -39.4088  -35.4088  -33.5199  -34.6588   \n\ntau^2 (estimated amount of total heterogeneity): 0 (SE = 0.0065)\ntau (square root of estimated tau^2 value):      0\nI^2 (total heterogeneity / total variability):   0.00%\nH^2 (total variability / sampling variability):  1.00\n\nTest for Heterogeneity:\nQ(df = 19) = 0.0000, p-val = 1.0000\n\nModel Results:\n\nestimate      se    zval    pval   ci.lb   ci.ub      \n  0.2000  0.0316  6.3246  &lt;.0001 signif. codes:&gt;\n\n\n\nThe previous approach can be also implemented combining replications into a single effect and then compare the original with the combined replication study.\nThis is similar to using the CI or PI approaches but the replication effect will probably by very precise due to pooling multiple studies.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Statistical methods for replication assessment</span>"
    ]
  },
  {
    "objectID": "chapters/chapter4.html#q-statistics",
    "href": "chapters/chapter4.html#q-statistics",
    "title": "\n4  Statistical methods for replication assessment\n",
    "section": "\n10.15 Q Statistics",
    "text": "10.15 Q Statistics\nAn interesting proposal is using the Q statistics (Hedges & Schauer, 2019a, 2019b, 2019c, 2021; Schauer, 2022; Schauer & Hedges, 2020; Schauer & Hedges, 2021), commonly used in meta-analysis to assess the presence of heterogeneity. Formally:\n\\[\nQ = \\sum_{i = 1}^{k} \\frac{(\\theta_i - \\bar \\theta_w)^2}{\\sigma^2_i}\n\\]\nWhere \\(\\bar \\theta_w\\) is the inverse-variance weighted average (i.g., fixed-effect model). The Q statistics is essentially a weighted sum of squares. Under the null hypothesis where all studies are equal \\(\\theta_1 = \\theta_2, ... = \\theta_i\\) the Q statistics has a \\(\\chi^2\\) distribution with \\(k - 1\\) degrees of freedom. Under the alternative hypothesis the distribution is a non-central \\(\\chi^2\\) with non centrality parameter \\(\\lambda\\). The expected value of the \\(Q\\) is \\(E(Q) = v + \\lambda\\), where \\(v\\) are the degrees of freedom.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Statistical methods for replication assessment</span>"
    ]
  },
  {
    "objectID": "chapters/chapter4.html#q-statistics-1",
    "href": "chapters/chapter4.html#q-statistics-1",
    "title": "\n4  Statistical methods for replication assessment\n",
    "section": "\n10.16 Q Statistics",
    "text": "10.16 Q Statistics\nHedges & Schauer proposed to use the Q statistics to evaluate the consistency of a series of replications:\n\nIn case of exact replication, \\(\\lambda = 0\\) because \\(\\theta_1 = \\theta_2, ... = \\theta_k\\).\nIn case of approximate replication, \\(\\lambda &lt; \\lambda_0\\) where \\(\\lambda_0\\) is the maximum value considered as equal to null (i.e., 0).\n\nThis approach is testing the consistency (i.e., homogeneity) of replications. A successful replication should minimize the heterogeneity and the presence of a significant Q statistics should bring evidence for not replicating the effect1.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Statistical methods for replication assessment</span>"
    ]
  },
  {
    "objectID": "chapters/chapter4.html#q-statistics-2",
    "href": "chapters/chapter4.html#q-statistics-2",
    "title": "\n4  Statistical methods for replication assessment\n",
    "section": "\n10.17 Q Statistics",
    "text": "10.17 Q Statistics\nThe method has been expanded and formalized in several papers with different objectives:\n. . .\n\nto cover different replications setup (burden of proof on replicating vs non-replicating, many-to-one and one-to-one, etc.)\n\n. . .\n\ninterpret and choose the \\(\\lambda\\) parameter given that is the core of the approach\n\n. . .\n\nevaluating the power and statistical properties under different replication scenarios\n\n. . .\n\nthe standard implementation put the burden of proof on non-replication. Thus \\(H_0\\) is that studies replicates. They provided also a series of tests with the opposite formulation.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Statistical methods for replication assessment</span>"
    ]
  },
  {
    "objectID": "chapters/chapter4.html#q-statistics-3",
    "href": "chapters/chapter4.html#q-statistics-3",
    "title": "\n4  Statistical methods for replication assessment\n",
    "section": "\n10.18 Q Statistics",
    "text": "10.18 Q Statistics\nIn the case of evaluating an exact replication we can use the Qrep() function that simply calculate the p-value based on the Q sampling distribution.\n\n\nFunction\nCode\nPlot\n\n\n\n\nQrep &lt;- function(yi, vi, lambda0 = 0, alpha = 0.05){\n  fit &lt;- metafor::rma(yi, vi)\n  k &lt;- fit$k\n  Q &lt;- fit$QE\n  df &lt;- k - 1\n  Qp &lt;- pchisq(Q, df = df, ncp = lambda0, lower.tail = FALSE)\n  pval &lt;- ifelse(Qp &lt; 0.001, \"p &lt; 0.001\", sprintf(\"p = %.3f\", Qp))\n  lambda &lt;- ifelse((Q - df) &lt; 0, 0, (Q - df))\n  res &lt;- list(Q = Q, lambda = lambda, pval = Qp, df = df, k = k, alpha = alpha, lambda0 = lambda0)\n  H0 &lt;- ifelse(lambda0 != 0, paste(\"H0: lambda &lt;\", lambda0), \"H0: lambda = 0\")\n  title &lt;- ifelse(lambda0 != 0, \"Q test for Approximate Replication\", \"Q test for Exact Replication\")\n  cli::cli_rule()\n  cat(cli::col_blue(cli::style_bold(title)), \"\\n\\n\")\n  cat(sprintf(\"Q = %.3f (df = %s), lambda = %.3f, %s\", res$Q, res$df, lambda, pval), \"\\n\")\n  cat(H0, \"\\n\")\n  cli::cli_rule()\n  class(res) &lt;- \"Qrep\"\n  invisible(res)\n}\n\n\n\n\nQ test for Exact Replication \n\nQ = 367.321 (df = 99), lambda = 268.321, p \n\n\nQres &lt;- Qrep(dat$yi, dat$vi)\nQ test for Exact Replication \n\nQ = 367.321 (df = 99), lambda = 268.321, p \n\n\n\n\nplot.Qrep(Qres)",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Statistical methods for replication assessment</span>"
    ]
  },
  {
    "objectID": "chapters/chapter4.html#q-statistics-for-approximate-replication",
    "href": "chapters/chapter4.html#q-statistics-for-approximate-replication",
    "title": "\n4  Statistical methods for replication assessment\n",
    "section": "\n10.19 Q Statistics for approximate replication",
    "text": "10.19 Q Statistics for approximate replication\n\nIn case of approximate replication we need to set \\(\\lambda_0\\) to a meaningful value but the overall test is the same. The critical \\(Q\\) is no longer evaluated with a central \\(\\chi^2\\) but a non-central \\(\\chi^2\\) with \\(\\lambda_0\\) as non-centrality parameter.\nHedges & Schauer (2019c) provide different strategies to choose \\(\\lambda_0\\). They found that under some assumptions, \\(\\lambda = (k - 1) \\frac{\\tau^2}{\\tilde{v}}\\)\nGiven that we introduced the \\(I^2\\) statistics we can derive a \\(\\lambda_0\\) based in \\(I^2\\). F. L. Schmidt & Hunter (2014) proposed that when \\(\\tilde{v}\\) is at least 75% of total variance \\(\\tilde{v} + \\tau^2\\) thus \\(\\tau^2\\) could be considered neglegible. This corresponds to a \\(I^2 = 25%\\) and a ratio \\(\\frac{\\tau^2}{\\tilde{v}} = 1/3\\) thus \\(\\lambda_0 = \\frac{(k - 1)}{3}\\) can be considered a neglegible heterogeneity",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Statistical methods for replication assessment</span>"
    ]
  },
  {
    "objectID": "chapters/chapter4.html#q-statistics-for-approximate-replication-1",
    "href": "chapters/chapter4.html#q-statistics-for-approximate-replication-1",
    "title": "\n4  Statistical methods for replication assessment\n",
    "section": "\n10.20 Q Statistics for approximate replication",
    "text": "10.20 Q Statistics for approximate replication\n\nk &lt;- 100\ndat &lt;- sim_studies(k, 0.5, 0, 50, 50)\nQrep(dat$yi, dat$vi, lambda0 = (k - 1)/3)\nQ test for Approximate Replication \n\nQ = 98.121 (df = 99), lambda = 0.000, p = 0.977 \nH0: lambda",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Statistical methods for replication assessment</span>"
    ]
  },
  {
    "objectID": "chapters/chapter4.html#small-telescopes-simonsohn2015-kg",
    "href": "chapters/chapter4.html#small-telescopes-simonsohn2015-kg",
    "title": "\n4  Statistical methods for replication assessment\n",
    "section": "\n10.21 Small Telescopes (Simonsohn, 2015)\n",
    "text": "10.21 Small Telescopes (Simonsohn, 2015)\n\nSimonsohn (2015) introduced 3 main questions when evaluating replicability:\n. . .\n\nWhen we combine data from the original and replication study, what is our best guess of the overall effect?\n\n. . .\nmeta-analysis\n. . .\n\nIs the effect of the replication study different from the original study?\n\n. . .\nmeta-analysis and standard tests, but problematic in terms of statistical power\n. . .\n\nDoes the replication study suggest that the effect of interest is undetectable different from zero?\n\n. . .\nsmall telescopes",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Statistical methods for replication assessment</span>"
    ]
  },
  {
    "objectID": "chapters/chapter4.html#small-telescopes-simonsohn2015-kg-1",
    "href": "chapters/chapter4.html#small-telescopes-simonsohn2015-kg-1",
    "title": "\n4  Statistical methods for replication assessment\n",
    "section": "\n10.22 Small Telescopes (Simonsohn, 2015)\n",
    "text": "10.22 Small Telescopes (Simonsohn, 2015)\n\nThe idea is simple but quite powerful and insightful. Let’s assume that an original study found an effect of \\(y_{orig} = 0.7\\) on a two-sample design with \\(n = 20\\) per group.\n. . .\n\nwe define a threshold as the effect size that is associated with a certain low power level e.g., \\(33\\%\\) given the sample size i.e. \\(\\theta_{small} = 0.5\\)\n\nthe replication study found an effect of \\(y_{rep} = 0.2\\) with \\(n = 100\\) subjects\n\n. . .\nIf the \\(y_{rep}\\) is lower (i.e., the upper bound of the confidence interval) than the small effect (\\(\\theta_{small} = 0.5\\)) we conclude that the effect is probably so tiny that could not have been detected by the original study. Thus there is no evidence for a replication.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Statistical methods for replication assessment</span>"
    ]
  },
  {
    "objectID": "chapters/chapter4.html#small-telescopes-simonsohn2015-kg-2",
    "href": "chapters/chapter4.html#small-telescopes-simonsohn2015-kg-2",
    "title": "\n4  Statistical methods for replication assessment\n",
    "section": "\n10.23 Small Telescopes (Simonsohn, 2015)\n",
    "text": "10.23 Small Telescopes (Simonsohn, 2015)\n\nWe can use the custom small_telescope() function on simulated data:\n\nsmall_telescope &lt;- function(or_d,\n                            or_se,\n                            rep_d,\n                            rep_se,\n                            small,\n                            ci = 0.95){\n  # quantile for the ci\n  qs &lt;- c((1 - ci)/2, 1 - (1 - ci)/2)\n  \n  # original confidence interval\n  or_ci &lt;- or_d + qnorm(qs) * or_se\n  \n  # replication confidence interval\n  rep_ci &lt;- rep_d + qnorm(qs) * rep_se\n  \n  # small power\n  is_replicated &lt;- rep_ci[2] &gt; small\n  \n  msg_original &lt;- sprintf(\"Original Study: d = %.3f %s CI = [%.3f, %.3f]\",\n                          or_d, ci, or_ci[1], or_ci[2])\n  \n  msg_replicated &lt;- sprintf(\"Replication Study: d = %.3f %s CI = [%.3f, %.3f]\",\n                            rep_d, ci, rep_ci[1], rep_ci[2])\n  \n  \n  if(is_replicated){\n    msg_res &lt;- sprintf(\"The replicated effect is not smaller than the small effect (%.3f), (probably) replication!\", small)\n    msg_res &lt;- cli::col_green(msg_res)\n  }else{\n    msg_res &lt;- sprintf(\"The replicated effect is smaller than the small effect (%.3f), no replication!\", small)\n    msg_res &lt;- cli::col_red(msg_res)\n  }\n  \n  out &lt;- data.frame(id = c(\"original\", \"replication\"),\n                    d = c(or_d, rep_d),\n                    lower = c(or_ci[1], rep_ci[1]),\n                    upper = c(or_ci[2], rep_ci[2]),\n                    small = small\n  )\n  \n  # nice message\n  cat(\n    msg_original,\n    msg_replicated,\n    cli::rule(),\n    msg_res,\n    sep = \"\\n\"\n  )\n  \n  invisible(out)\n  \n}",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Statistical methods for replication assessment</span>"
    ]
  },
  {
    "objectID": "chapters/chapter4.html#small-telescopes-simonsohn2015-kg-3",
    "href": "chapters/chapter4.html#small-telescopes-simonsohn2015-kg-3",
    "title": "\n4  Statistical methods for replication assessment\n",
    "section": "\n10.24 Small Telescopes (Simonsohn, 2015)\n",
    "text": "10.24 Small Telescopes (Simonsohn, 2015)\n\n\nset.seed(2025)\n\nd &lt;- 0.2 # real effect\n\n# original study\nor_n &lt;- 20\nor_d &lt;- 0.7\nor_se &lt;- sqrt(1/20 + 1/20)\nd_small &lt;- pwr::pwr.t.test(or_n, power = 0.33)$d\n\n# replication\nrep_n &lt;- 100 # sample size of replication study\ng0 &lt;- rnorm(rep_n, 0, 1)\ng1 &lt;- rnorm(rep_n, d, 1)\n\nrep_d &lt;- mean(g1) - mean(g0)\nrep_se &lt;- sqrt(var(g1)/rep_n + var(g0)/rep_n)\n\nHere we are using the pwr::pwr.t.test() to compute the effect size \\(\\theta_{small}\\) (in code d) associated with 33% power.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Statistical methods for replication assessment</span>"
    ]
  },
  {
    "objectID": "chapters/chapter4.html#small-telescopes-simonsohn2015-kg-4",
    "href": "chapters/chapter4.html#small-telescopes-simonsohn2015-kg-4",
    "title": "\n4  Statistical methods for replication assessment\n",
    "section": "\n10.25 Small Telescopes (Simonsohn, 2015)\n",
    "text": "10.25 Small Telescopes (Simonsohn, 2015)\n\n\nsmall_telescope(or_d, or_se, rep_d, rep_se, d_small, ci = 0.95)\nOriginal Study: d = 0.700 0.95 CI = [0.080, 1.320]\nReplication Study: d = 0.214 0.95 CI = [-0.061, 0.490]\n────────────────────────────────────────────────────────────────────────────────\nThe replicated effect is smaller than the small effect (0.493), no replication!\n\n\nAnd a (quite over-killed) plot:",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Statistical methods for replication assessment</span>"
    ]
  },
  {
    "objectID": "chapters/chapter4.html#bayes-factor",
    "href": "chapters/chapter4.html#bayes-factor",
    "title": "\n4  Statistical methods for replication assessment\n",
    "section": "\n4.9 Bayes factor",
    "text": "4.9 Bayes factor",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Statistical methods for replication assessment</span>"
    ]
  },
  {
    "objectID": "chapters/chapter4.html#bayesian-inference",
    "href": "chapters/chapter4.html#bayesian-inference",
    "title": "\n4  Statistical methods for replication assessment\n",
    "section": "\n11.2 Bayesian inference",
    "text": "11.2 Bayesian inference\nBayesian inference is the statistical procedure where prior beliefs about a phenomenon are combined, using the Bayes theorem, with evidence from data to obtain the posterior beliefs.\n. . .\nThe interesting part is that the researcher express the prior beliefs in probabilistic terms. Then after collecting data, evidence from the experiment is combined increasing or decreasing the plausibility of prior beliefs.\n. . .\nLet’s make an (not a very innovative :smile:) example. We need to evaluate the fairness of a coin. The crucial parameter is \\(\\theta\\) that is the probability of success (e.g., head). We have our prior belief about the coin (e.g., fair but with some uncertainty). We toss the coin \\(k\\) times and we observe \\(x\\) heads. What are my conclusions?",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Statistical methods for replication assessment</span>"
    ]
  },
  {
    "objectID": "chapters/chapter4.html#bayesian-inference-1",
    "href": "chapters/chapter4.html#bayesian-inference-1",
    "title": "\n4  Statistical methods for replication assessment\n",
    "section": "\n11.3 Bayesian inference",
    "text": "11.3 Bayesian inference\n\\[\np(\\theta|D) = \\frac{p(D|\\theta) \\; p(\\theta)}{p(D)}\n\\] Where \\(\\theta\\) is our parameter and \\(D\\) the data. \\(p(\\theta|D)\\) is the posterior distribution that is the product between the likelihood \\(p(D|\\theta)\\) and the prior \\(p(\\theta)\\). \\(p(D)\\) is the probability of the data (aka marginal likelihood) and is necessary only for the posterior to be a proper probability distribution.\nWe can “read” the formula as: The probability of the parameter given the data is the product between the likelihood of the data given the parameter and the prior probability of the parameter.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Statistical methods for replication assessment</span>"
    ]
  },
  {
    "objectID": "chapters/chapter4.html#bayesian-inference-2",
    "href": "chapters/chapter4.html#bayesian-inference-2",
    "title": "\n4  Statistical methods for replication assessment\n",
    "section": "\n11.4 Bayesian inference",
    "text": "11.4 Bayesian inference\nLet’s express our prior belief in probabilistic terms:",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Statistical methods for replication assessment</span>"
    ]
  },
  {
    "objectID": "chapters/chapter4.html#bayesian-inference-3",
    "href": "chapters/chapter4.html#bayesian-inference-3",
    "title": "\n4  Statistical methods for replication assessment\n",
    "section": "\n11.5 Bayesian inference",
    "text": "11.5 Bayesian inference\nNow we collect data and we observe \\(x = 40\\) tails out of \\(k = 50\\) trials thus \\(\\hat{\\theta} = 0.8\\) and compute the likelihood:",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Statistical methods for replication assessment</span>"
    ]
  },
  {
    "objectID": "chapters/chapter4.html#bayesian-inference-4",
    "href": "chapters/chapter4.html#bayesian-inference-4",
    "title": "\n4  Statistical methods for replication assessment\n",
    "section": "\n11.6 Bayesian inference",
    "text": "11.6 Bayesian inference\nFinally we combine, using the Bayes rule, prior and likelihood to obtain the posterior distribution:",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Statistical methods for replication assessment</span>"
    ]
  },
  {
    "objectID": "chapters/chapter4.html#bayes-factor-1",
    "href": "chapters/chapter4.html#bayes-factor-1",
    "title": "\n4  Statistical methods for replication assessment\n",
    "section": "\n11.7 Bayes Factor",
    "text": "11.7 Bayes Factor\nThe idea of the Bayes Factor is computing the evidence of the data under two competing hypotheses, \\(H_0\\) and \\(H_1\\) (~ \\(\\theta\\) in our previous example):\n\\[\n\\frac{p(H_0|D)}{p(H_1|D)} = \\frac{f(D|H_0)}{f(D|H_1)} \\times \\frac{p(H_0)}{p(H_1)}\n\\]\nWhere \\(f\\) is the likelihood function, \\(y\\) are the data. The \\(\\frac{p(H_0)}{p(H_1)}\\) is the prior odds of the two hypothesis. The Bayes Factor is the ratio between the likelihood of the data under the two hypotheses.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Statistical methods for replication assessment</span>"
    ]
  },
  {
    "objectID": "chapters/chapter4.html#bayes-factor-using-the-sdr",
    "href": "chapters/chapter4.html#bayes-factor-using-the-sdr",
    "title": "\n4  Statistical methods for replication assessment\n",
    "section": "\n11.8 Bayes Factor using the SDR",
    "text": "11.8 Bayes Factor using the SDR\nCalculating the BF can be problematic in some condition. The SDR is a convenient shortcut to calculate the Bayes Factor (Wagenmakers et al., 2010). The idea is that the ratio between the prior and posterior density distribution for the \\(H_1\\) is an estimate of the Bayes factor calculated in the standard way.\n\\[\nBF_{01} = \\frac{p(D|H_0)}{p(D|H_1)} = \\frac{p(\\theta = x|D, H_1)}{p(\\theta = x, H_1)}\n\\]\nWhere \\(\\theta\\) is the parameter of interest and \\(x\\) is the null value under \\(H_0\\) e.g., 0. and \\(D\\) are the data.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Statistical methods for replication assessment</span>"
    ]
  },
  {
    "objectID": "chapters/chapter4.html#bayes-factor-using-the-sdr-example",
    "href": "chapters/chapter4.html#bayes-factor-using-the-sdr-example",
    "title": "\n4  Statistical methods for replication assessment\n",
    "section": "\n11.9 Bayes Factor using the SDR, Example:",
    "text": "11.9 Bayes Factor using the SDR, Example:\nFollowing the previous example \\(H_0: \\theta = 0.5\\). Under \\(H_1\\) we use a completely uninformative prior by setting \\(\\theta \\sim Beta(1, 1)\\).\nWe flip again the coin 20 times and we found that \\(\\hat \\theta = 0.75\\).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Statistical methods for replication assessment</span>"
    ]
  },
  {
    "objectID": "chapters/chapter4.html#bayes-factor-using-the-sdr-example-1",
    "href": "chapters/chapter4.html#bayes-factor-using-the-sdr-example-1",
    "title": "\n4  Statistical methods for replication assessment\n",
    "section": "\n11.10 Bayes Factor using the SDR, Example:",
    "text": "11.10 Bayes Factor using the SDR, Example:\nThe ratio between the two black dots is the Bayes Factor.\n\n\n\n\n\n\n\n\n\nIf the probability density of the null value decrease after seeing data (from prior to posterior) this means that the Bayes factor should favor the alternative hypothesis. On the left, the density of 0.5 is lower after seeing the data –&gt; evidence for H1 On the right the density of 0.5 is higher after seeing the data –&gt; evidence for H0",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Statistical methods for replication assessment</span>"
    ]
  },
  {
    "objectID": "chapters/chapter4.html#verhagen2014-tx-model",
    "href": "chapters/chapter4.html#verhagen2014-tx-model",
    "title": "\n4  Statistical methods for replication assessment\n",
    "section": "\n11.11 Verhagen & Wagenmakers (2014) model2\n",
    "text": "11.11 Verhagen & Wagenmakers (2014) model2\n\nThe idea is using the posterior distribution of the original study as prior for a Bayesian hypothesis testing where:\n\n\n\\(H_0: \\theta_{rep} = 0\\) thus there is no effect in the replication study\n\n\\(H_1: \\theta_{rep} \\neq 0\\) and in particular is distributed as \\(\\delta \\sim \\mathcal{N}(\\theta_{orig}, \\sigma^2_{orig})\\) where \\(\\theta_{orig}\\) and \\(\\sigma^2_{orig}\\) are the mean and standard error of the original study\n\nIf \\(H_0\\) is more likely after seeing the data, there is evidence against the replication (i.e., \\(BF_{r0} &gt; 1\\)) otherwise there is evidence for a successful replication (\\(BF_{r1} &gt; 1\\)).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Statistical methods for replication assessment</span>"
    ]
  },
  {
    "objectID": "chapters/chapter4.html#verhagen2014-tx-model-1",
    "href": "chapters/chapter4.html#verhagen2014-tx-model-1",
    "title": "\n4  Statistical methods for replication assessment\n",
    "section": "\n11.12 Verhagen & Wagenmakers (2014) model",
    "text": "11.12 Verhagen & Wagenmakers (2014) model\n\n\n\n\n\n\nWarning\n\n\n\nDisclaimer: The actual implementation of Verhagen & Wagenmakers (2014) is different (they use the \\(t\\) statistics). The proposed implementation for the current workshop use a standard linear model.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Statistical methods for replication assessment</span>"
    ]
  },
  {
    "objectID": "chapters/chapter4.html#example",
    "href": "chapters/chapter4.html#example",
    "title": "\n4  Statistical methods for replication assessment\n",
    "section": "\n11.13 Example",
    "text": "11.13 Example\nLet’s assume that the original study (\\(n = 30\\)) estimate a \\(y_{orig} = 0.4\\) and a standard error of \\(\\sigma^2/n\\).\n\n# original study\nn &lt;- 30\nyorig &lt;- 0.4\nse &lt;- sqrt(1/30)\n\n\n\n\n\n\n\nNote\n\n\n\nThe assumption of Verhagen & Wagenmakers (2014) is that the original study performed a Bayesian analysis with a completely flat prior. Thus the confidence interval is the same as the Bayesian credible interval.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Statistical methods for replication assessment</span>"
    ]
  },
  {
    "objectID": "chapters/chapter4.html#example-1",
    "href": "chapters/chapter4.html#example-1",
    "title": "\n4  Statistical methods for replication assessment\n",
    "section": "\n11.14 Example",
    "text": "11.14 Example\nFor this reason, the posterior distribution of the original study can be approximated as:\n\n\n\n\n\n\n\n\n\nWith an uninformative prior the credible interval is the same as the confidence interval",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Statistical methods for replication assessment</span>"
    ]
  },
  {
    "objectID": "chapters/chapter4.html#example-2",
    "href": "chapters/chapter4.html#example-2",
    "title": "\n4  Statistical methods for replication assessment\n",
    "section": "\n11.15 Example",
    "text": "11.15 Example\nLet’s imagine that a new study tried to replicate the original one. They collected \\(n = 100\\) participants with the same protocol and found and effect of \\(y_{rep} = 0.1\\).\n\nnrep &lt;- 100\nyrep &lt;- MASS::mvrnorm(nrep, mu = 0.1, Sigma = 1, empirical = TRUE)[, 1]\ndat &lt;- data.frame(y = yrep)\nhist(yrep, main = \"Replication Study (n1 = 100)\", xlab = latex2exp::TeX(\"$y_{rep}$\"))\nabline(v = mean(yrep), lwd = 2, col = \"firebrick\")",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Statistical methods for replication assessment</span>"
    ]
  },
  {
    "objectID": "chapters/chapter4.html#example-3",
    "href": "chapters/chapter4.html#example-3",
    "title": "\n4  Statistical methods for replication assessment\n",
    "section": "\n11.16 Example",
    "text": "11.16 Example\nWe can analyze these data with an intercept-only regression model setting as prior the posterior distribution of the original study:\n\n# setting the prior on the intercept parameter\nprior &lt;- rstanarm::normal(location = yorig,\n                          scale = se)\n\n# fitting the bayesian linear regression\nfit &lt;- stan_glm(y ~ 1, \n                data = dat, \n                prior_intercept = prior,\n                refresh = FALSE)\n\nsummary(fit)\n\nModel Info:\n function:     stan_glm\n family:       gaussian [identity]\n formula:      y ~ 1\n algorithm:    sampling\n sample:       4000 (posterior sample size)\n priors:       see help('prior_summary')\n observations: 100\n predictors:   1\n\nEstimates:\n              mean   sd   10%   50%   90%\n(Intercept) 0.2    0.1  0.1   0.2   0.3  \nsigma       1.0    0.1  0.9   1.0   1.1  \n\nFit Diagnostics:\n           mean   sd   10%   50%   90%\nmean_PPD 0.2    0.1  0.0   0.2   0.3  \n\nThe mean_ppd is the sample average posterior predictive distribution of the outcome variable (for details see help('summary.stanreg')).\n\nMCMC diagnostics\n              mcse Rhat n_eff\n(Intercept)   0.0  1.0  2609 \nsigma         0.0  1.0  2608 \nmean_PPD      0.0  1.0  3185 \nlog-posterior 0.0  1.0  1693 \n\nFor each parameter, mcse is Monte Carlo standard error, n_eff is a crude measure of effective sample size, and Rhat is the potential scale reduction factor on split chains (at convergence Rhat=1).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Statistical methods for replication assessment</span>"
    ]
  },
  {
    "objectID": "chapters/chapter4.html#example-4",
    "href": "chapters/chapter4.html#example-4",
    "title": "\n4  Statistical methods for replication assessment\n",
    "section": "\n11.17 Example",
    "text": "11.17 Example\n\n\nResults\nPlot\n\n\n\nWe can use the bayestestR::bayesfactor_pointnull() to calculate the BF using the Savage-Dickey density ratio.\n\nbf &lt;- bayestestR::bayesfactor_pointnull(fit, null = 0)\nprint(bf)\n\n\n\n\nplot(bf)",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Statistical methods for replication assessment</span>"
    ]
  },
  {
    "objectID": "chapters/chapter4.html#example-5",
    "href": "chapters/chapter4.html#example-5",
    "title": "\n4  Statistical methods for replication assessment\n",
    "section": "\n11.18 Example",
    "text": "11.18 Example\nYou can also use the bf_replication() function:\n\nbf_replication &lt;- function(mu_original,\n                           se_original,\n                           replication){\n  \n  # prior based on the original study\n  prior &lt;- rstanarm::normal(location = mu_original, scale = se_original)\n  \n  # to dataframe\n  replication &lt;- data.frame(y = replication)\n  \n  fit &lt;- rstanarm::stan_glm(y ~ 1,\n                            data = replication,\n                            prior_intercept = prior, \n                            refresh = 0) # avoid printing\n  \n  bf &lt;- bayestestR::bayesfactor_pointnull(fit, null = 0, verbose = FALSE)\n  \n  title &lt;- \"Bayes Factor Replication Rate\"\n  posterior &lt;- \"Posterior Distribution ~ Mean: %.3f, SE: %.3f\"\n  replication &lt;- \"Evidence for replication: %3f (log %.3f)\"\n  non_replication &lt;- \"Evidence for non replication: %3f (log %.3f)\"\n  \n  if(bf$log_BF &gt; 0){\n    replication &lt;- cli::col_green(sprintf(replication, exp(bf$log_BF), bf$log_BF))\n    non_replication &lt;- sprintf(non_replication, 1/exp(bf$log_BF), -bf$log_BF)\n  }else{\n    replication &lt;- sprintf(replication, exp(bf$log_BF), bf$log_BF)\n    non_replication &lt;- cli::col_red(sprintf(non_replication, 1/exp(bf$log_BF), -bf$log_BF))\n  }\n  \n  outlist &lt;- list(\n    fit = fit,\n    bf = bf\n  )\n  \n  cat(\n    cli::col_blue(title),\n    cli::rule(),\n    sprintf(posterior, fit$coefficients, fit$ses),\n    \"\\n\",\n    replication,\n    non_replication,\n    sep = \"\\n\"\n  )\n  \n  invisible(outlist)\n  \n}",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Statistical methods for replication assessment</span>"
    ]
  },
  {
    "objectID": "chapters/chapter4.html#example-6",
    "href": "chapters/chapter4.html#example-6",
    "title": "\n4  Statistical methods for replication assessment\n",
    "section": "\n11.19 Example",
    "text": "11.19 Example\n\nbf_replication(mu_original = yorig, se_original = se, replication = yrep)",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Statistical methods for replication assessment</span>"
    ]
  },
  {
    "objectID": "chapters/chapter4.html#example-7",
    "href": "chapters/chapter4.html#example-7",
    "title": "\n4  Statistical methods for replication assessment\n",
    "section": "\n11.20 Example",
    "text": "11.20 Example\nA better custom plot:\n\nbfplot &lt;- data.frame(\n  prior = rnorm(1e5, yorig, se),\n  posterior = rnorm(1e5, fit$coefficients, fit$ses)\n)\n \nplt &lt;- ggplot() +\n  stat_function(geom = \"line\", \n                aes(color = \"Original Study (Prior)\"),\n                linewidth = 1,\n                alpha = 0.3,\n                fun = dnorm, args = list(mean = yorig, sd = se)) +\n  stat_function(geom = \"line\",\n                linewidth = 1,\n                aes(color = \"Replication Study (Posterior)\"),\n                fun = dnorm, args = list(mean = fit$coefficients, sd = fit$ses)) +\n  xlim(c(-0.5, 1.2)) +\n  geom_point(aes(x = c(0, 0), y = c(dnorm(0, yorig, sd = se),\n                                    dnorm(0, fit$coefficients, sd = fit$ses))),\n             size = 3) +\n  xlab(latex2exp::TeX(\"\\\\delta\")) +\n  ylab(\"Density\") +\n  theme(legend.position = \"bottom\",\n        legend.title = element_blank())",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Statistical methods for replication assessment</span>"
    ]
  },
  {
    "objectID": "chapters/chapter4.html#example-8",
    "href": "chapters/chapter4.html#example-8",
    "title": "\n4  Statistical methods for replication assessment\n",
    "section": "\n11.21 Example",
    "text": "11.21 Example\nA better custom plot:",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Statistical methods for replication assessment</span>"
    ]
  },
  {
    "objectID": "chapters/chapter4.html#references",
    "href": "chapters/chapter4.html#references",
    "title": "\n4  Statistical methods for replication assessment\n",
    "section": "\n4.10 References",
    "text": "4.10 References\n\n\n\n\nAnderson, S. F., & Kelley, K. (2024). Sample size planning for replication studies: The devil is in the design. Psychological Methods, 29, 844–867. https://doi.org/10.1037/met0000520\n\n\nBushman, B. J., & Wang, M. C. (2009). Vote-counting procedures in meta-analysis. In The handbook of research synthesis and meta-analysis (pp. 207–220). Russell Sage Foundation.\n\n\nHedges, L. V., & Olkin, I. (1980). Vote-counting methods in research synthesis. Psychological Bulletin, 88, 359–369. https://doi.org/10.1037/0033-2909.88.2.359\n\n\nHedges, L. V., & Schauer, J. M. (2019). Statistical analyses for studying replication: Meta-analytic perspectives. Psychological Methods, 24, 557–570. https://doi.org/10.1037/met0000189\n\n\nHeyard, R., Pawel, S., Frese, J., Voelkl, B., Würbel, H., McCann, S., Held, L., Wever, K. E., Hartmann, H., Townsin, L., & Zellers, S. (2024). A scoping review on metrics to quantify reproducibility: A multitude of questions leads to a multitude of metrics. https://scholar.google.com/citations?view_op=view_citation&hl=en&citation_for_view=JAb7P1QAAAAJ:ldfaerwXgEUC\n\n\nLy, A., Etz, A., Marsman, M., & Wagenmakers, E.-J. (2019). Replication bayes factors from evidence updating. Behavior Research Methods, 51, 2498–2508. https://doi.org/10.3758/s13428-018-1092-x\n\n\nValentine, J. C., Biglan, A., Boruch, R. F., Castro, F. G., Collins, L. M., Flay, B. R., Kellam, S., Mościcki, E. K., & Schinke, S. P. (2011). Replication in prevention science. Prevention Science: The Official Journal of the Society for Prevention Research, 12, 103–117. https://doi.org/10.1007/s11121-011-0217-6\n\n\nVerhagen, J., & Wagenmakers, E.-J. (2014). Bayesian tests to quantify the result of a replication attempt. Journal of Experimental Psychology. General, 143, 1457–1475. https://doi.org/10.1037/a0036731",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Statistical methods for replication assessment</span>"
    ]
  },
  {
    "objectID": "chapters/chapter4.html#footnotes",
    "href": "chapters/chapter4.html#footnotes",
    "title": "\n4  Statistical methods for replication assessment\n",
    "section": "",
    "text": "The approach has been debated by a series of opinion papers (see Hedges & Schauer, 2019a; Mathur & VanderWeele, 2019)↩︎\nsee also Ly et al. (2019) for an improvement↩︎",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Statistical methods for replication assessment</span>"
    ]
  },
  {
    "objectID": "chapters/chapter5.html",
    "href": "chapters/chapter5.html",
    "title": "\n5  Meta-analysis and publication bias\n",
    "section": "",
    "text": "6 Meta-analysis",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Meta-analysis and publication bias</span>"
    ]
  },
  {
    "objectID": "chapters/chapter5.html#meta-analysis-1",
    "href": "chapters/chapter5.html#meta-analysis-1",
    "title": "\n5  Meta-analysis and publication bias\n",
    "section": "\n6.1 Meta-analysis",
    "text": "6.1 Meta-analysis\n\nThe meta-analysis is a statistical procedure to combine evidence from a group of studies.\n\n. . .\n\nThe idea is to “switch” the statistical unit from e.g., participants to studies\n\n. . .\n\nThe motto could be that (appropriately) combining similar studies with a similar aim is the best way to understand something about a phenomenon",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Meta-analysis and publication bias</span>"
    ]
  },
  {
    "objectID": "chapters/chapter5.html#meta-analysis-and-systematic-review",
    "href": "chapters/chapter5.html#meta-analysis-and-systematic-review",
    "title": "\n5  Meta-analysis and publication bias\n",
    "section": "\n6.2 Meta-analysis and Systematic Review",
    "text": "6.2 Meta-analysis and Systematic Review\nUsually a meta-analysis work follows these steps:\n\n\nIdentify the research question: is the treatment x effective?, Does the experimental effect y exist?\n\nDefine inclusion/exclusion criteria: From the research question (1), keep only e.g., randomized controlled trials, studies with healthy participants, etc.\n\nSystematically search for studies: Analyze the literature to find all relevant studies\n\nExtract relevant information: Read, extract and organize relevant information e.g., sample size, treatment type, age, etc.\n\nSummarize the results: Create a narrative (flowcharts, tables, etc.) summary of included studies. This is the Systematic review part.\n\nChoose an effect size: Choose a way to standardize the effect across included studies\n\nMeta-analysis model: Choose and implement a meta-analysis model\nInterpret and report results",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Meta-analysis and publication bias</span>"
    ]
  },
  {
    "objectID": "chapters/chapter5.html#before-the-fun-part",
    "href": "chapters/chapter5.html#before-the-fun-part",
    "title": "\n5  Meta-analysis and publication bias\n",
    "section": "\n6.3 Before the fun part…",
    "text": "6.3 Before the fun part…\n\n\nWe are dealing only with the statistical part. The study selection, data extraction, studies evaluation etc. is another story\nThe quality of the meta-analysis is the quality of included studies\n\n\n\n. . .",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Meta-analysis and publication bias</span>"
    ]
  },
  {
    "objectID": "chapters/chapter5.html#unstandardized-effect-sizes",
    "href": "chapters/chapter5.html#unstandardized-effect-sizes",
    "title": "\n5  Meta-analysis and publication bias\n",
    "section": "\n6.4 Unstandardized effect sizes",
    "text": "6.4 Unstandardized effect sizes\nThe basic idea of an effect size is just using the raw measure. For example studies using reaction times we can calculate the difference between two conditions as \\(\\overline X_1 - \\overline X_2\\):",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Meta-analysis and publication bias</span>"
    ]
  },
  {
    "objectID": "chapters/chapter5.html#unstandardized-effect-sizes-1",
    "href": "chapters/chapter5.html#unstandardized-effect-sizes-1",
    "title": "\n5  Meta-analysis and publication bias\n",
    "section": "\n6.5 Unstandardized effect sizes",
    "text": "6.5 Unstandardized effect sizes\nBut another study (with the same research question) could use another measure, e.g., accuracy. We can still (not the best strategy but) compute the difference between the group means.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Meta-analysis and publication bias</span>"
    ]
  },
  {
    "objectID": "chapters/chapter5.html#unstandardized-effect-sizes-2",
    "href": "chapters/chapter5.html#unstandardized-effect-sizes-2",
    "title": "\n5  Meta-analysis and publication bias\n",
    "section": "\n6.6 Unstandardized effect sizes",
    "text": "6.6 Unstandardized effect sizes\nClearly we cannot directly compare the two effects but we need to standardize the measure.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Meta-analysis and publication bias</span>"
    ]
  },
  {
    "objectID": "chapters/chapter5.html#standardized-effect-sizes",
    "href": "chapters/chapter5.html#standardized-effect-sizes",
    "title": "\n5  Meta-analysis and publication bias\n",
    "section": "\n6.7 Standardized effect sizes",
    "text": "6.7 Standardized effect sizes\nTo compare results from different studies, we should use a common metric. Frequently meta-analysts use standardized effect sizes. For example the Pearson correlation or the Cohen’s \\(d\\).\n\\[\nr = \\frac{\\sum{(x_i - \\bar{x})(y_i - \\bar{y})}}{\\sqrt{\\sum{(x_i - \\bar{x})^2}\\sum{(y_i - \\bar{y})^2}}}\n\\tag{6.1}\\]\n\\[\nd = \\frac{\\bar{x_1} - \\bar{x_2}}{s_p}\n\\]\n\\[\ns_p = \\sqrt{\\frac{(n_1 - 1)s_1^2 + (n_2 - 1)s_2^2}{n_1 + n_2 - 2}}\n\\]",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Meta-analysis and publication bias</span>"
    ]
  },
  {
    "objectID": "chapters/chapter5.html#standardized-effect-sizes-1",
    "href": "chapters/chapter5.html#standardized-effect-sizes-1",
    "title": "\n5  Meta-analysis and publication bias\n",
    "section": "\n6.8 Standardized effect sizes",
    "text": "6.8 Standardized effect sizes\nThe advantage of standardized effect size is that regardless the original variable, the interpretation and the scale is the same. For example the pearson correlation ranges between -1 and 1 and the Cohen’s \\(d\\) between \\(- \\infty\\) and \\(\\infty\\) and is interpreted as how many standard deviations the two groups/conditions differs.\n\nCodeS &lt;- matrix(c(1, 0.7, 0.7, 1), nrow = 2)\nX &lt;- MASS::mvrnorm(100, c(0, 2), S, empirical = TRUE)\n\npar(mfrow = c(1,2))\nplot(X, xlab = \"x\", ylab = \"y\", cex = 1.3, pch = 19,\n     cex.lab = 1.2, cex.axis = 1.2,\n     main = latex2exp::TeX(sprintf(\"$r = %.2f$\", cor(X[, 1], X[, 2]))))\nabline(lm(X[, 2] ~ X[, 1]), col = \"firebrick\", lwd = 2)\n\n\nplot(density(X[, 1]), xlim = c(-5, 7), ylim = c(0, 0.5), col = \"dodgerblue\", lwd = 2,\n     main = latex2exp::TeX(sprintf(\"$d = %.2f$\", lsr::cohensD(X[, 1], X[, 2]))),\n     xlab = \"\")\nlines(density(X[, 2]), col = \"firebrick\", lwd = 2)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Meta-analysis and publication bias</span>"
    ]
  },
  {
    "objectID": "chapters/chapter5.html#standardized-vs-unstandardized",
    "href": "chapters/chapter5.html#standardized-vs-unstandardized",
    "title": "\n5  Meta-analysis and publication bias\n",
    "section": "\n6.9 Standardized vs unstandardized",
    "text": "6.9 Standardized vs unstandardized\nThe main difference is (usually) the absence of a effect-size-variance relationship for unstandardized effects. For example, the variance of the difference between two groups is:\n\\[\nV_d = \\frac{s^2_1}{n_1} + \\frac{s^2_2}{n_2}\n\\tag{6.2}\\]\nWhile the variance of a Cohen’s \\(d\\) can be calculated as:\n\\[\nV_d = \\frac{n_1 + n_2}{n_1 n_2} + \\frac{d^2}{2(n_1 + n_2)}\n\\]",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Meta-analysis and publication bias</span>"
    ]
  },
  {
    "objectID": "chapters/chapter5.html#standardized-vs-unstandardized-1",
    "href": "chapters/chapter5.html#standardized-vs-unstandardized-1",
    "title": "\n5  Meta-analysis and publication bias\n",
    "section": "\n6.10 Standardized vs unstandardized",
    "text": "6.10 Standardized vs unstandardized\nIn this amazing blog post James Pustejovsky explained where the equations comes from. Basically, the \\(\\frac{n_1 + n_2}{n_1 n_2}\\) term is the same as the \\(\\frac{s^2_1}{n_1} + \\frac{s^2_2}{n_2}\\) while the extra \\(\\frac{d^2}{2(n_1 + n_2)}\\) is for the non-centrality induced by the standardized difference.\n\nCoden &lt;- c(10, 50, 100)\nd &lt;- seq(0, 2, 0.001)\n\ndd &lt;- expand.grid(n = n, d = d)\n\ndd$vumd &lt;- with(dd, 1/n + 1/n)\ndd$vd &lt;- with(dd, (n + n) / (n * n) + d^2/(2 * (n + n)))\n\ntidyr::pivot_longer(dd, 3:4) |&gt; \n  ggplot(aes(x = d, y = value, color = name, linetype = factor(n))) +\n  geom_line() +\n  labs(linetype = \"Sample Size\",\n       color = NULL)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Meta-analysis and publication bias</span>"
    ]
  },
  {
    "objectID": "chapters/chapter5.html#sec-effsize-se",
    "href": "chapters/chapter5.html#sec-effsize-se",
    "title": "\n5  Meta-analysis and publication bias\n",
    "section": "\n6.11 Effect size sampling variability",
    "text": "6.11 Effect size sampling variability\nCrucially, we can calculate also the sampling variability of each effect size. The sampling variability is the precision of estimated value.\nFor example, there are multiple methods to estimate the Cohen’s \\(d\\) sampling variability. For example:\n\\[\nV_d = \\frac{n_1 + n_2}{n_1 n_2} + \\frac{d^2}{2(n_1 + n_2)}\n\\]\nEach effect size has a specific formula for the sampling variability. The sample size is usually the most important information. Studies with high sample size have low sampling variability.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Meta-analysis and publication bias</span>"
    ]
  },
  {
    "objectID": "chapters/chapter5.html#effect-size-sampling-variability",
    "href": "chapters/chapter5.html#effect-size-sampling-variability",
    "title": "\n5  Meta-analysis and publication bias\n",
    "section": "\n6.12 Effect size sampling variability",
    "text": "6.12 Effect size sampling variability\nAs the sample size grows and tends to infinity, the sampling variability approach zero.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Meta-analysis and publication bias</span>"
    ]
  },
  {
    "objectID": "chapters/chapter5.html#unstandardized-effect-sizes-3",
    "href": "chapters/chapter5.html#unstandardized-effect-sizes-3",
    "title": "\n5  Meta-analysis and publication bias\n",
    "section": "\n6.13 Unstandardized effect sizes",
    "text": "6.13 Unstandardized effect sizes\nFor the examples and plots I’m going to use simulated data. We simulate unstandardized effect sizes (UMD) because the computations are easier and the estimator is unbiased (e.g., Viechtbauer, 2005)\nMore specifically we simulate hypothetical studies where two independent groups are compared:\n\\[\n\\Delta = \\overline{X_1} - \\overline{X_2}\n\\tag{6.3}\\]\n\\[\nSE_{\\Delta} = \\sqrt{\\frac{s^2_1}{n_1} + \\frac{s^2_2}{n_2}}\n\\]\nWith \\(X_{1_i} \\sim \\mathcal{N}(0, 1)\\) and \\(X_{2_i} \\sim \\mathcal{N}(\\Delta, 1)\\)\nThe main advantage is that, compared to standardized effect size, the sampling variability do not depends on the effect size itself, simplifying the computations.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Meta-analysis and publication bias</span>"
    ]
  },
  {
    "objectID": "chapters/chapter5.html#simulating-a-single-study---umd",
    "href": "chapters/chapter5.html#simulating-a-single-study---umd",
    "title": "\n5  Meta-analysis and publication bias\n",
    "section": "\n7.1 Simulating a single study - UMD",
    "text": "7.1 Simulating a single study - UMD\nTo simulate a single study using a UMD we need to generate data according to the appropriate model. Here we have a difference between two groups. We can assume that the two groups comes from a normal distribution where group 1 \\(g_1 \\sim \\mathcal{N}(0, 1)\\) and group 2 \\(g_2 \\sim \\mathcal{N}(D, 1)\\) where \\(D\\) is the effect size. Then using Equations 6.2, 6.3 we can estimate the effect size and the variance.\n\nD &lt;- 1  # effect size\nn &lt;- 50 # sample size\ng1 &lt;- rnorm(n, mean = 0, sd = 1)\ng2 &lt;- rnorm(n, mean = D, sd = 1)\n\n# effect size\nmean(g2) - mean(g1)\n#&gt; [1] 0.9566799\n\n# variance\nvar(g1)/n + var(g2)/n\n#&gt; [1] 0.0360547",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Meta-analysis and publication bias</span>"
    ]
  },
  {
    "objectID": "chapters/chapter5.html#simulating-a-single-study---umd-1",
    "href": "chapters/chapter5.html#simulating-a-single-study---umd-1",
    "title": "\n5  Meta-analysis and publication bias\n",
    "section": "\n7.2 Simulating a single study - UMD",
    "text": "7.2 Simulating a single study - UMD\nFor simplicity we can wrap everything within a function:\n\n# default sd = 1\nsim_umd &lt;- function(n1, n2 = NULL, D, sd = 1){\n  if(is.null(n2)) n2 &lt;- n1 # same to n1 if null \n  g1 &lt;- rnorm(n1, mean = 0, sd = sd)\n  g2 &lt;- rnorm(n2, mean = D, sd = sd)\n  yi &lt;- mean(g2) - mean(g1)\n  vi &lt;- var(g1)/n1 + var(g2)/n2\n  data.frame(yi, vi)\n}\n\nsim_umd(100, D = 0.5)\n#&gt;         yi         vi\n#&gt; 1 0.226548 0.01773798\nsim_umd(50, D = 0.1)\n#&gt;            yi         vi\n#&gt; 1 -0.03957109 0.04342838",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Meta-analysis and publication bias</span>"
    ]
  },
  {
    "objectID": "chapters/chapter5.html#simulating-a-single-study---umd-2",
    "href": "chapters/chapter5.html#simulating-a-single-study---umd-2",
    "title": "\n5  Meta-analysis and publication bias\n",
    "section": "\n7.3 Simulating a single study - UMD",
    "text": "7.3 Simulating a single study - UMD\nWe can also generate a large number of studies and check the distribution of effect size and sampling variances. Note that the real \\(D = 1\\) and the real variance \\(V_D = 1/50 + 1/50 = 0.04\\)\n\nstudies &lt;- replicate(1000, sim_umd(n1 = 50, D = 1), simplify = FALSE) # simplify = FALSE return a list\nstudies &lt;- do.call(rbind, studies) # to dataframe\nhead(studies)\n\n#&gt;          yi         vi\n#&gt; 1 0.5765818 0.03413209\n#&gt; 2 0.9926937 0.04289115\n#&gt; 3 1.1580807 0.04379710\n#&gt; 4 1.2464796 0.04524347\n#&gt; 5 0.8797195 0.03960170\n#&gt; 6 1.2336143 0.03266831",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Meta-analysis and publication bias</span>"
    ]
  },
  {
    "objectID": "chapters/chapter5.html#sec-umd-sampling-distribution",
    "href": "chapters/chapter5.html#sec-umd-sampling-distribution",
    "title": "\n5  Meta-analysis and publication bias\n",
    "section": "\n7.4 Simulating a single study - UMD",
    "text": "7.4 Simulating a single study - UMD\nThen we can plot the sampling distributions:",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Meta-analysis and publication bias</span>"
    ]
  },
  {
    "objectID": "chapters/chapter5.html#simulating-a-single-study---smd",
    "href": "chapters/chapter5.html#simulating-a-single-study---smd",
    "title": "\n5  Meta-analysis and publication bias\n",
    "section": "\n7.5 Simulating a single study - SMD",
    "text": "7.5 Simulating a single study - SMD\nThe idea is the same when simulating a SDM but we need extra steps. Let’s adjust the previous function:\n\nsim_smd &lt;- function(n1, n2 = NULL, D){\n  if(is.null(n2)) n2 &lt;- n1 # same to n1 if null \n  g1 &lt;- rnorm(n1, mean = 0, sd = 1)\n  g2 &lt;- rnorm(n2, mean = D, sd = 1)\n  \n  v1 &lt;- var(g1)\n  v2 &lt;- var(g2)\n  \n  # pooled standard deviation\n  sp &lt;- sqrt((v1 * (n1 - 1) + v2 * (n2 - 1)) / (n1 + n2 - 2))\n  \n  yi &lt;- (mean(g2) - mean(g1)) / sp\n  vi &lt;- (n1 + n2) / (n1 * n2) + yi^2/(2*(n1 + n2))\n  data.frame(yi, vi)\n}",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Meta-analysis and publication bias</span>"
    ]
  },
  {
    "objectID": "chapters/chapter5.html#simulating-a-single-study---smd-1",
    "href": "chapters/chapter5.html#simulating-a-single-study---smd-1",
    "title": "\n5  Meta-analysis and publication bias\n",
    "section": "\n7.6 Simulating a single study - SMD",
    "text": "7.6 Simulating a single study - SMD\nWhen working with SMD, calculating the sampling variance can be challenging. Veroniki et al. (2016) identified 16 different estimators with different properties. Furthermore, it is a common practice to correct the SDM effect and variance using the Hedges’s correction (Hedges, 1989).\nYou can directly implement another equation for the sampling variance or the Hedges’s correction directly in the simulation function.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Meta-analysis and publication bias</span>"
    ]
  },
  {
    "objectID": "chapters/chapter5.html#simulating-a-single-study---pearson-rho",
    "href": "chapters/chapter5.html#simulating-a-single-study---pearson-rho",
    "title": "\n5  Meta-analysis and publication bias\n",
    "section": "\n7.7 Simulating a single study - Pearson \\(\\rho\\)\n",
    "text": "7.7 Simulating a single study - Pearson \\(\\rho\\)\n\nAnother common effect size is the Pearson correlation coefficient \\(\\rho\\) (and the estimate \\(r\\), see Equation 6.1). The variance of the correlation is calculated as:\n\\[\nV_{r} = \\frac{(1 - r^2)^2}{n - 1}\n\\]",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Meta-analysis and publication bias</span>"
    ]
  },
  {
    "objectID": "chapters/chapter5.html#simulating-a-single-study---pearson-rho-1",
    "href": "chapters/chapter5.html#simulating-a-single-study---pearson-rho-1",
    "title": "\n5  Meta-analysis and publication bias\n",
    "section": "\n7.8 Simulating a single study - Pearson \\(\\rho\\)\n",
    "text": "7.8 Simulating a single study - Pearson \\(\\rho\\)\n\nThere is a huge dependency between \\(r\\) and it’s sampling variance (similar to the Cohen’s \\(d\\)):\n\nCoden &lt;- 50\nr &lt;- seq(0, 1, 0.01)\nv &lt;- (1 - r^2)^2 / (n - 1) \n\nplot(r, v, type = \"l\", main = \"N = 50\", xlab = \"r\", ylab = latex2exp::TeX(\"$V_r$\"))",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Meta-analysis and publication bias</span>"
    ]
  },
  {
    "objectID": "chapters/chapter5.html#simulating-a-single-study---pearson-rho-2",
    "href": "chapters/chapter5.html#simulating-a-single-study---pearson-rho-2",
    "title": "\n5  Meta-analysis and publication bias\n",
    "section": "\n7.9 Simulating a single study - Pearson \\(\\rho\\)\n",
    "text": "7.9 Simulating a single study - Pearson \\(\\rho\\)\n\nFor this reason the so-called Fisher’s \\(z\\) transformation is used to stabilize the relationship.\n\\[\nz = \\frac{\\log{\\frac{1 + r}{1 - r}}}{2}\n\\]\n\\[\nV_z = \\frac{1}{n - 3}\n\\]\nNow the variance is completely independent from the correlation value.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Meta-analysis and publication bias</span>"
    ]
  },
  {
    "objectID": "chapters/chapter5.html#simulating-a-single-study---pearson-rho-3",
    "href": "chapters/chapter5.html#simulating-a-single-study---pearson-rho-3",
    "title": "\n5  Meta-analysis and publication bias\n",
    "section": "\n7.10 Simulating a single study - Pearson \\(\\rho\\)\n",
    "text": "7.10 Simulating a single study - Pearson \\(\\rho\\)\n\nThis is the relationship between \\(r\\) and \\(z\\):\n\nCoden &lt;- 50\nr &lt;- seq(-1, 1, 0.01)\nv &lt;- (1 - r^2)^2 / (n - 1) \nz &lt;- log((1 + r)/(1 - r))/2\n\nplot(z, r, type = \"l\", xlab = \"Fisher's z\", ylab = \"Correlation\", main = \"Correlation to Fisher's z\")",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Meta-analysis and publication bias</span>"
    ]
  },
  {
    "objectID": "chapters/chapter5.html#simulating-a-single-study---pearson-rho-4",
    "href": "chapters/chapter5.html#simulating-a-single-study---pearson-rho-4",
    "title": "\n5  Meta-analysis and publication bias\n",
    "section": "\n7.11 Simulating a single study - Pearson \\(\\rho\\)\n",
    "text": "7.11 Simulating a single study - Pearson \\(\\rho\\)\n\nTo simulate a study using correlations we can use the MASS::mvrnorm() function that can generate correlated data from a multivariate normal distribution.\n\nsim_r &lt;- function(n, r){\n  R &lt;- r + diag(1 - r, nrow = 2) # 2 x 2 correlation matrix\n  X &lt;- MASS::mvrnorm(n, mu = c(0, 0), Sigma = R) # the means are not relevant here\n  r &lt;- cor(X)[1, 2] # extract correlation\n  vr &lt;- (1 - r^2)^2 / (n - 1)  # variance of r\n  yi &lt;- log((1 + r)/(1 - r))/2 # fisher z\n  vi &lt;- 1 / (n - 3) # fisher z variance\n  data.frame(yi, vi, r, vr) # including also the pearson correlation and variance\n}",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Meta-analysis and publication bias</span>"
    ]
  },
  {
    "objectID": "chapters/chapter5.html#simulating-a-single-study---pearson-rho-5",
    "href": "chapters/chapter5.html#simulating-a-single-study---pearson-rho-5",
    "title": "\n5  Meta-analysis and publication bias\n",
    "section": "\n7.12 Simulating a single study - Pearson \\(\\rho\\)\n",
    "text": "7.12 Simulating a single study - Pearson \\(\\rho\\)\n\n\nsim_r(100, 0.5)\n#&gt;          yi         vi         r          vr\n#&gt; 1 0.6166665 0.01030928 0.5488028 0.004932759\nsim_r(50, 0.8)\n#&gt;         yi        vi       r          vr\n#&gt; 1 1.282351 0.0212766 0.85711 0.001437086\n\n# also here the sampling distributions\nstudies &lt;- replicate(1000, sim_r(50, 0.7), simplify = FALSE)\nstudies &lt;- do.call(rbind, studies)\nsummary(studies)\n#&gt;        yi               vi                r                vr          \n#&gt;  Min.   :0.4456   Min.   :0.02128   Min.   :0.4182   Min.   :0.001244  \n#&gt;  1st Qu.:0.7792   1st Qu.:0.02128   1st Qu.:0.6522   1st Qu.:0.003917  \n#&gt;  Median :0.8736   Median :0.02128   Median :0.7032   Median :0.005215  \n#&gt;  Mean   :0.8773   Mean   :0.02128   Mean   :0.6977   Mean   :0.005469  \n#&gt;  3rd Qu.:0.9720   3rd Qu.:0.02128   3rd Qu.:0.7496   3rd Qu.:0.006737  \n#&gt;  Max.   :1.3241   Max.   :0.02128   Max.   :0.8678   Max.   :0.013893",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Meta-analysis and publication bias</span>"
    ]
  },
  {
    "objectID": "chapters/chapter5.html#more-on-effect-sizes",
    "href": "chapters/chapter5.html#more-on-effect-sizes",
    "title": "\n5  Meta-analysis and publication bias\n",
    "section": "\n7.13 More on effect sizes",
    "text": "7.13 More on effect sizes\nThe same logic can be applied to any situation. Just understand the data generation process, find the effect size equations and generate data.\n\n\nBorenstein et al. (2009) for all effect sizes equations. Also with equations to convert among effect sizes (useful in real-world meta-analyses)\n\nthe metafor::escalc() function implements basically any effect size. You can see also the source code to see the actual R implementation.\n\nGuide to effect sizes: a modern and complete overview of effect sizes",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Meta-analysis and publication bias</span>"
    ]
  },
  {
    "objectID": "chapters/chapter5.html#simulating-from-sampling-distributions-extra",
    "href": "chapters/chapter5.html#simulating-from-sampling-distributions-extra",
    "title": "\n5  Meta-analysis and publication bias\n",
    "section": "\n7.14 Simulating from sampling distributions [#extra]",
    "text": "7.14 Simulating from sampling distributions [#extra]\nThe previous simulation examples are participant-level simulations. In fact we simulated \\(n\\) observations then we aggregated calculating the effect sizes.\n. . .\nThis is the most flexible and general data simulation strategy but is computationally not efficient.\n. . .\nAnother strategy individuate the exact effect size sampling distribution. Then we can sample directly from it. The downside is that we need to derive (or find) the equation.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Meta-analysis and publication bias</span>"
    ]
  },
  {
    "objectID": "chapters/chapter5.html#simulating-from-sampling-distributions-extra-1",
    "href": "chapters/chapter5.html#simulating-from-sampling-distributions-extra-1",
    "title": "\n5  Meta-analysis and publication bias\n",
    "section": "\n7.15 Simulating from sampling distributions [#extra]",
    "text": "7.15 Simulating from sampling distributions [#extra]\nFor example, when generating UMD we can simulate from the sampling distribution presented in Section 7.4.\n\\[\ny_i \\sim \\mathcal{N}(\\theta, \\sqrt{\\sigma^2_i})\n\\] \\[\n\\sigma^2_i \\sim \\frac{\\chi^2_{n_1 + n_2 - 2}}{n_1 + n_2 - 2} (\\frac{1}{n_1} + \\frac{1}{n_2})\n\\]\nIn this way we can sample \\(k\\) effects and sampling variances directly from the sampling distributions. Without generating data and then aggregate.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Meta-analysis and publication bias</span>"
    ]
  },
  {
    "objectID": "chapters/chapter5.html#simulating-from-sampling-distributions-extra-2",
    "href": "chapters/chapter5.html#simulating-from-sampling-distributions-extra-2",
    "title": "\n5  Meta-analysis and publication bias\n",
    "section": "\n7.16 Simulating from sampling distributions [#extra]",
    "text": "7.16 Simulating from sampling distributions [#extra]\nWe can again put everything within a function:\nsim_k_umd &lt;- function(k, D, n1, n2 = NULL){\n  if(is.null(n2)) n2 &lt;- n1\n  yi &lt;- rnorm(k, D, sqrt(1/n1 + 1/n2))\n  vi &lt;- (rchisq(k, n1 + n2 - 2) / (n1 + n2 - 2)) * (1/n1 + 1/n2)\n  data.frame(yi, vi)\n}",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Meta-analysis and publication bias</span>"
    ]
  },
  {
    "objectID": "chapters/chapter5.html#simulating-from-sampling-distributions-extra-3",
    "href": "chapters/chapter5.html#simulating-from-sampling-distributions-extra-3",
    "title": "\n5  Meta-analysis and publication bias\n",
    "section": "\n7.17 Simulating from sampling distributions [#extra]",
    "text": "7.17 Simulating from sampling distributions [#extra]\n\nsim_k_umd(k = 10, D = 0.5, n1 = 50)\n#&gt;           yi         vi\n#&gt; 1  0.5738466 0.04315912\n#&gt; 2  0.6738248 0.03468778\n#&gt; 3  0.8680684 0.03492845\n#&gt; 4  0.3577568 0.04286838\n#&gt; 5  0.4943219 0.04206381\n#&gt; 6  0.8086167 0.04057567\n#&gt; 7  0.6781413 0.04862282\n#&gt; 8  0.7103435 0.04557039\n#&gt; 9  0.1040651 0.03661780\n#&gt; 10 0.3135765 0.03810990",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Meta-analysis and publication bias</span>"
    ]
  },
  {
    "objectID": "chapters/chapter5.html#simulating-from-sampling-distributions-extra-4",
    "href": "chapters/chapter5.html#simulating-from-sampling-distributions-extra-4",
    "title": "\n5  Meta-analysis and publication bias\n",
    "section": "\n7.18 Simulating from sampling distributions [#extra]",
    "text": "7.18 Simulating from sampling distributions [#extra]\nWe can compare the two methods and see that we are sampling from the same data generation process.\n\nCodek &lt;- 1e4\ns_umd &lt;- sim_k_umd(k, D = 1, n1 = 50)\nip_umd &lt;- replicate(k, sim_umd(n1 = 50, D = 1), simplify = FALSE)\nip_umd &lt;- do.call(rbind, ip_umd)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Meta-analysis and publication bias</span>"
    ]
  },
  {
    "objectID": "chapters/chapter5.html#simulating-from-sampling-distributions-extra-5",
    "href": "chapters/chapter5.html#simulating-from-sampling-distributions-extra-5",
    "title": "\n5  Meta-analysis and publication bias\n",
    "section": "\n7.19 Simulating from sampling distributions [#extra]",
    "text": "7.19 Simulating from sampling distributions [#extra]\nThe actual advantage is in terms of computational speed. To simulate \\(k = 10\\) studies for 1000 times (similar to a standard Monte Carlo simulation):\n\nbench &lt;- microbenchmark::microbenchmark(\n  sampling = sim_k_umd(k = 10, n1 = 50, D = 1),\n  participant = replicate(10, sim_umd(n1 = 50, D = 1)),\n  times = 1000 \n)\n\n(bench &lt;- summary(bench))\n#&gt;          expr      min       lq      mean    median       uq      max neval cld\n#&gt; 1    sampling  121.088  127.500  135.7546  131.7425  138.205  320.583  1000  a \n#&gt; 2 participant 1533.724 1564.777 1662.7264 1597.1325 1614.285 6321.375  1000   b\n\nbench$mean[2] / bench$mean[1] # faster\n#&gt; [1] 12.24803",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Meta-analysis and publication bias</span>"
    ]
  },
  {
    "objectID": "chapters/chapter5.html#notation",
    "href": "chapters/chapter5.html#notation",
    "title": "\n5  Meta-analysis and publication bias\n",
    "section": "\n8.1 Notation",
    "text": "8.1 Notation\nMeta-analysis notation is a little bit inconsistent in textbooks and papers. We define here some rules to simplify the work.\n\n\n\\(k\\) is the number of studies\n\n\\(n_j\\) is the sample size of the group \\(j\\) within a study\n\n\\(y_i\\) are the observed effect size included in the meta-analysis\n\n\\(\\sigma_i^2\\) are the observed sampling variance of studies and \\(\\epsilon_i\\) are the sampling errors\n\n\\(\\theta\\) is the equal-effects parameter (see Equation 10.1)\n\n\\(\\delta_i\\) is the random-effect (see Equation 11.2)\n\n\\(\\mu_\\theta\\) is the average effect of a random-effects model (see Equation 11.1)\n\n\\(w_i\\) are the meta-analysis weights\n\n\\(\\tau^2\\) is the heterogeneity (see Equation 11.2)\n\n\\(\\Delta\\) is the (generic) population effect size\n\n\\(s_j^2\\) is the variance of the group \\(j\\) within a study",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Meta-analysis and publication bias</span>"
    ]
  },
  {
    "objectID": "chapters/chapter5.html#simulation-setup-1",
    "href": "chapters/chapter5.html#simulation-setup-1",
    "title": "\n5  Meta-analysis and publication bias\n",
    "section": "\n8.2 Simulation setup",
    "text": "8.2 Simulation setup\nGiven the introduction to effect sizes, from now we will simulate data using UMD and the individual-level data.\nBasically we are simulating an effect size \\(D\\) coming from the comparison of two independent groups \\(G_1\\) and \\(G_2\\).\nEach group is composed by \\(n\\) participants measured on a numerical outcome (e.g., reaction times)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Meta-analysis and publication bias</span>"
    ]
  },
  {
    "objectID": "chapters/chapter5.html#simulation-setup-2",
    "href": "chapters/chapter5.html#simulation-setup-2",
    "title": "\n5  Meta-analysis and publication bias\n",
    "section": "\n8.3 Simulation setup",
    "text": "8.3 Simulation setup\nA more general, clear and realistic approach to simulate data is by generating \\(k\\) studies with same/different sample sizes and (later) true effect sizes.\n\nk &lt;- 10 # number of studies\nn1 &lt;- n2 &lt;- 10 + rpois(k, 30 - 10) # sample size from poisson distribution with lambda 40 and minimum 10\nD &lt;- 0.5 # effect size\n\nyi &lt;- rep(NA, k)\nvi &lt;- rep(NA, k)\n  \nfor(i in 1:k){\n  g1 &lt;- rnorm(n1[i], 0, 1)\n  g2 &lt;- rnorm(n2[i], D, 1)\n  yi[i] &lt;- mean(g2) - mean(g1)\n  vi[i] &lt;- var(g1)/n1[i] + var(g2)/n2[i]\n}\n  \nsim &lt;- data.frame(id = 1:k, yi, vi)\n\nhead(sim)\n#&gt;   id        yi         vi\n#&gt; 1  1 0.4365388 0.04486757\n#&gt; 2  2 0.4558415 0.05654053\n#&gt; 3  3 0.7217043 0.04830127\n#&gt; 4  4 0.4447339 0.07858359\n#&gt; 5  5 0.2832747 0.06478037\n#&gt; 6  6 0.6719835 0.06095362",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Meta-analysis and publication bias</span>"
    ]
  },
  {
    "objectID": "chapters/chapter5.html#simulation-setup-3",
    "href": "chapters/chapter5.html#simulation-setup-3",
    "title": "\n5  Meta-analysis and publication bias\n",
    "section": "\n8.4 Simulation setup",
    "text": "8.4 Simulation setup\nWe can again put everything within a function:\n\nsim_studies &lt;- function(k, es, n1, n2 = NULL){\n  if(length(n1) == 1) n1 &lt;- rep(n1, k)\n  if(is.null(n2)) n2 &lt;- n1\n  if(length(es) == 1) es &lt;- rep(es, k)\n  \n  yi &lt;- rep(NA, k)\n  vi &lt;- rep(NA, k)\n  \n  for(i in 1:k){\n    g1 &lt;- rnorm(n1[i], 0, 1)\n    g2 &lt;- rnorm(n2[i], es[i], 1)\n    yi[i] &lt;- mean(g2) - mean(g1)\n    vi[i] &lt;- var(g1)/n1[i] + var(g2)/n2[i]\n  }\n  \n  sim &lt;- data.frame(id = 1:k, yi, vi, n1 = n1, n2 = n2)\n  \n  # convert to escalc for using metafor methods\n  sim &lt;- metafor::escalc(yi = yi, vi = vi, data = sim)\n  \n  return(sim)\n}",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Meta-analysis and publication bias</span>"
    ]
  },
  {
    "objectID": "chapters/chapter5.html#simulation-setup---disclaimer",
    "href": "chapters/chapter5.html#simulation-setup---disclaimer",
    "title": "\n5  Meta-analysis and publication bias\n",
    "section": "\n8.5 Simulation setup - Disclaimer",
    "text": "8.5 Simulation setup - Disclaimer\nThe proposed simulation approach using a for loop and separated vectors. For the purpose of the workshop this is the best option. In real-world meta-analysis simulations you can choose a more functional approach starting from a simulation grid as data.frame and mapping the simulation functions.\nFor some examples see:\n\n(Gambarota2023-on?)\nwww.jepusto.com/simulating-correlated-smds",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Meta-analysis and publication bias</span>"
    ]
  },
  {
    "objectID": "chapters/chapter5.html#simulation-setup---disclaimer-1",
    "href": "chapters/chapter5.html#simulation-setup---disclaimer-1",
    "title": "\n5  Meta-analysis and publication bias\n",
    "section": "\n8.6 Simulation setup - Disclaimer",
    "text": "8.6 Simulation setup - Disclaimer\nFor a more extended overview of the simulation setup we have an entire paper. Supplementary materials (github.com/shared-research/simulating-meta-analysis) contains also more examples for complex (multilevel and multivariate models.)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Meta-analysis and publication bias</span>"
    ]
  },
  {
    "objectID": "chapters/chapter5.html#combining-studies-1",
    "href": "chapters/chapter5.html#combining-studies-1",
    "title": "\n5  Meta-analysis and publication bias\n",
    "section": "\n9.1 Combining studies",
    "text": "9.1 Combining studies\nLet’s imagine to have \\(k = 10\\) studies, a \\(D = 0.5\\) and heterogeneous sample sizes in each study.\n\nk &lt;- 10\nD &lt;- 0.5\nn &lt;- 10 + rpois(k, lambda = 20) \ndat &lt;- sim_studies(k = k, es = D, n1 = n)\nhead(dat)\n#&gt; \n#&gt;   id      yi     vi n1 n2 \n#&gt; 1  1 -0.0933 0.0539 28 28 \n#&gt; 2  2  0.5874 0.0546 35 35 \n#&gt; 3  3 -0.1772 0.1335 23 23 \n#&gt; 4  4  0.4703 0.0617 30 30 \n#&gt; 5  5  0.2611 0.0557 32 32 \n#&gt; 6  6  0.5936 0.0607 37 37\n\n. . .\nWhat is the best way to combine the studies?",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Meta-analysis and publication bias</span>"
    ]
  },
  {
    "objectID": "chapters/chapter5.html#combining-studies-2",
    "href": "chapters/chapter5.html#combining-studies-2",
    "title": "\n5  Meta-analysis and publication bias\n",
    "section": "\n9.2 Combining studies",
    "text": "9.2 Combining studies\nWe can take the average effect size and considering it as a huge study. This can be considered the best way to combine the effects.\n\\[\n\\hat{D} = \\frac{\\sum^{k}_{i = 1} D_i}{k}\n\\]\n\nmean(dat$yi)\n#&gt; [1] 0.3721872\n\n. . .\nIt is appropriate? What do you think? Are we missing something?",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Meta-analysis and publication bias</span>"
    ]
  },
  {
    "objectID": "chapters/chapter5.html#weighting-studies",
    "href": "chapters/chapter5.html#weighting-studies",
    "title": "\n5  Meta-analysis and publication bias\n",
    "section": "\n9.3 Weighting studies",
    "text": "9.3 Weighting studies\nWe are not considering that some studies, despite providing a similar effect size could give more information. An higher sample size (or lower sampling variance) produce a more reliable estimation.\n. . .\nWould you trust more a study with \\(n = 100\\) and \\(D = 0.5\\) or a study with \\(n = 10\\) and \\(D = 0.5\\)? The “meta-analysis” that we did before is completely ignoring this information.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Meta-analysis and publication bias</span>"
    ]
  },
  {
    "objectID": "chapters/chapter5.html#weighting-studies-1",
    "href": "chapters/chapter5.html#weighting-studies-1",
    "title": "\n5  Meta-analysis and publication bias\n",
    "section": "\n9.4 Weighting studies",
    "text": "9.4 Weighting studies\nWe need to find a value (called weight \\(w_i\\)) that allows assigning more trust to a study because it provide more information.\n. . .\nThe simplest weights are just the sample size, but in practice we use the so-called inverse-variance weighting. We use the (inverse) of the sampling variance of the effect size to weight each study.\n. . .\nThe basic version of a meta-analysis is just a weighted average:\n\\[\n\\overline D_w = \\frac{\\sum^k_{i = 1}{w_iD_i}}{\\sum^k_{i = 1}{w_i}}\n\\]\n. . .\n\nwi &lt;- 1/dat$vi\nsum(dat$yi * wi) / sum(wi)\n#&gt; [1] 0.3911878\n# weighted.mean(dat$yi, wi)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Meta-analysis and publication bias</span>"
    ]
  },
  {
    "objectID": "chapters/chapter5.html#weighting-studies-2",
    "href": "chapters/chapter5.html#weighting-studies-2",
    "title": "\n5  Meta-analysis and publication bias\n",
    "section": "\n9.5 Weighting studies",
    "text": "9.5 Weighting studies\nGraphically, the two models can be represented in this way:\n\nCodedw &lt;- weighted.mean(dat$yi, 1/dat$vi)\ndunw &lt;- mean(dat$yi)\n\nunw_plot &lt;- ggplot(dat, aes(x = yi, y = factor(id))) +\n  geom_point(size = 3) +\n  xlim(c(-0.5, 1.5)) +\n  geom_vline(xintercept = dunw) +\n  xlab(latex2exp::TeX(\"$y_i$\")) +\n  ylab(\"Study\") +\n  theme_minimal(15) +\n  annotate(\"label\", x = 0.5, y = k + 0.4, label = latex2exp::TeX(sprintf(\"$\\\\bar{D} = %.2f$\", dunw))) +\n  geom_label(aes(x = 1, y = id, label = paste0(\"n = \", n1)))\n\nw_plot &lt;- ggplot(dat, aes(x = yi, y = factor(id))) +\n  geom_point(aes(size = 1/vi),\n             show.legend = FALSE) +\n  xlim(c(-0.5, 1.5)) +\n  geom_vline(xintercept = dw) +\n  xlab(latex2exp::TeX(\"$y_i$\")) +\n  ylab(\"Study\") +\n  theme_minimal(15) +\n  annotate(\"label\", x = 0.5, y = k + 0.4, label = latex2exp::TeX(sprintf(\"$\\\\bar{D}_w = %.2f$\", dw))) +\n  geom_label(aes(x = 1, y = id, label = paste0(\"n = \", n1)))\n\nunw_plot + w_plot",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Meta-analysis and publication bias</span>"
    ]
  },
  {
    "objectID": "chapters/chapter5.html#ee-meta-analysis",
    "href": "chapters/chapter5.html#ee-meta-analysis",
    "title": "\n5  Meta-analysis and publication bias\n",
    "section": "\n10.1 EE meta-analysis",
    "text": "10.1 EE meta-analysis\nWhat we did in the last example (the weighted mean) is the exactly a meta-analysis model called equal-effects (or less precisely fixed-effect). The assumptions are very simple:\n\nthere is a unique, true effect size to estimate \\(\\theta\\)\n\neach study is a more or less precise estimate of \\(\\theta\\)\n\nthere is no TRUE variability among studies. The observed variability is due to studies that are imprecise (i.e., sampling error)\nassuming that each study has a very large sample size, the observed variability is close to zero.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Meta-analysis and publication bias</span>"
    ]
  },
  {
    "objectID": "chapters/chapter5.html#ee-meta-analysis-formally",
    "href": "chapters/chapter5.html#ee-meta-analysis-formally",
    "title": "\n5  Meta-analysis and publication bias\n",
    "section": "\n10.2 EE meta-analysis, formally",
    "text": "10.2 EE meta-analysis, formally\n\\[\ny_i = \\theta + \\epsilon_i\n\\tag{10.1}\\]\n\\[\n\\epsilon_i \\sim \\mathcal{N}(0, \\sigma^2_i)\n\\tag{10.2}\\]\nWhere \\(\\sigma^2_i\\) is the vector of sampling variabilities of \\(k\\) studies. This is a standard linear model but with heterogeneous sampling variances.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Meta-analysis and publication bias</span>"
    ]
  },
  {
    "objectID": "chapters/chapter5.html#ee-meta-analysis-1",
    "href": "chapters/chapter5.html#ee-meta-analysis-1",
    "title": "\n5  Meta-analysis and publication bias\n",
    "section": "\n10.3 EE meta-analysis",
    "text": "10.3 EE meta-analysis",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Meta-analysis and publication bias</span>"
    ]
  },
  {
    "objectID": "chapters/chapter5.html#simulating-an-ee-model",
    "href": "chapters/chapter5.html#simulating-an-ee-model",
    "title": "\n5  Meta-analysis and publication bias\n",
    "section": "\n10.4 Simulating an EE model",
    "text": "10.4 Simulating an EE model\nWhat we were doing with the sim_studies() function so far was simulating an EE model. In fact, there were a single \\(\\theta\\) parameter and the observed variability was a function of the rnorm() randomness.\nBased on previous assumptions and thinking a little bit, what could be the result of simulating studies with a very large \\(n\\)?\n. . .\n\nns &lt;- c(10, 50, 100, 1000, 1e4)\nD &lt;- 0.5\ndats &lt;- lapply(ns, function(n) sim_studies(10, es = D, n1 = n))",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Meta-analysis and publication bias</span>"
    ]
  },
  {
    "objectID": "chapters/chapter5.html#sec-ee-impact-n",
    "href": "chapters/chapter5.html#sec-ee-impact-n",
    "title": "\n5  Meta-analysis and publication bias\n",
    "section": "\n10.5 Simulating an EE modelm",
    "text": "10.5 Simulating an EE modelm",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Meta-analysis and publication bias</span>"
    ]
  },
  {
    "objectID": "chapters/chapter5.html#simulating-an-ee-model-1",
    "href": "chapters/chapter5.html#simulating-an-ee-model-1",
    "title": "\n5  Meta-analysis and publication bias\n",
    "section": "\n10.6 Simulating an EE model",
    "text": "10.6 Simulating an EE model\nFormulating the model as a intercept-only regression (see Equations Equation 10.1 and Equation 10.2) we can generate data directly:\n\nD &lt;- 0.5\nn &lt;- 30\nk &lt;- 10\n\nyi &lt;- D + rnorm(k, 0, sqrt(1/n + 1/n))\n# or equivalently\n# yi &lt;- rnorm(k, D, sqrt(1/n + 1/n))\n\nAs we did for the aggregated data approach. Clearly we need to simulate also the vi vector from the appropriate distribution. Given that we simulated data starting from the participant-level the uncertainty of yi and vi is already included.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Meta-analysis and publication bias</span>"
    ]
  },
  {
    "objectID": "chapters/chapter5.html#fitting-an-ee-model",
    "href": "chapters/chapter5.html#fitting-an-ee-model",
    "title": "\n5  Meta-analysis and publication bias\n",
    "section": "\n10.7 Fitting an EE model",
    "text": "10.7 Fitting an EE model\nThe model can be fitted using the metafor::rma() function, with method = \"EE\"1.\n\ntheta &lt;- 0.5\nk &lt;- 15\nn &lt;- 10 + rpois(k, 30 - 10)\ndat &lt;- sim_studies(k = k, es = theta, n1 = n)\nfit &lt;- rma(yi, vi, data = dat, method = \"EE\")\nsummary(fit)\n#&gt; \n#&gt; Equal-Effects Model (k = 15)\n#&gt; \n#&gt;   logLik  deviance       AIC       BIC      AICc   \n#&gt;  -2.7508   18.5609    7.5016    8.2097    7.8093   \n#&gt; \n#&gt; I^2 (total heterogeneity / total variability):   24.57%\n#&gt; H^2 (total variability / sampling variability):  1.33\n#&gt; \n#&gt; Test for Heterogeneity:\n#&gt; Q(df = 14) = 18.5609, p-val = 0.1824\n#&gt; \n#&gt; Model Results:\n#&gt; \n#&gt; estimate      se    zval    pval   ci.lb   ci.ub      \n#&gt;   0.4242  0.0663  6.3950  &lt;.0001  0.2942  0.5541  *** \n#&gt; \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Meta-analysis and publication bias</span>"
    ]
  },
  {
    "objectID": "chapters/chapter5.html#interpreting-an-ee-model",
    "href": "chapters/chapter5.html#interpreting-an-ee-model",
    "title": "\n5  Meta-analysis and publication bias\n",
    "section": "\n10.8 Interpreting an EE model",
    "text": "10.8 Interpreting an EE model\n\nThe first section (logLik, deviance, etc.) presents some general model statistics and information criteria\nThe \\(I^2\\) and \\(H^2\\) are statistics evaluating the observed heterogeneity (see next slides)\nThe Test of Heterogeneity section presents the test of the \\(Q\\) statistics for the observed heterogeneity (see next slides)\nThe Model Results section presents the estimation of the \\(\\theta\\) parameter along with the standard error and the Wald \\(z\\) test (\\(H_0: \\theta = 0\\))\n\nThe metafor package has a several well documented functions to calculate and plot model results, residuals analysis etc.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Meta-analysis and publication bias</span>"
    ]
  },
  {
    "objectID": "chapters/chapter5.html#interpreting-an-ee-model-1",
    "href": "chapters/chapter5.html#interpreting-an-ee-model-1",
    "title": "\n5  Meta-analysis and publication bias\n",
    "section": "\n10.9 Interpreting an EE model",
    "text": "10.9 Interpreting an EE model\n\nplot(fit) # general plots",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Meta-analysis and publication bias</span>"
    ]
  },
  {
    "objectID": "chapters/chapter5.html#interpreting-an-ee-model-2",
    "href": "chapters/chapter5.html#interpreting-an-ee-model-2",
    "title": "\n5  Meta-analysis and publication bias\n",
    "section": "\n10.10 Interpreting an EE Model",
    "text": "10.10 Interpreting an EE Model\nThe main function for plotting model results is the forest() function that produce the forest plot.\n\nforest(fit)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Meta-analysis and publication bias</span>"
    ]
  },
  {
    "objectID": "chapters/chapter5.html#interpreting-an-ee-model-3",
    "href": "chapters/chapter5.html#interpreting-an-ee-model-3",
    "title": "\n5  Meta-analysis and publication bias\n",
    "section": "\n10.11 Interpreting an EE Model",
    "text": "10.11 Interpreting an EE Model\nWe did not introduced the concept of heterogeneity, but the \\(I^2\\), \\(H^2\\) and \\(Q\\) statistics basically evaluate if the observed heterogeneity should be attributed to sampling variability (uncertainty in estimating \\(\\theta\\) because we have a limited \\(k\\) and \\(n\\)) or sampling variability plus other sources of heterogeneity.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Meta-analysis and publication bias</span>"
    ]
  },
  {
    "objectID": "chapters/chapter5.html#ee-model-as-a-weighted-average",
    "href": "chapters/chapter5.html#ee-model-as-a-weighted-average",
    "title": "\n5  Meta-analysis and publication bias\n",
    "section": "\n10.12 EE model as a weighted Average",
    "text": "10.12 EE model as a weighted Average\nFormally \\(\\theta\\) is estimated as (see Borenstein et al., 2009, p. 66)\n\\[\n\\hat{\\theta} = \\frac{\\sum^k_{i = 1}{w_iy_i}}{\\sum^k_{i = 1}{w_i}}; \\;\\;\\; w_i = \\frac{1}{\\sigma^2_i}\n\\]\n\\[\nSE_{\\theta} = \\frac{1}{\\sum^k_{i = 1}{w_i}}\n\\]\n\nwi &lt;- 1/dat$vi\ntheta_hat &lt;- with(dat, sum(yi * wi)/sum(wi))\nse_theta_hat &lt;- sqrt(1/sum(wi))\nc(theta = theta_hat, se = se_theta_hat, z = theta_hat / se_theta_hat)\n#&gt;      theta         se          z \n#&gt; 0.42415231 0.06632602 6.39496065",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Meta-analysis and publication bias</span>"
    ]
  },
  {
    "objectID": "chapters/chapter5.html#are-the-ee-assumptions-realistic",
    "href": "chapters/chapter5.html#are-the-ee-assumptions-realistic",
    "title": "\n5  Meta-analysis and publication bias\n",
    "section": "\n11.1 Are the EE assumptions realistic?",
    "text": "11.1 Are the EE assumptions realistic?\nThe EE model is appropriate if our studies are somehow exact replications of the exact same effect. We are assuming that there is no real variability.\n. . .\nHowever, meta-analysis rarely report the results of \\(k\\) exact replicates. It is more common to include studies answering the same research question but with different methods, participants, etc.\n. . .\n\npeople with different ages or other participant-level differences\ndifferent methodology\n…",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Meta-analysis and publication bias</span>"
    ]
  },
  {
    "objectID": "chapters/chapter5.html#are-the-ee-assumptions-realistic-1",
    "href": "chapters/chapter5.html#are-the-ee-assumptions-realistic-1",
    "title": "\n5  Meta-analysis and publication bias\n",
    "section": "\n11.2 Are the EE assumptions realistic?",
    "text": "11.2 Are the EE assumptions realistic?\n. . .\nIf we relax the previous assumption we are able to combine studies that are not exact replications.\n. . .\nThus the real effect \\(\\theta\\) is no longer a single true value but can be larger or smaller in some conditions.\n. . .\nIn other terms we are assuming that there could be some variability (i.e., heterogeneity) among studies that is independent from the sample size. Even with studies with \\(\\lim_{n\\to\\infty}\\) the observed variability is not zero.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Meta-analysis and publication bias</span>"
    ]
  },
  {
    "objectID": "chapters/chapter5.html#random-effects-model-re",
    "href": "chapters/chapter5.html#random-effects-model-re",
    "title": "\n5  Meta-analysis and publication bias\n",
    "section": "\n11.3 Random-effects model (RE)",
    "text": "11.3 Random-effects model (RE)\nWe can extend the EE model including another source of variability, \\(\\tau^2\\). \\(\\tau^2\\) is the true heterogeneity among studies caused by methdological differences or intrisic variability in the phenomenon.\nFormally we can extend Equation 10.1 as: \\[\ny_i = \\mu_{\\theta} + \\delta_i + \\epsilon_i\n\\tag{11.1}\\]\n\\[\n\\delta_i \\sim \\mathcal{N}(0, \\tau^2)\n\\tag{11.2}\\]\n\\[\n\\epsilon_i \\sim \\mathcal{N}(0, \\sigma^2_i)\n\\]\nWhere \\(\\mu_{\\theta}\\) is the average effect size and \\(\\delta_i\\) is the study-specific deviation from the average effect (regulated by \\(\\tau^2\\)). Clearly each study specific effect is \\(\\theta_i = \\mu_{\\theta} + \\delta_i\\).",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Meta-analysis and publication bias</span>"
    ]
  },
  {
    "objectID": "chapters/chapter5.html#re-model",
    "href": "chapters/chapter5.html#re-model",
    "title": "\n5  Meta-analysis and publication bias\n",
    "section": "\n11.4 RE model",
    "text": "11.4 RE model",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Meta-analysis and publication bias</span>"
    ]
  },
  {
    "objectID": "chapters/chapter5.html#re-model-estimation",
    "href": "chapters/chapter5.html#re-model-estimation",
    "title": "\n5  Meta-analysis and publication bias\n",
    "section": "\n11.5 RE model estimation",
    "text": "11.5 RE model estimation\nGiven that we extended the EE model equation. Also the estimation of the average effect need to be extended. Basically the RE is still a weighted average but weights need to include also \\(\\tau^2\\).\n\\[\n\\overline y = \\frac{\\sum_{i = 1}^k y_iw^*_i}{\\sum_{i = 1}^k w^*_i}\n\\tag{11.3}\\]\n\\[\nw^*_i = \\frac{1}{\\sigma^2_i + \\tau^2}\n\\tag{11.4}\\]\nThe weights are different compared to the EE model. Extremely precise/imprecise studies will have less impact in the RE model.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Meta-analysis and publication bias</span>"
    ]
  },
  {
    "objectID": "chapters/chapter5.html#re-vs-ee-model",
    "href": "chapters/chapter5.html#re-vs-ee-model",
    "title": "\n5  Meta-analysis and publication bias\n",
    "section": "\n11.6 RE vs EE model",
    "text": "11.6 RE vs EE model\nThe crucial difference with the EE model is that even with large \\(n\\), only the \\(\\mu_{\\theta} + \\delta_i\\) are estimated (almost) without error. As long \\(\\tau^2 \\neq 0\\) there will be variability in the effect sizes.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Meta-analysis and publication bias</span>"
    ]
  },
  {
    "objectID": "chapters/chapter5.html#simulating-a-re-model",
    "href": "chapters/chapter5.html#simulating-a-re-model",
    "title": "\n5  Meta-analysis and publication bias\n",
    "section": "\n11.7 Simulating a RE Model",
    "text": "11.7 Simulating a RE Model\nTo simulate the RE model we simply need to include \\(\\tau^2\\) in the EE model simulation.\n\nk &lt;- 15 # number of studies\nmu &lt;- 0.5 # average effect\ntau2 &lt;- 0.1 # heterogeneity\nn &lt;- 10 + rpois(k, 30 - 10) # sample size\ndeltai &lt;- rnorm(k, 0, sqrt(tau2)) # random-effects\nthetai &lt;- mu + deltai # true study effect\n\ndat &lt;- sim_studies(k = k, es = thetai, n1 = n)\n\nhead(dat)\n#&gt; \n#&gt;   id     yi     vi n1 n2 \n#&gt; 1  1 0.8142 0.0532 31 31 \n#&gt; 2  2 0.1607 0.0837 28 28 \n#&gt; 3  3 0.9681 0.0720 31 31 \n#&gt; 4  4 0.3218 0.0557 32 32 \n#&gt; 5  5 0.5520 0.0796 28 28 \n#&gt; 6  6 0.7136 0.0447 32 32",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Meta-analysis and publication bias</span>"
    ]
  },
  {
    "objectID": "chapters/chapter5.html#simulating-a-re-model-1",
    "href": "chapters/chapter5.html#simulating-a-re-model-1",
    "title": "\n5  Meta-analysis and publication bias\n",
    "section": "\n11.8 Simulating a RE model",
    "text": "11.8 Simulating a RE model\nAgain, we can put everything within a function expanding the previous sim_studies() by including \\(\\tau^2\\):\nsim_studies &lt;- function(k, es, tau2 = 0, n1, n2 = NULL, add = NULL){\n  if(length(n1) == 1) n1 &lt;- rep(n1, k)\n  if(is.null(n2)) n2 &lt;- n1\n  if(length(es) == 1) es &lt;- rep(es, k)\n  \n  yi &lt;- rep(NA, k)\n  vi &lt;- rep(NA, k)\n  \n  # random effects\n  deltai &lt;- rnorm(k, 0, sqrt(tau2))\n  \n  for(i in 1:k){\n    g1 &lt;- rnorm(n1[i], 0, 1)\n    g2 &lt;- rnorm(n2[i], es[i] + deltai[i], 1)\n    yi[i] &lt;- mean(g2) - mean(g1)\n    vi[i] &lt;- var(g1)/n1[i] + var(g2)/n2[i]\n  }\n  \n  sim &lt;- data.frame(id = 1:k, yi, vi, n1 = n1, n2 = n2)\n  \n  if(!is.null(add)){\n    sim &lt;- cbind(sim, add)\n  }\n  \n  # convert to escalc for using metafor methods\n  sim &lt;- metafor::escalc(yi = yi, vi = vi, data = sim)\n  \n  return(sim)\n}",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Meta-analysis and publication bias</span>"
    ]
  },
  {
    "objectID": "chapters/chapter5.html#simulating-a-re-model-2",
    "href": "chapters/chapter5.html#simulating-a-re-model-2",
    "title": "\n5  Meta-analysis and publication bias\n",
    "section": "\n11.9 Simulating a RE model",
    "text": "11.9 Simulating a RE model\nThe data are similar to the EE simulation but we have an extra source of heterogeneity.\n\nCodedat |&gt;\n  summary() |&gt;\n  qforest()",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Meta-analysis and publication bias</span>"
    ]
  },
  {
    "objectID": "chapters/chapter5.html#simulating-a-re-model-3",
    "href": "chapters/chapter5.html#simulating-a-re-model-3",
    "title": "\n5  Meta-analysis and publication bias\n",
    "section": "\n11.10 Simulating a RE model",
    "text": "11.10 Simulating a RE model\nTo see the actual impact of \\(\\tau^2\\) we can follow the same approach of Section 10.5 thus using a large \\(n\\). The sampling variance vi of each study is basically 0.\n\n# ... other parameters as before\nn &lt;- 1e4\ndeltai &lt;- rnorm(k, 0, sqrt(tau2)) # random-effects\nthetai &lt;- mu + deltai # true study effect\ndat &lt;- sim_studies(k = k, es = thetai, n1 = n)\n# or equivalently \n# dat &lt;- sim_studies(k = k, es = mu, tau2 = tau2, n1 = n)\n\nhead(dat)\n#&gt; \n#&gt;   id     yi     vi    n1    n2 \n#&gt; 1  1 0.5101 0.0002 10000 10000 \n#&gt; 2  2 0.3129 0.0002 10000 10000 \n#&gt; 3  3 0.2215 0.0002 10000 10000 \n#&gt; 4  4 0.2343 0.0002 10000 10000 \n#&gt; 5  5 0.4691 0.0002 10000 10000 \n#&gt; 6  6 0.6939 0.0002 10000 10000",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Meta-analysis and publication bias</span>"
    ]
  },
  {
    "objectID": "chapters/chapter5.html#simulating-a-re-model-4",
    "href": "chapters/chapter5.html#simulating-a-re-model-4",
    "title": "\n5  Meta-analysis and publication bias\n",
    "section": "\n11.11 Simulating a RE Model",
    "text": "11.11 Simulating a RE Model\nClearly, compared to Section 10.5, even with large \\(n\\) the variability is not reduced because \\(\\tau^2 \\neq 0\\). As \\(\\tau^2\\) approach zero the EE and RE models are similar.\n\nCodedat |&gt;\n  summary() |&gt;\n  qforest()",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Meta-analysis and publication bias</span>"
    ]
  },
  {
    "objectID": "chapters/chapter5.html#re-model-estimation-1",
    "href": "chapters/chapter5.html#re-model-estimation-1",
    "title": "\n5  Meta-analysis and publication bias\n",
    "section": "\n11.12 RE model estimation",
    "text": "11.12 RE model estimation\nGiven that we extended the EE model equation. Also the estimation of the average effect need to be extended. Basically the RE is still a weighted average but weights need to include also \\(\\tau^2\\).\n\\[\n\\overline y = \\frac{\\sum_{i = 1}^k y_iw^*_i}{\\sum_{i = 1}^k w^*_i}\n\\tag{11.5}\\]\n\\[\nw^*_i = \\frac{1}{\\sigma^2_i + \\tau^2}\n\\tag{11.6}\\]\nThe weights are different compared to the EE model. Extremely precise/imprecise studies will have less impact in the RE model.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Meta-analysis and publication bias</span>"
    ]
  },
  {
    "objectID": "chapters/chapter5.html#fitting-a-re-model",
    "href": "chapters/chapter5.html#fitting-a-re-model",
    "title": "\n5  Meta-analysis and publication bias\n",
    "section": "\n11.13 Fitting a RE model",
    "text": "11.13 Fitting a RE model\nIn R we can use the metafor::rma() function using the method = \"REML\".\n\nfit &lt;- rma(yi, vi, data = dat, method = \"REML\")\nsummary(fit)\n#&gt; \n#&gt; Random-Effects Model (k = 15; tau^2 estimator: REML)\n#&gt; \n#&gt;   logLik  deviance       AIC       BIC      AICc   \n#&gt;  -3.2908    6.5816   10.5816   11.8597   11.6725   \n#&gt; \n#&gt; tau^2 (estimated amount of total heterogeneity): 0.0935 (SE = 0.0354)\n#&gt; tau (square root of estimated tau^2 value):      0.3058\n#&gt; I^2 (total heterogeneity / total variability):   99.79%\n#&gt; H^2 (total variability / sampling variability):  469.23\n#&gt; \n#&gt; Test for Heterogeneity:\n#&gt; Q(df = 14) = 6580.9456, p-val &lt; .0001\n#&gt; \n#&gt; Model Results:\n#&gt; \n#&gt; estimate      se    zval    pval   ci.lb   ci.ub      \n#&gt;   0.5637  0.0790  7.1325  &lt;.0001  0.4088  0.7186  *** \n#&gt; \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Meta-analysis and publication bias</span>"
    ]
  },
  {
    "objectID": "chapters/chapter5.html#intepreting-the-re-model",
    "href": "chapters/chapter5.html#intepreting-the-re-model",
    "title": "\n5  Meta-analysis and publication bias\n",
    "section": "\n11.14 Intepreting the RE model",
    "text": "11.14 Intepreting the RE model\nThe model output is quite similar to the EE model and also the intepretation is similar.\nThe only extra section is tau^2/tau that is the estimation of the between-study heterogeneity.\n\nsummary(fit)\n#&gt; \n#&gt; Random-Effects Model (k = 15; tau^2 estimator: REML)\n#&gt; \n#&gt;   logLik  deviance       AIC       BIC      AICc   \n#&gt;  -3.2908    6.5816   10.5816   11.8597   11.6725   \n#&gt; \n#&gt; tau^2 (estimated amount of total heterogeneity): 0.0935 (SE = 0.0354)\n#&gt; tau (square root of estimated tau^2 value):      0.3058\n#&gt; I^2 (total heterogeneity / total variability):   99.79%\n#&gt; H^2 (total variability / sampling variability):  469.23\n#&gt; \n#&gt; Test for Heterogeneity:\n#&gt; Q(df = 14) = 6580.9456, p-val &lt; .0001\n#&gt; \n#&gt; Model Results:\n#&gt; \n#&gt; estimate      se    zval    pval   ci.lb   ci.ub      \n#&gt;   0.5637  0.0790  7.1325  &lt;.0001  0.4088  0.7186  *** \n#&gt; \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Meta-analysis and publication bias</span>"
    ]
  },
  {
    "objectID": "chapters/chapter5.html#estimating-tau2",
    "href": "chapters/chapter5.html#estimating-tau2",
    "title": "\n5  Meta-analysis and publication bias\n",
    "section": "\n11.15 Estimating \\(\\tau^2\\)\n",
    "text": "11.15 Estimating \\(\\tau^2\\)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Meta-analysis and publication bias</span>"
    ]
  },
  {
    "objectID": "chapters/chapter5.html#estimating-tau2-1",
    "href": "chapters/chapter5.html#estimating-tau2-1",
    "title": "\n5  Meta-analysis and publication bias\n",
    "section": "\n11.16 Estimating \\(\\tau^2\\)\n",
    "text": "11.16 Estimating \\(\\tau^2\\)\n\nThe Restricted Maximum Likelihood (REML) estimator is considered one of the best. We can compare the results using the all_rma() custom function that tests all the estimators2.\n\nfitl &lt;- all_rma(fit)\nround(filor::compare_rma(fitlist = fitl), 3)\n#&gt;           DL     HE     HS    HSk     SJ     ML   REML     EB     PM    PMM\n#&gt; b      0.564  0.564  0.564  0.564  0.564  0.564  0.564  0.564  0.564  0.564\n#&gt; se     0.079  0.079  0.076  0.079  0.079  0.076  0.079  0.079  0.079  0.081\n#&gt; zval   7.126  7.133  7.376  7.126  7.133  7.383  7.132  7.133  7.133  6.962\n#&gt; pval   0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000\n#&gt; ci.lb  0.409  0.409  0.414  0.409  0.409  0.414  0.409  0.409  0.409  0.405\n#&gt; ci.ub  0.719  0.719  0.713  0.719  0.719  0.713  0.719  0.719  0.719  0.722\n#&gt; I2    99.787 99.787 99.772 99.787 99.787 99.772 99.787 99.787 99.787 99.797\n#&gt; tau2   0.094  0.093  0.087  0.094  0.093  0.087  0.093  0.093  0.093  0.098",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Meta-analysis and publication bias</span>"
    ]
  },
  {
    "objectID": "chapters/chapter5.html#intepreting-heterogeneity-tau2",
    "href": "chapters/chapter5.html#intepreting-heterogeneity-tau2",
    "title": "\n5  Meta-analysis and publication bias\n",
    "section": "\n11.17 Intepreting heterogeneity \\(\\tau^2\\)\n",
    "text": "11.17 Intepreting heterogeneity \\(\\tau^2\\)\n\nLooking at Equation 11.2, \\(\\tau^2\\) is essentially the variance of the random-effect. This means that we can intepret it as the variability (or the standard deviation) of the true effect size distribution.\n\nCodetau2s &lt;- c(0.01, 0.05, 0.1, 0.2)\ntau2s_t &lt;- latex2exp::TeX(sprintf(\"$\\\\tau^2 = %.2f$\", tau2s))\n\npar(mfrow = c(1, 3))\nhist(rnorm(1e4, 0, sqrt(tau2s[1])), xlim = c(-2, 2), xlab = latex2exp::TeX(\"$y_i$\"), main = tau2s_t[1], probability = TRUE, ylim = c(0, 4.5), col = \"dodgerblue\")\nhist(rnorm(1e4, 0, sqrt(tau2s[2])), xlim = c(-2, 2), xlab = latex2exp::TeX(\"$y_i$\"), main = tau2s_t[2], probability = TRUE, ylim = c(0, 4.5), col = \"dodgerblue\")\n#hist(rnorm(1e4, 0, sqrt(tau2s[3])), xlim = c(-2, 2), xlab = latex2exp::TeX(\"$y_i$\"), main = tau2s_t[3], probability = TRUE, ylim = c(0, 5))\nhist(rnorm(1e4, 0, sqrt(tau2s[4])), xlim = c(-2, 2), xlab = latex2exp::TeX(\"$y_i$\"), main = tau2s_t[4], probability = TRUE, ylim = c(0, 4.5), col = \"dodgerblue\")",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Meta-analysis and publication bias</span>"
    ]
  },
  {
    "objectID": "chapters/chapter5.html#intepreting-tau2",
    "href": "chapters/chapter5.html#intepreting-tau2",
    "title": "\n5  Meta-analysis and publication bias\n",
    "section": "\n11.18 Intepreting \\(\\tau^2\\)\n",
    "text": "11.18 Intepreting \\(\\tau^2\\)\n\nAs in the previus plot we can assume \\(n = \\infty\\) and generate true effects from Equation 11.2. In this way we understand the impact of assuming (or estimating) a certain \\(\\tau^2\\).\nFor example, a \\(\\tau = 0.2\\) and a \\(\\mu_{\\theta} = 0.5\\), 50% of the true effects ranged between:\n\nD &lt;- 0.5\nyis &lt;- D + rnorm(1e5, 0, 0.2)\nquantile(yis, c(0.75, 0.25))\n#&gt;       75%       25% \n#&gt; 0.6355544 0.3662390",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Meta-analysis and publication bias</span>"
    ]
  },
  {
    "objectID": "chapters/chapter5.html#the-q-statistics",
    "href": "chapters/chapter5.html#the-q-statistics",
    "title": "\n5  Meta-analysis and publication bias\n",
    "section": "\n11.19 The \\(Q\\) Statistics3\n",
    "text": "11.19 The \\(Q\\) Statistics3\n\nThe Q statistics is used to make inference on the heterogeneity. Can be considered as a weighted sum of squares:\n\\[\nQ = \\sum^k_{i = 1}w_i(y_i - \\hat \\mu)^2\n\\]\nWhere \\(\\hat \\mu\\) is EE estimation (regardless if \\(\\tau^2 \\neq 0\\)) and \\(w_i\\) are the inverse-variance weights. Note that in the case of \\(w_1 = w_2 ... = w_i\\), Q is just a standard sum of squares (or deviance).",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Meta-analysis and publication bias</span>"
    ]
  },
  {
    "objectID": "chapters/chapter5.html#the-q-statistics-1",
    "href": "chapters/chapter5.html#the-q-statistics-1",
    "title": "\n5  Meta-analysis and publication bias\n",
    "section": "\n11.20 The \\(Q\\) Statistics",
    "text": "11.20 The \\(Q\\) Statistics\n\nGiven that we are summing up squared distances, they should be approximately \\(\\chi^2\\) with \\(df = k - 1\\). In case of no heterogeneity (\\(\\tau^2 = 0\\)) the observed variability is only caused by sampling error and the expectd value of the \\(\\chi^2\\) is just the degrees of freedom (\\(df = k - 1\\)).\nIn case of \\(\\tau^2 \\neq 0\\), the expected value is \\(k - 1 + \\lambda\\) where \\(\\lambda\\) is a non-centrality parameter.\nIn other terms, if the expected value of \\(Q\\) exceed the expected value assuming no heterogeneity, we have evidence that \\(\\tau^2 \\neq 0\\).",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Meta-analysis and publication bias</span>"
    ]
  },
  {
    "objectID": "chapters/chapter5.html#the-q-statistics-2",
    "href": "chapters/chapter5.html#the-q-statistics-2",
    "title": "\n5  Meta-analysis and publication bias\n",
    "section": "\n11.21 The \\(Q\\) Statistics",
    "text": "11.21 The \\(Q\\) Statistics\nLet’s try a more practical approach. We simulate a lot of meta-analysis with and without heterogeneity and we check the Q statistics.\n\nCodeget_Q &lt;- function(yi, vi){\n  wi &lt;- 1/vi\n  theta_ee &lt;- weighted.mean(yi, wi)\n  sum(wi*(yi - theta_ee)^2)\n}\n\nk &lt;- 30\nn &lt;- 30\ntau2 &lt;- 0.1\nnsim &lt;- 1e4\n\nQs_tau2_0 &lt;- rep(0, nsim)\nQs_tau2 &lt;- rep(0, nsim)\nres2_tau2_0 &lt;- vector(\"list\", nsim)\nres2_tau2 &lt;- vector(\"list\", nsim)\n\nfor(i in 1:nsim){\n  dat_tau2_0 &lt;- sim_studies(k = 30, es = 0.5, tau2 = 0, n1 = n)\n  dat_tau2 &lt;- sim_studies(k = 30, es = 0.5, tau2 = tau2, n1 = n)\n  \n  theta_ee_tau2_0 &lt;- weighted.mean(dat_tau2_0$yi, 1/dat_tau2_0$vi)\n  theta_ee &lt;- weighted.mean(dat_tau2$yi, 1/dat_tau2$vi)\n  \n  res2_tau2_0[[i]] &lt;- dat_tau2_0$yi - theta_ee_tau2_0\n  res2_tau2[[i]] &lt;- dat_tau2$yi - theta_ee\n  \n  Qs_tau2_0[i] &lt;- get_Q(dat_tau2_0$yi, dat_tau2_0$vi)\n  Qs_tau2[i] &lt;- get_Q(dat_tau2$yi, dat_tau2$vi)\n}",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Meta-analysis and publication bias</span>"
    ]
  },
  {
    "objectID": "chapters/chapter5.html#the-q-statistics-3",
    "href": "chapters/chapter5.html#the-q-statistics-3",
    "title": "\n5  Meta-analysis and publication bias\n",
    "section": "\n11.22 The \\(Q\\) Statistics",
    "text": "11.22 The \\(Q\\) Statistics\nLet’s try a more practical approach. We simulate a lot of meta-analysis with and without heterogeneity and we check the Q statistics.\n. . .\n\nclearly, in the presence of heterogeneity, the expected value of the Q statistics is higher (due to \\(\\lambda \\neq 0\\)) and also residuals are larger (the \\(\\chi^2\\) is just a sum of squared weighted residuals)\n\n. . .\n\nwe can calculate a p-value for deviation from the \\(\\tau^2 = 0\\) case as evidence agaist the absence of heterogeneity",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Meta-analysis and publication bias</span>"
    ]
  },
  {
    "objectID": "chapters/chapter5.html#i2-higgins2002-fh",
    "href": "chapters/chapter5.html#i2-higgins2002-fh",
    "title": "\n5  Meta-analysis and publication bias\n",
    "section": "\n11.23 \\(I^2\\) (Higgins & Thompson, 2002)\n",
    "text": "11.23 \\(I^2\\) (Higgins & Thompson, 2002)\n\nWe have two sources of variability in a random-effects meta-analysis, the sampling variability \\(\\sigma_i^2\\) and true heterogeneity \\(\\tau^2\\). We can use the \\(I^2\\) to express the interplay between the two. \\[\nI^2 = 100\\% \\times \\frac{\\hat{\\tau}^2}{\\hat{\\tau}^2 + \\tilde{v}}\n\\tag{11.7}\\]\n\\[\n\\tilde{v} = \\frac{(k-1) \\sum w_i}{(\\sum w_i)^2 - \\sum w_i^2},\n\\]\nWhere \\(\\tilde{v}\\) is the typical sampling variability. \\(I^2\\) is intepreted as the proportion of total variability due to real heterogeneity (i.e., \\(\\tau^2\\))",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Meta-analysis and publication bias</span>"
    ]
  },
  {
    "objectID": "chapters/chapter5.html#i2-higgins2002-fh-1",
    "href": "chapters/chapter5.html#i2-higgins2002-fh-1",
    "title": "\n5  Meta-analysis and publication bias\n",
    "section": "\n11.24 \\(I^2\\) (Higgins & Thompson, 2002)4\n",
    "text": "11.24 \\(I^2\\) (Higgins & Thompson, 2002)4\n\nNote that we can have the same \\(I^2\\) in two completely different meta-analysis. An high \\(I^2\\) does not represent high heterogeneity. Let’s assume to have two meta-analysis with \\(k\\) studies and small (\\(n = 30\\)) vs large (\\(n = 500\\)) sample sizes.\nLet’s solve Equation 11.7 for \\(\\tau^2\\) (using filor::tau2_from_I2()) and we found that the same \\(I^2\\) can be obtained with two completely different \\(\\tau^2\\) values:\n\nn_1 &lt;- 30\nvi_1 &lt;- 1/n_1 + 1/n_1\ntau2_1 &lt;- filor::tau2_from_I2(0.8, vi_1)\ntau2_1\n#&gt; [1] 0.2666667\n\nn_2 &lt;- 500\nvi_2 &lt;- 1/n_2 + 1/n_2\ntau2_2 &lt;- filor::tau2_from_I2(0.8, vi_2)\ntau2_2\n#&gt; [1] 0.016",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Meta-analysis and publication bias</span>"
    ]
  },
  {
    "objectID": "chapters/chapter5.html#i2-higgins2002-fh-2",
    "href": "chapters/chapter5.html#i2-higgins2002-fh-2",
    "title": "\n5  Meta-analysis and publication bias\n",
    "section": "\n11.25 \\(I^2\\) (Higgins & Thompson, 2002)\n",
    "text": "11.25 \\(I^2\\) (Higgins & Thompson, 2002)\n\n\nn_1 &lt;- 30\nvi_1 &lt;- 1/n_1 + 1/n_1\ntau2_1 &lt;- filor::tau2_from_I2(0.8, vi_1)\ntau2_1\n#&gt; [1] 0.2666667\n\nn_2 &lt;- 500\nvi_2 &lt;- 1/n_2 + 1/n_2\ntau2_2 &lt;- filor::tau2_from_I2(0.8, vi_2)\ntau2_2\n#&gt; [1] 0.016\n\n. . .\nIn other terms, the \\(I^2\\) can be considered a good index of heterogeneity only when the total variance (\\(\\tilde{v} + \\tau^2\\)) is similar.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Meta-analysis and publication bias</span>"
    ]
  },
  {
    "objectID": "chapters/chapter5.html#what-about-tildev",
    "href": "chapters/chapter5.html#what-about-tildev",
    "title": "\n5  Meta-analysis and publication bias\n",
    "section": "\n11.26 What about \\(\\tilde{v}\\)?",
    "text": "11.26 What about \\(\\tilde{v}\\)?\n\\(\\tilde{v}\\) is considered the “typical” within-study variability (see https://www.metafor-project.org/doku.php/tips:i2_multilevel_multivariate). There are different estimators but Equation 11.8 is the most common.\n\\[\n\\tilde{v} = \\frac{(k-1) \\sum w_i}{(\\sum w_i)^2 - \\sum w_i^2}\n\\tag{11.8}\\]",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Meta-analysis and publication bias</span>"
    ]
  },
  {
    "objectID": "chapters/chapter5.html#what-about-tildev-1",
    "href": "chapters/chapter5.html#what-about-tildev-1",
    "title": "\n5  Meta-analysis and publication bias\n",
    "section": "\n11.27 What about \\(\\tilde{v}\\)?",
    "text": "11.27 What about \\(\\tilde{v}\\)?\nIn the hypothetical case where \\(\\sigma^2_1 = \\dots = \\sigma^2_k\\), \\(\\tilde{v}\\) is just \\(\\sigma^2\\). This fact is commonly used to calculate the statistical power analytically (Borenstein et al., 2009, Chapter 29).\n\nvtilde &lt;- function(wi){\n  k &lt;- length(wi)\n  (k - 1) * sum(wi) / (sum(wi)^2 - sum(wi^2))\n}\n\nk &lt;- 20\n\n# same vi\nvi &lt;- rep((1/30 + 1/30), k)\nhead(vi)\n#&gt; [1] 0.06666667 0.06666667 0.06666667 0.06666667 0.06666667 0.06666667\nvtilde(1/vi)\n#&gt; [1] 0.06666667\n\n# heterogeneous vi\nn &lt;- 10 + rpois(k, 30 - 10)\nvi &lt;- sim_vi(k = k, n1 = n)\nvtilde(1/vi)\n#&gt; [1] 0.06389075",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Meta-analysis and publication bias</span>"
    ]
  },
  {
    "objectID": "chapters/chapter5.html#what-about-tildev-2",
    "href": "chapters/chapter5.html#what-about-tildev-2",
    "title": "\n5  Meta-analysis and publication bias\n",
    "section": "\n11.28 What about \\(\\tilde{v}\\)?",
    "text": "11.28 What about \\(\\tilde{v}\\)?\nUsing simulations we can see that \\(\\tilde{v}\\) with heterogenenous variances (i.e., sample sizes in this case) can be approximated by the central tendency of the sample size distribution. Note that we are fixing \\(\\sigma^2 = 1\\) thus we are not including uncertainty.\n\nCodek &lt;- 100 # number of studies\nn &lt;- 30 # sample size\n\nvti &lt;- rep(NA, 1e5)\n\nfor(i in 1:1e5){\n  ni &lt;- rpois(k, n)\n  vi &lt;- 1/ni + 1/ni\n  vti[i] &lt;- vtilde(1/vi)\n}\n\n# vtilde calculated from lambda\nvt &lt;- 1/n + 1/n",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Meta-analysis and publication bias</span>"
    ]
  },
  {
    "objectID": "chapters/chapter5.html#h2",
    "href": "chapters/chapter5.html#h2",
    "title": "\n5  Meta-analysis and publication bias\n",
    "section": "\n11.29 \\(H^2\\)\n",
    "text": "11.29 \\(H^2\\)\n\nThe \\(H^2\\) is an alternative index of heterogeneity. Is calculated as:\n\\[\nH^2 = \\frac{Q}{k - 1}\n\\]\nWe defined \\(Q\\) as the weighted sum of squares representing the total variability. \\(k - 1\\) is the expected value of the \\(\\chi^2\\) statistics (i.e., sum of squares) when \\(\\tau^2 = 0\\) (or \\(\\lambda = 0\\)).\nThus \\(H^2\\) is the ratio between total heterogeneity and sampling variability. Higher \\(H^2\\) is associated with higher heterogeneity relative to the sampling variability. \\(H^2\\) is not a measure of absolute heterogeneity.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Meta-analysis and publication bias</span>"
    ]
  },
  {
    "objectID": "chapters/chapter5.html#h2-1",
    "href": "chapters/chapter5.html#h2-1",
    "title": "\n5  Meta-analysis and publication bias\n",
    "section": "\n11.30 \\(H^2\\)\n",
    "text": "11.30 \\(H^2\\)\n\nWhen we are fitting a RE model, the \\(I^2\\) and \\(H^2\\) equations are slightly different (Higgins & Thompson, 2002). See also the metafor source code.\n\nk &lt;- 100\nmu &lt;- 0.5\ntau2 &lt;- 0.1\nn &lt;- 30\n\ndat &lt;- sim_studies(k = k, es = mu, tau2 = tau2, n1 = n)\nfit_re &lt;- rma(yi, vi, data = dat, method = \"REML\")\nfit_ee &lt;- rma(yi, vi, data = dat, method = \"EE\")\n\n# H2 with EE model\n\ntheta_ee &lt;- fit_ee$b[[1]] # weighted.mean(dat$yi, 1/dat$vi)\nwi &lt;- 1/dat$vi\nQ &lt;- with(dat, sum((1/vi)*(yi - theta_ee)^2))\nc(Q, fit_ee$QE) # same\n#&gt; [1] 350.1654 350.1654\n\nc(H2 = fit_ee$QE / (fit_ee$k - fit_ee$p), H2_model = fit_ee$H2) # same\n#&gt;       H2 H2_model \n#&gt; 3.537024 3.537024\n\n# H2 with RE model\n\nvt &lt;- vtilde(1/dat$vi)\nc(H2 = fit_re$tau2 / vt + 1, H2_model = fit_re$H2) # same\n#&gt;       H2 H2_model \n#&gt; 3.495119 3.495119",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Meta-analysis and publication bias</span>"
    ]
  },
  {
    "objectID": "chapters/chapter5.html#confidence-intervals",
    "href": "chapters/chapter5.html#confidence-intervals",
    "title": "\n5  Meta-analysis and publication bias\n",
    "section": "\n11.31 Confidence Intervals",
    "text": "11.31 Confidence Intervals\nWhat is reported in the model summary as ci.lb and ci.ub refers to the 95% confidence interval representing the uncertainty in estimating the effect (or a meta-regression parameter).\nWithout looking at the equations, let’s try to implement this idea using simulations.\n\nchoose \\(k\\), \\(\\tau^2\\) and \\(n\\)\n\nsimulate data (several times) accordingly and fit the RE model\nextract the estimated effect size\ncompare the simulated sampling distribution with the analytical result",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Meta-analysis and publication bias</span>"
    ]
  },
  {
    "objectID": "chapters/chapter5.html#confidence-intervals-1",
    "href": "chapters/chapter5.html#confidence-intervals-1",
    "title": "\n5  Meta-analysis and publication bias\n",
    "section": "\n11.32 Confidence Intervals",
    "text": "11.32 Confidence Intervals\n\nk &lt;- 30\nn &lt;- 30\ntau2 &lt;- 0.05\nmu &lt;- 0.5\nnsim &lt;- 5e3\n\n# true parameters (see Borenstein, 2009; Chapter 29)\nvt &lt;- 1/n + 1/n\nvs &lt;- (vt + tau2)/ k\nse &lt;- sqrt(vs)\n\nmui &lt;- rep(NA, nsim)\n\nfor(i in 1:nsim){\n  dat &lt;- sim_studies(k = k, es = mu, tau2 = tau2, n1 = n)\n  fit &lt;- rma(yi, vi, data = dat)\n  mui[i] &lt;- coef(fit)[1]\n}\n\n# standard error\nc(simulated = sd(mui), analytical = fit$se)\n#&gt;  simulated analytical \n#&gt; 0.06322640 0.06508356\n\n# confidence interval\nrbind(\n  \"simulated\"  = quantile(mui, c(0.05, 0.975)),\n  \"analytical\" = c(\"2.5%\" = fit$ci.lb, \"97.5%\" = fit$ci.ub)\n)\n#&gt;                   5%     97.5%\n#&gt; simulated  0.3958958 0.6226868\n#&gt; analytical 0.3721610 0.6272838",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Meta-analysis and publication bias</span>"
    ]
  },
  {
    "objectID": "chapters/chapter5.html#confidence-intervals-2",
    "href": "chapters/chapter5.html#confidence-intervals-2",
    "title": "\n5  Meta-analysis and publication bias\n",
    "section": "\n11.33 Confidence Intervals",
    "text": "11.33 Confidence Intervals\n\nhist(mui, breaks = 50, freq = FALSE, main = \"Sampling Distribution\", xlab = latex2exp::TeX(\"$\\\\mu_{\\\\theta}$\"))\ncurve(dnorm(x, mu, se), add = TRUE, col = \"firebrick\", lwd = 1.5)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Meta-analysis and publication bias</span>"
    ]
  },
  {
    "objectID": "chapters/chapter5.html#confidence-intervals-3",
    "href": "chapters/chapter5.html#confidence-intervals-3",
    "title": "\n5  Meta-analysis and publication bias\n",
    "section": "\n11.34 Confidence Intervals",
    "text": "11.34 Confidence Intervals\nNow the equation for the 95% confidence interval should be more clear. The standard error is a function of the within study sampling variances (depending mainly on \\(n\\)), \\(\\tau^2\\) and \\(k\\). As we increase \\(k\\) the standard error tends towards zero.\n\\[\nCI = \\hat \\mu_{\\theta} \\pm z SE_{\\mu_{\\theta}}\n\\]\n\\[\nSE_{\\mu_{\\theta}} = \\sqrt{\\frac{1}{\\sum^{k}_{i = 1}w^{\\star}_i}}\n\\]\n\\[\nw^{\\star}_i = \\frac{1}{\\sigma^2_i + \\tau^2}\n\\]",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Meta-analysis and publication bias</span>"
    ]
  },
  {
    "objectID": "chapters/chapter5.html#confidence-intervals-4",
    "href": "chapters/chapter5.html#confidence-intervals-4",
    "title": "\n5  Meta-analysis and publication bias\n",
    "section": "\n11.35 Confidence Intervals",
    "text": "11.35 Confidence Intervals\nWe can also see it analytically, there is a huge impact of \\(k\\).\n\nCode# true parameters (see Borenstein, 2009; Chapter 29)\nvt &lt;- 1/n + 1/n\nvs &lt;- (vt + tau2)/ k\nse &lt;- sqrt(vs)\n\nk &lt;- c(10, 50, 100, 500, 1000, 5000)\nn &lt;- c(10, 50, 100, 500, 1000, 5000)\ntau2 &lt;- c(0, 0.05, 0.1, 0.2)\n\ndd &lt;- expand.grid(k = k, n = n, tau2 = tau2)\n\ndd$vt &lt;- with(dd, 1/n + 1/n)\ndd$vs &lt;- with(dd, (vt + tau2)/ k)\ndd$se &lt;- sqrt(dd$vs)\n\ndd$k &lt;- as_tex_label(dd$k, \"$k = %s$\")\n\nggplot(dd, aes(x = n, y = se, color = factor(tau2))) +\n  geom_line() +\n  facet_wrap(~k, labeller = label_parsed) +\n  labs(color = latex2exp::TeX(\"\\\\tau^2\")) +\n  xlab(\"Sample Size (n)\") +\n  ylab(latex2exp::TeX(\"$SE_{\\\\mu_{\\\\theta}}$\"))",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Meta-analysis and publication bias</span>"
    ]
  },
  {
    "objectID": "chapters/chapter5.html#prediction-intervals-pi",
    "href": "chapters/chapter5.html#prediction-intervals-pi",
    "title": "\n5  Meta-analysis and publication bias\n",
    "section": "\n11.36 Prediction intervals (PI)",
    "text": "11.36 Prediction intervals (PI)\nWe could say that the CI is not completely taking into account the between-study heterogeneity (\\(\\tau^2\\)). After a meta-analysis we would like to know how confident we are in the parameters estimation BUT also what would be the expected effect running a new experiment tomorrow?.\nThe prediction interval (IntHout et al., 2016; Riley et al., 2011) is exactly the range of effects that I expect in predicting a new study.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Meta-analysis and publication bias</span>"
    ]
  },
  {
    "objectID": "chapters/chapter5.html#pi-for-a-sample-mean",
    "href": "chapters/chapter5.html#pi-for-a-sample-mean",
    "title": "\n5  Meta-analysis and publication bias\n",
    "section": "\n11.37 PI for a sample mean",
    "text": "11.37 PI for a sample mean\nTo understand the concept, let’s assume to have a sample \\(X\\) of size \\(n\\) and we estimate the mean \\(\\overline X\\). The PI is calculated as5:\n\\[\nPI = \\overline X \\pm t_{\\alpha/2} s_x \\sqrt{1 + \\frac{1}{n}}\n\\]\nWhere \\(s\\) is the sample standard deviation. Basically we are combining the uncertainty in estimating \\(\\overline X\\) (i.e, \\(\\frac{s_x}{n}\\)) with the standard deviation of the data \\(s_x\\). Compare it with the confidence interval containing only \\(\\frac{s_x}{n}\\).",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Meta-analysis and publication bias</span>"
    ]
  },
  {
    "objectID": "chapters/chapter5.html#pi-in-meta-analysis",
    "href": "chapters/chapter5.html#pi-in-meta-analysis",
    "title": "\n5  Meta-analysis and publication bias\n",
    "section": "\n11.38 PI in meta-analysis",
    "text": "11.38 PI in meta-analysis\nFor meta-analysis the equation6 is conceptually similar but with different quantities.\n\\[\nPI = \\hat \\mu_{\\theta} \\pm z \\sqrt{\\tau^2 + SE_{\\mu_{\\theta}}}\n\\]\nBasically we are combining all the sources of uncertainty. As long as \\(\\tau^2 \\neq 0\\) the PI is greater than the CI (in the EE model they are the same). Thus even with very precise \\(\\mu_{\\theta}\\) estimation, large \\(\\tau^2\\) leads to uncertain predictions.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Meta-analysis and publication bias</span>"
    ]
  },
  {
    "objectID": "chapters/chapter5.html#pi-in-meta-analysis-1",
    "href": "chapters/chapter5.html#pi-in-meta-analysis-1",
    "title": "\n5  Meta-analysis and publication bias\n",
    "section": "\n11.39 PI in meta-analysis",
    "text": "11.39 PI in meta-analysis\nIn R the PI can be calculated using predict(). By default the model assume a standard normal distribution thus using \\(z\\) scores. To use the Riley et al. (2011) approach (\\(t\\) distribution) the model need to be fitted using test = \"t\".\n\nk &lt;- 100\ndat &lt;- sim_studies(k = k, es = 0.5, tau2 = 0.1, n1 = 30)\nfit_z &lt;- rma(yi, vi, data = dat, test = \"z\") # test = \"z\" is the default\npredict(fit_z) # notice pi.ub/pi.lb vs ci.ub/ci.lb\n#&gt; \n#&gt;    pred     se  ci.lb  ci.ub   pi.lb  pi.ub \n#&gt;  0.4423 0.0424 0.3592 0.5254 -0.2299 1.1145\n# manually\nfit_z$b[[1]] + qnorm(c(0.025, 0.975)) * sqrt(fit_z$se^2 + fit_z$tau2)\n#&gt; [1] -0.229882  1.114516\n\nfit_t &lt;- rma(yi, vi, data = dat, test = \"t\")\npredict(fit_t) # notice pi.ub/pi.lb vs ci.ub/ci.lb\n#&gt; \n#&gt;    pred     se  ci.lb  ci.ub   pi.lb  pi.ub \n#&gt;  0.4423 0.0424 0.3582 0.5265 -0.2382 1.1228\n# manually\nfit_z$b[[1]] + qt(c(0.025, 0.975), k - 2) * sqrt(fit_t$se^2 + fit_t$tau2)\n#&gt; [1] -0.2382858  1.1229198",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Meta-analysis and publication bias</span>"
    ]
  },
  {
    "objectID": "chapters/chapter5.html#ma-as-weighted-linear-regression",
    "href": "chapters/chapter5.html#ma-as-weighted-linear-regression",
    "title": "\n5  Meta-analysis and publication bias\n",
    "section": "\n12.1 MA as (weighted) linear regression",
    "text": "12.1 MA as (weighted) linear regression\nBoth the EE and RE model can be seen as standard (weighted) linear regression models. Precisely, there is a difference in fitting a meta-analysis using lm or lme4::lmer() and rma (see https://www.metafor-project.org/doku.php/tips:rma_vs_lm_lme_lmer).\n. . .\nBeyond these differences a general the EE and RE models are intercept-only linear regressions.\n\\[\n\\boldsymbol{Y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\epsilon}\n\\]\nThe EE model:\n\\[\ny_i = \\beta_0 + \\epsilon_i\n\\]\nThe RE model:\n\\[\ny_i = \\beta_0 + \\beta_{0_i} + \\epsilon_i\n\\]",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Meta-analysis and publication bias</span>"
    ]
  },
  {
    "objectID": "chapters/chapter5.html#ma-as-weighted-linear-regression-1",
    "href": "chapters/chapter5.html#ma-as-weighted-linear-regression-1",
    "title": "\n5  Meta-analysis and publication bias\n",
    "section": "\n12.2 MA as (weighted) linear regression",
    "text": "12.2 MA as (weighted) linear regression\nIn the EE model \\(\\beta_0\\) is \\(\\theta\\) and \\(\\epsilon_i \\sim \\mathcal{N}(0, \\sigma^2_i)\\)\n\\[\ny_i = \\beta_0 + \\epsilon_i\n\\]\nIn the RE model \\(\\beta_0\\) is \\(\\mu_{\\theta}\\) and \\(\\beta_{0_i}\\) are the \\(\\delta_i\\).",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Meta-analysis and publication bias</span>"
    ]
  },
  {
    "objectID": "chapters/chapter5.html#explaining-tau2",
    "href": "chapters/chapter5.html#explaining-tau2",
    "title": "\n5  Meta-analysis and publication bias\n",
    "section": "\n12.3 Explaining \\(\\tau^2\\)\n",
    "text": "12.3 Explaining \\(\\tau^2\\)\n\nSo far we simply assumed \\(\\tau^2 = 0\\) (for the EE model) or estimated it using the RE model.\n. . .\nWe can extend the intercept-only meta-analysis by including study-level predictors (as in standard linear regression) to explain the estimated true heterogeneity.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Meta-analysis and publication bias</span>"
    ]
  },
  {
    "objectID": "chapters/chapter5.html#explaining-tau2-1",
    "href": "chapters/chapter5.html#explaining-tau2-1",
    "title": "\n5  Meta-analysis and publication bias\n",
    "section": "\n12.4 Explaining \\(\\tau^2\\)\n",
    "text": "12.4 Explaining \\(\\tau^2\\)\n\nLet’s make an example where we simulate a meta-analysis with \\(k = 100\\) studies. Beyond the effect size, we extracted an experimental condition where 50 studies where lab-based experiments \\(x_{lab}\\) and 50 studies where online experiments.\nWe assume that there could be a lab effect thus we included a predictor in the model.\n\nk &lt;- 100\nn &lt;- 10 + rpois(k, 40 - 10)\nexp &lt;- rep(c(\"lab\", \"online\"), each = k/2)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Meta-analysis and publication bias</span>"
    ]
  },
  {
    "objectID": "chapters/chapter5.html#explaining-tau2-2",
    "href": "chapters/chapter5.html#explaining-tau2-2",
    "title": "\n5  Meta-analysis and publication bias\n",
    "section": "\n12.5 Explaining \\(\\tau^2\\)\n",
    "text": "12.5 Explaining \\(\\tau^2\\)\n\nNow the model have a predictor \\(x\\) (the type of experiment) and two parameters \\(\\beta_0\\) and \\(\\beta_1\\). Depending on the contrast coding (default to contr.treatment() in R) the \\(\\beta_0\\) is different. Coding exp as 0 for lab-based experiments and 1 for online experiments:\n\\[\ny_i = \\beta_0 + \\beta_1X_{1_i} + \\epsilon_i\n\\]\n\\[\ny_{\\text{lab}_i} = \\beta_0 + \\epsilon_i\n\\]\n\\[\ny_{\\text{online}_i} = \\beta_0 + \\beta_1 + \\epsilon_i\n\\]",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Meta-analysis and publication bias</span>"
    ]
  },
  {
    "objectID": "chapters/chapter5.html#explaining-tau2-3",
    "href": "chapters/chapter5.html#explaining-tau2-3",
    "title": "\n5  Meta-analysis and publication bias\n",
    "section": "\n12.6 Explaining \\(\\tau^2\\)\n",
    "text": "12.6 Explaining \\(\\tau^2\\)\n\nWhat is missing is the random-effect. Basically we still have \\(\\tau^2\\) determining the \\(\\delta_i \\sim \\mathcal{N}(0, \\tau^2)\\) but now is the residual \\(\\tau^2_r\\). The heterogeneity after including the predictor.\n\\[\ny_i = \\beta_0 + \\beta_{0_i} + \\beta_1X_{1_i} + \\epsilon_i\n\\tag{12.1}\\]\n\\[\n\\beta_{0_i} \\sim \\mathcal{N}(0, \\tau^2_r)\n\\]\nClearly the difference between \\(\\tau^2\\) (the total heterogeneity) and \\(\\tau^2_r\\) (residual heterogeneity) is an index of the impact of \\(X\\).",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Meta-analysis and publication bias</span>"
    ]
  },
  {
    "objectID": "chapters/chapter5.html#simulating-the-x-effect",
    "href": "chapters/chapter5.html#simulating-the-x-effect",
    "title": "\n5  Meta-analysis and publication bias\n",
    "section": "\n12.7 Simulating the \\(X\\) effect",
    "text": "12.7 Simulating the \\(X\\) effect\nTo simulate a meta-regression we just need to choose the parameters values (\\(\\beta_0\\) and \\(\\beta_1\\)) and implement Equation 12.1. Using treatment coding, \\(\\beta_0\\) is the effect size when \\(X = 0\\) (i.e., lab-based experiments) and \\(\\beta_1\\) is the difference between lab and online experiments.\n\nb0 &lt;- 0.3 # lab-based effect size\nb1 &lt;- 0.5 # online - lab-based --&gt; online = b0 + b1\nexp_dummy &lt;- ifelse(exp == \"lab\", 0, 1) # dummy version\nes &lt;- b0 + b1 * exp_dummy\nht(data.frame(exp, exp_dummy, es))\n#&gt;        exp exp_dummy  es\n#&gt; 1      lab         0 0.3\n#&gt; 2      lab         0 0.3\n#&gt; 3      lab         0 0.3\n#&gt; 4      lab         0 0.3\n#&gt; 5      lab         0 0.3\n#&gt; 95  online         1 0.8\n#&gt; 96  online         1 0.8\n#&gt; 97  online         1 0.8\n#&gt; 98  online         1 0.8\n#&gt; 99  online         1 0.8\n#&gt; 100 online         1 0.8",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Meta-analysis and publication bias</span>"
    ]
  },
  {
    "objectID": "chapters/chapter5.html#simulating-the-x-effects",
    "href": "chapters/chapter5.html#simulating-the-x-effects",
    "title": "\n5  Meta-analysis and publication bias\n",
    "section": "\n12.8 Simulating the \\(X\\) effects",
    "text": "12.8 Simulating the \\(X\\) effects\nNow we can use the sim_studies() function as usual. The difference is that es is no longer a single value but a vector (with different values according to the \\(X\\) level) and tau2 is \\(\\tau^2_r\\) (this the leftover heterogeneity after including the \\(X\\) effect)\n\ntau2r &lt;- 0.05 # residual heterogeneity\ndat &lt;- sim_studies(k = k, es = es, tau2 = tau2r, n1 = n, add = list(exp = exp))\nht(dat)\n#&gt; \n#&gt;      id      yi     vi n1 n2    exp \n#&gt; 1     1  0.3868 0.0634 40 40    lab \n#&gt; 2     2  0.7162 0.0620 40 40    lab \n#&gt; 3     3 -0.0864 0.0601 36 36    lab \n#&gt; 4     4  0.0536 0.0516 40 40    lab \n#&gt; 5     5  0.2547 0.0573 34 34    lab \n#&gt; 95   95  1.4493 0.0530 31 31 online \n#&gt; 96   96  0.9304 0.0527 41 41 online \n#&gt; 97   97  1.0688 0.0420 47 47 online \n#&gt; 98   98  0.6288 0.0511 38 38 online \n#&gt; 99   99  1.2018 0.0462 41 41 online \n#&gt; 100 100  0.6452 0.0702 29 29 online",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Meta-analysis and publication bias</span>"
    ]
  },
  {
    "objectID": "chapters/chapter5.html#fitting-a-meta-regression-model",
    "href": "chapters/chapter5.html#fitting-a-meta-regression-model",
    "title": "\n5  Meta-analysis and publication bias\n",
    "section": "\n12.9 Fitting a meta-regression Model",
    "text": "12.9 Fitting a meta-regression Model\nTo fit a meta-regression we still use the metafor::rma() function, adding the mods = ~ parameter with the model formula (same as the right-hand side of a y ~ x call in lm). The name of the predictor in the formula need to match a column of the data = dataframe.\n\nfit &lt;- rma(yi, vi, mods = ~ exp, data = dat, method = \"REML\")\nsummary(fit)\n#&gt; \n#&gt; Mixed-Effects Model (k = 100; tau^2 estimator: REML)\n#&gt; \n#&gt;   logLik  deviance       AIC       BIC      AICc   \n#&gt; -30.6576   61.3152   67.3152   75.0701   67.5706   \n#&gt; \n#&gt; tau^2 (estimated amount of residual heterogeneity):     0.0610 (SE = 0.0158)\n#&gt; tau (square root of estimated tau^2 value):             0.2471\n#&gt; I^2 (residual heterogeneity / unaccounted variability): 55.34%\n#&gt; H^2 (unaccounted variability / sampling variability):   2.24\n#&gt; R^2 (amount of heterogeneity accounted for):            48.11%\n#&gt; \n#&gt; Test for Residual Heterogeneity:\n#&gt; QE(df = 98) = 220.5886, p-val &lt; .0001\n#&gt; \n#&gt; Test of Moderators (coefficient 2):\n#&gt; QM(df = 1) = 53.2544, p-val &lt; .0001\n#&gt; \n#&gt; Model Results:\n#&gt; \n#&gt;            estimate      se    zval    pval   ci.lb   ci.ub      \n#&gt; intrcpt      0.3301  0.0472  6.9990  &lt;.0001  0.2377  0.4226  *** \n#&gt; exponline    0.4871  0.0667  7.2976  &lt;.0001  0.3563  0.6179  *** \n#&gt; \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Meta-analysis and publication bias</span>"
    ]
  },
  {
    "objectID": "chapters/chapter5.html#intepreting-a-meta-regression-model",
    "href": "chapters/chapter5.html#intepreting-a-meta-regression-model",
    "title": "\n5  Meta-analysis and publication bias\n",
    "section": "\n12.10 Intepreting a meta-regression Model",
    "text": "12.10 Intepreting a meta-regression Model\nThe output is similar to the RE model with few additions:\n\nEverything related to the heterogeneity (\\(H^2\\), \\(I^2\\), \\(Q\\), etc.) is now about residual heterogeneity\n\nThere is the (pseudo) \\(R^2\\)\n\nThere is an overall test for the moderators \\(Q_M\\)\n\nThere is a section (similar to standard regression models) with the estimated parameters, standard error and Wald test",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Meta-analysis and publication bias</span>"
    ]
  },
  {
    "objectID": "chapters/chapter5.html#model-parameters",
    "href": "chapters/chapter5.html#model-parameters",
    "title": "\n5  Meta-analysis and publication bias\n",
    "section": "\n12.11 Model parameters",
    "text": "12.11 Model parameters\nintrcpt and exponline are the estimates of \\(\\beta_0\\) and \\(\\beta_1\\). The interpretation depends on the scale of the effect size and the contrast coding.\nWe can plot the model results using the metafor::regplot()7.\n\nregplot(fit)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Meta-analysis and publication bias</span>"
    ]
  },
  {
    "objectID": "chapters/chapter5.html#omnibus-moderator-test",
    "href": "chapters/chapter5.html#omnibus-moderator-test",
    "title": "\n5  Meta-analysis and publication bias\n",
    "section": "\n12.12 Omnibus Moderator Test",
    "text": "12.12 Omnibus Moderator Test\nThe Test of Moderators section report the so-called omnibus test for model coeffiecients. Is a simultaneous test for 1 or more coefficients where \\(H_0: \\beta_j = 0\\).\nIn this case, coefficient 2 means that we are testing only the 2nd coefficient \\(\\beta_1\\). By default, the intercept is ignored. In fact, the exponline line and the omnibus test are the same (the \\(\\chi^2\\) is just the \\(z^2\\))\n\n#&gt; Test of Moderators (coefficient 2):\n#&gt; QM(df = 1) = 53.2544, p-val &lt; .0001\n#&gt;            estimate      se    zval    pval   ci.lb   ci.ub      \n#&gt; intrcpt      0.3301  0.0472  6.9990  &lt;.0001  0.2377  0.4226  *** \n#&gt; exponline    0.4871  0.0667  7.2976  &lt;.0001  0.3563  0.6179  ***",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Meta-analysis and publication bias</span>"
    ]
  },
  {
    "objectID": "chapters/chapter5.html#general-linear-hypotheses-testing-glht",
    "href": "chapters/chapter5.html#general-linear-hypotheses-testing-glht",
    "title": "\n5  Meta-analysis and publication bias\n",
    "section": "\n12.13 General Linear Hypotheses Testing (GLHT)",
    "text": "12.13 General Linear Hypotheses Testing (GLHT)\nWe can also test any combination of parameters. For example we could test if lab-based experiments and online experiments are both different from 0. This is the same as fitting a model without the intercept8 thus estimating the cell means (see Schad et al., 2020).\n\n# now we are testing two coefficients\nfit_no_int &lt;- rma(yi, vi, mods = ~ 0 + exp, data = dat)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Meta-analysis and publication bias</span>"
    ]
  },
  {
    "objectID": "chapters/chapter5.html#general-linear-hypotheses-testing-glht-1",
    "href": "chapters/chapter5.html#general-linear-hypotheses-testing-glht-1",
    "title": "\n5  Meta-analysis and publication bias\n",
    "section": "\n12.14 General Linear Hypotheses Testing (GLHT)",
    "text": "12.14 General Linear Hypotheses Testing (GLHT)\n\nfit_no_int\n#&gt; \n#&gt; Mixed-Effects Model (k = 100; tau^2 estimator: REML)\n#&gt; \n#&gt; tau^2 (estimated amount of residual heterogeneity):     0.0610 (SE = 0.0158)\n#&gt; tau (square root of estimated tau^2 value):             0.2471\n#&gt; I^2 (residual heterogeneity / unaccounted variability): 55.34%\n#&gt; H^2 (unaccounted variability / sampling variability):   2.24\n#&gt; \n#&gt; Test for Residual Heterogeneity:\n#&gt; QE(df = 98) = 220.5886, p-val &lt; .0001\n#&gt; \n#&gt; Test of Moderators (coefficients 1:2):\n#&gt; QM(df = 2) = 348.4548, p-val &lt; .0001\n#&gt; \n#&gt; Model Results:\n#&gt; \n#&gt;            estimate      se     zval    pval   ci.lb   ci.ub      \n#&gt; explab       0.3301  0.0472   6.9990  &lt;.0001  0.2377  0.4226  *** \n#&gt; exponline    0.8172  0.0472  17.3052  &lt;.0001  0.7247  0.9098  *** \n#&gt; \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Meta-analysis and publication bias</span>"
    ]
  },
  {
    "objectID": "chapters/chapter5.html#general-linear-hypotheses-testing-glht-2",
    "href": "chapters/chapter5.html#general-linear-hypotheses-testing-glht-2",
    "title": "\n5  Meta-analysis and publication bias\n",
    "section": "\n12.15 General Linear Hypotheses Testing (GLHT)",
    "text": "12.15 General Linear Hypotheses Testing (GLHT)\nA more elegant way is by using the GLHT framework. Basically we provide a contrast matrix expressing linear combinations of model parameters to be tested. In our case \\(\\text{lab} = \\beta_0 = 0\\) and \\(\\text{online} = \\beta_0 + \\beta_1 = 0\\).\nPractically, the matrix formulation is the following:\n\\[\n\\begin{pmatrix}  \n1 & 0 \\\\\n1 & 1\n\\end{pmatrix}\n\\begin{pmatrix}  \n\\beta_0\\\\\n\\beta_1\n\\end{pmatrix}\n=\n\\begin{pmatrix}  \n0\\\\\n0\n\\end{pmatrix}\n\\]\nIn R:\n\nC &lt;- rbind(c(1, 0), c(1, 1))\nB &lt;- coef(fit)\nC %*% B # same as coef(fit)[1] and coef(fit)[1] +  coef(fit)[2]\n#&gt;           [,1]\n#&gt; [1,] 0.3301324\n#&gt; [2,] 0.8172112",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Meta-analysis and publication bias</span>"
    ]
  },
  {
    "objectID": "chapters/chapter5.html#general-linear-hypotheses-testing-glht-3",
    "href": "chapters/chapter5.html#general-linear-hypotheses-testing-glht-3",
    "title": "\n5  Meta-analysis and publication bias\n",
    "section": "\n12.16 General Linear Hypotheses Testing (GLHT)",
    "text": "12.16 General Linear Hypotheses Testing (GLHT)\nWe can use the anova() function providing the model and the hypothesis matrix.\n\nanova(fit) # the default\n#&gt; \n#&gt; Test of Moderators (coefficient 2):\n#&gt; QM(df = 1) = 53.2544, p-val &lt; .0001\nanova(fit, X = C)\n#&gt; \n#&gt; Hypotheses:                           \n#&gt; 1:             intrcpt = 0 \n#&gt; 2: intrcpt + exponline = 0 \n#&gt; \n#&gt; Results:\n#&gt;    estimate     se    zval   pval \n#&gt; 1:   0.3301 0.0472  6.9990 &lt;.0001 \n#&gt; 2:   0.8172 0.0472 17.3052 &lt;.0001 \n#&gt; \n#&gt; Omnibus Test of Hypotheses:\n#&gt; QM(df = 2) = 348.4548, p-val &lt; .0001\n\nNotice that is the same as the model without the intercept.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Meta-analysis and publication bias</span>"
    ]
  },
  {
    "objectID": "chapters/chapter5.html#likelihood-ratio-test-lrt",
    "href": "chapters/chapter5.html#likelihood-ratio-test-lrt",
    "title": "\n5  Meta-analysis and publication bias\n",
    "section": "\n12.17 Likelihood Ratio Test (LRT)",
    "text": "12.17 Likelihood Ratio Test (LRT)\nAs in standard regression modelling, we can also compare models using LRT. The anova() function will compute the LRT when two (nested) models are provided. In this case we compared a null (intercept-only) model with the model including the predictor.\n\n# the null model\nfit0 &lt;- rma(yi, vi, data = dat, method = \"REML\")\nanova(fit0, fit, refit = TRUE) # refit = TRUE because LRT with REML is not meaningful, using ML instead\n#&gt; \n#&gt;         df      AIC      BIC     AICc   logLik     LRT   pval       QE  tau^2 \n#&gt; Full     3  66.5817  74.3972  66.8317 -30.2908                220.5886 0.0589 \n#&gt; Reduced  2 108.5228 113.7331 108.6465 -52.2614 43.9411 &lt;.0001 336.6655 0.1160 \n#&gt;              R^2 \n#&gt; Full             \n#&gt; Reduced 49.2018%",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Meta-analysis and publication bias</span>"
    ]
  },
  {
    "objectID": "chapters/chapter5.html#r2",
    "href": "chapters/chapter5.html#r2",
    "title": "\n5  Meta-analysis and publication bias\n",
    "section": "\n12.18 \\(R^2\\)\n",
    "text": "12.18 \\(R^2\\)\n\nThe \\(R^2\\) value reported in the model output is not calculated as in standard regression analysis.\n\\[\nR^2 = 1 - \\frac{\\tau^2_r}{\\tau^2}\n\\]\nBasically is the percentage of heterogeneity reduction from the intercept-only model to the model including predictors.\nIn R:\n\n(1 - fit$tau2/fit0$tau2)*100\n#&gt; [1] 48.1096\nfit$R2\n#&gt; [1] 48.1096",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Meta-analysis and publication bias</span>"
    ]
  },
  {
    "objectID": "chapters/chapter5.html#r2-1",
    "href": "chapters/chapter5.html#r2-1",
    "title": "\n5  Meta-analysis and publication bias\n",
    "section": "\n12.19 \\(R^2\\)\n",
    "text": "12.19 \\(R^2\\)\n\nDespite useful, the \\(R^2\\) has some limitations:\n\n\nLópez-López et al. (2014) showed that precise estimations require a large number of studies \\(k\\)\n\nSometimes could results in negative values (usually truncated to zero)\nDepends on the \\(\\tau^2\\) estimator\n\nMore about \\(R^2\\) and limitations can be found:\n\nhttps://www.metafor-project.org/doku.php/faq#for_mixed-effects_models_how_i\nhttps://www.metafor-project.org/doku.php/tips:ci_for_r2",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Meta-analysis and publication bias</span>"
    ]
  },
  {
    "objectID": "chapters/chapter5.html#numerical-predictor",
    "href": "chapters/chapter5.html#numerical-predictor",
    "title": "\n5  Meta-analysis and publication bias\n",
    "section": "\n12.20 Numerical predictor",
    "text": "12.20 Numerical predictor\nThe same logic of simulating a meta-regression can be applied to numerical predictors. We still have \\(\\beta_0\\) and \\(\\beta_1\\) but \\(X\\) has more levels. Let’s simulate an impact of the average participants’ age on the effect size.\n\n\n\\(\\beta_0\\) is the effect size when age is zero\n\n\\(\\beta_1\\) is the expected increase in the effect size for a unit increase in age\n\n\nHow we can choose plausible values for the parameters and parametrize the model correctly?",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Meta-analysis and publication bias</span>"
    ]
  },
  {
    "objectID": "chapters/chapter5.html#parametrize-beta_0",
    "href": "chapters/chapter5.html#parametrize-beta_0",
    "title": "\n5  Meta-analysis and publication bias\n",
    "section": "\n12.21 Parametrize \\(\\beta_0\\)\n",
    "text": "12.21 Parametrize \\(\\beta_0\\)\n\nThe intepretation (and the inference) of \\(\\beta_0\\) is strongly dependent on the type of numerical predictor. An age of zero is (probably) empirically meaningless thus the \\(\\beta_0\\) is somehow not useful.\nWe can for example mean-center (or other type of centering procedure) moving the zero on a meaningful value.\n\nage &lt;- 10:50 # the raw vector\nage0 &lt;- age - mean(age) # centering on the mean\nage20 &lt;- age - min(age) # centering on the minimum\n\nht(data.frame(age, age0, age20))\n#&gt;    age age0 age20\n#&gt; 1   10  -20     0\n#&gt; 2   11  -19     1\n#&gt; 3   12  -18     2\n#&gt; 4   13  -17     3\n#&gt; 5   14  -16     4\n#&gt; 36  45   15    35\n#&gt; 37  46   16    36\n#&gt; 38  47   17    37\n#&gt; 39  48   18    38\n#&gt; 40  49   19    39\n#&gt; 41  50   20    40",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Meta-analysis and publication bias</span>"
    ]
  },
  {
    "objectID": "chapters/chapter5.html#parametrize-beta_0-1",
    "href": "chapters/chapter5.html#parametrize-beta_0-1",
    "title": "\n5  Meta-analysis and publication bias\n",
    "section": "\n12.22 Parametrize \\(\\beta_0\\)\n",
    "text": "12.22 Parametrize \\(\\beta_0\\)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Meta-analysis and publication bias</span>"
    ]
  },
  {
    "objectID": "chapters/chapter5.html#parametrize-beta_0-2",
    "href": "chapters/chapter5.html#parametrize-beta_0-2",
    "title": "\n5  Meta-analysis and publication bias\n",
    "section": "\n12.23 Parametrize \\(\\beta_0\\)\n",
    "text": "12.23 Parametrize \\(\\beta_0\\)\n\nUsing different parametrizations will only affect the estimation (and the interpretation) of \\(\\beta_0\\). Other parameters and indexes will be the same.\n\nk &lt;- 100\nb0 &lt;- 0.2 # effect size when age 0\nb1 &lt;- 0.05 # slope (random for now)\nage &lt;- round(runif(k, 20, 50)) # sampling from uniform distribution\ntau2r &lt;- 0.05\nn &lt;- 10 + rpois(k, 30 - 10)\n\nes &lt;- b0 + b1 * age # raw\n\nage0 &lt;- age - mean(age)\nage20 &lt;- age - 20\n\ndat &lt;- sim_studies(k = k, es = es, tau2 = tau2r, n1 = n, add = list(age = age, age0 = age0, age20 = age20))\n\nfit &lt;- rma(yi, vi, mods = ~ age, data = dat)\nfit0 &lt;- rma(yi, vi, mods = ~ age0, data = dat)\nfit20 &lt;- rma(yi, vi, mods = ~ age20, data = dat)\n\n# showing the intercept\ncompare_rma(fit, fit0, fit20, extra_params = \"R2\") |&gt; \n  round(3)\n#&gt; fit: rma(yi = yi, vi = vi, mods = ~age, data = dat)\n#&gt; fit0: rma(yi = yi, vi = vi, mods = ~age0, data = dat)\n#&gt; fit20: rma(yi = yi, vi = vi, mods = ~age20, data = dat)\n#&gt;                fit   fit0  fit20\n#&gt; b (intrcpt)  0.371  2.026  1.290\n#&gt; se           0.153  0.036  0.076\n#&gt; zval         2.427 56.591 17.076\n#&gt; pval         0.015  0.000  0.000\n#&gt; ci.lb        0.071  1.956  1.142\n#&gt; ci.ub        0.671  2.096  1.438\n#&gt; R2          71.443 71.443 71.443\n#&gt; I2          49.486 49.486 49.486\n#&gt; tau2         0.063  0.063  0.063\n\n  # showing the intercept\ncompare_rma(fit, fit0, fit20, b = \"age\", extra_params = \"R2\") |&gt; \n  round(3)\n#&gt; fit: rma(yi = yi, vi = vi, mods = ~age, data = dat)\n#&gt; fit0: rma(yi = yi, vi = vi, mods = ~age0, data = dat)\n#&gt; fit20: rma(yi = yi, vi = vi, mods = ~age20, data = dat)\n#&gt;            fit   fit0  fit20\n#&gt; b (age)  0.046  0.046  0.046\n#&gt; se       0.004  0.004  0.004\n#&gt; zval    11.152 11.152 11.152\n#&gt; pval     0.000  0.000  0.000\n#&gt; ci.lb    0.038  0.038  0.038\n#&gt; ci.ub    0.054  0.054  0.054\n#&gt; R2      71.443 71.443 71.443\n#&gt; I2      49.486 49.486 49.486\n#&gt; tau2     0.063  0.063  0.063",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Meta-analysis and publication bias</span>"
    ]
  },
  {
    "objectID": "chapters/chapter5.html#choosing-beta_1",
    "href": "chapters/chapter5.html#choosing-beta_1",
    "title": "\n5  Meta-analysis and publication bias\n",
    "section": "\n12.24 Choosing \\(\\beta_1\\)\n",
    "text": "12.24 Choosing \\(\\beta_1\\)\n\nThe core of the model is \\(\\beta_1\\) that is the age effect. Compared to the categorical case where \\(\\beta_1\\) is just the standardized difference between two conditions, with numerical \\(X\\) choosing a meaningful \\(\\beta_1\\) is more challenging.\nTwo (maybe more) strategies:\n\nsimulating a lot of effects sizes fixing \\(beta_0\\) and \\(\\beta_1\\) and see the expected range of \\(y_i\\)\n\nfixing a certain \\(R^2\\) and choose the \\(\\beta_1\\) producing that \\(R^2\\)\n\n…",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Meta-analysis and publication bias</span>"
    ]
  },
  {
    "objectID": "chapters/chapter5.html#beta_1-by-simulations",
    "href": "chapters/chapter5.html#beta_1-by-simulations",
    "title": "\n5  Meta-analysis and publication bias\n",
    "section": "\n12.25 \\(\\beta_1\\) by simulations",
    "text": "12.25 \\(\\beta_1\\) by simulations\nA strategy could be to simulate from the generative model a large number of studies and see the expected range of effect size (Gelman et al., 2020, Chapter 5 and p. 97). A large number of unplausible values suggest that the chosen \\(\\beta_1\\) is probably not appropriate.\n\nk &lt;- 1e3\nn &lt;- 30\ntau2 &lt;- 0\nx &lt;- runif(k, 20, 50) # age\nb0 &lt;- 0.1\nb1 &lt;- c(0.001, 0.05, 0.2)\nesl &lt;- lapply(b1, function(b) b0 + b*x)\ndatl &lt;- lapply(esl, function(es) sim_studies(k = k, es = es, tau2 = tau2, n1 = n, add = list(x = x)))\nnames(datl) &lt;- b1\ndat &lt;- dplyr::bind_rows(datl, .id = \"b1\")\nht(dat)\n#&gt; \n#&gt;         b1   id      yi     vi n1 n2        x \n#&gt; 1    0.001    1 -0.0234 0.0649 30 30 36.66325 \n#&gt; 2    0.001    2 -0.2818 0.0528 30 30 37.59875 \n#&gt; 3    0.001    3 -0.1208 0.0969 30 30 37.89483 \n#&gt; 4    0.001    4  0.3004 0.0509 30 30 24.06663 \n#&gt; 5    0.001    5 -0.1076 0.0560 30 30 22.65956 \n#&gt; 2995   0.2  995  8.9069 0.0616 30 30 43.62926 \n#&gt; 2996   0.2  996  4.6568 0.0618 30 30 21.95671 \n#&gt; 2997   0.2  997  5.9464 0.0631 30 30 30.73639 \n#&gt; 2998   0.2  998  6.6676 0.0712 30 30 31.20466 \n#&gt; 2999   0.2  999  9.8789 0.0994 30 30 48.16696 \n#&gt; 3000   0.2 1000  5.7442 0.0702 30 30 28.71515",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Meta-analysis and publication bias</span>"
    ]
  },
  {
    "objectID": "chapters/chapter5.html#beta_1-by-simulations-1",
    "href": "chapters/chapter5.html#beta_1-by-simulations-1",
    "title": "\n5  Meta-analysis and publication bias\n",
    "section": "\n12.26 \\(\\beta_1\\) by simulations",
    "text": "12.26 \\(\\beta_1\\) by simulations\nClearly given the limited range of the \\(x\\) variable (age) some \\(\\beta_1\\) values are implausible leading to effect sizes that are out of a meaningful empirical range.\n\nCodedat$b1 &lt;- factor(dat$b1, labels = latex2exp::TeX(sprintf(\"$\\\\beta_1 = %s$\", unique(dat$b1))))\ndat |&gt; \n  ggplot(aes(x = x, y = yi)) +\n  geom_point() +\n  facet_wrap(~b1, scales = \"free_y\", labeller = label_parsed) +\n  xlab(\"Age\") +\n  ylab(latex2exp::TeX(\"$y_i$\"))",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Meta-analysis and publication bias</span>"
    ]
  },
  {
    "objectID": "chapters/chapter5.html#fixing-r2",
    "href": "chapters/chapter5.html#fixing-r2",
    "title": "\n5  Meta-analysis and publication bias\n",
    "section": "\n12.27 Fixing \\(R^2\\)\n",
    "text": "12.27 Fixing \\(R^2\\)\n\nWe can use the approach by López-López et al. (2014) where predictors \\(x\\) are sampled from a standard normal distribution (or standardized). \\(\\beta_1\\) is calculated as \\(\\beta_1 = \\sqrt{\\tau^2 R^2}\\) and the residual heterogeneity as \\(\\tau^2_r = \\tau^2 - \\beta^2_1\\).\n\nk &lt;- 100\nn &lt;- 30\ntau2 &lt;- 0.3\nR2 &lt;- 0.4\nb0 &lt;- 0.1\nb1_2 &lt;- tau2 * R2\nb1 &lt;- sqrt(b1_2)\ntau2r &lt;- tau2 - b1_2",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Meta-analysis and publication bias</span>"
    ]
  },
  {
    "objectID": "chapters/chapter5.html#fixing-r2-1",
    "href": "chapters/chapter5.html#fixing-r2-1",
    "title": "\n5  Meta-analysis and publication bias\n",
    "section": "\n12.28 Fixing \\(R^2\\)\n",
    "text": "12.28 Fixing \\(R^2\\)\n\nWe can check the simulation approach:\n\nk &lt;- 1e3\n1 - tau2r/tau2\n#&gt; [1] 0.4\nx &lt;- rnorm(k)\nes &lt;- b0 + b1 * x\ndat &lt;- sim_studies(k, es, tau2r, n1 = 1e3, add = list(x = x))\nfit &lt;- rma(yi, vi, data = dat, mods = ~x)\nsummary(fit)\n#&gt; \n#&gt; Mixed-Effects Model (k = 1000; tau^2 estimator: REML)\n#&gt; \n#&gt;    logLik   deviance        AIC        BIC       AICc   \n#&gt; -562.0096  1124.0193  1130.0193  1144.7365  1130.0434   \n#&gt; \n#&gt; tau^2 (estimated amount of residual heterogeneity):     0.1786 (SE = 0.0081)\n#&gt; tau (square root of estimated tau^2 value):             0.4226\n#&gt; I^2 (residual heterogeneity / unaccounted variability): 98.89%\n#&gt; H^2 (unaccounted variability / sampling variability):   90.40\n#&gt; R^2 (amount of heterogeneity accounted for):            43.09%\n#&gt; \n#&gt; Test for Residual Heterogeneity:\n#&gt; QE(df = 998) = 90168.4129, p-val &lt; .0001\n#&gt; \n#&gt; Test of Moderators (coefficient 2):\n#&gt; QM(df = 1) = 749.0935, p-val &lt; .0001\n#&gt; \n#&gt; Model Results:\n#&gt; \n#&gt;          estimate      se     zval    pval   ci.lb   ci.ub      \n#&gt; intrcpt    0.0830  0.0134   6.1759  &lt;.0001  0.0567  0.1094  *** \n#&gt; x          0.3628  0.0133  27.3696  &lt;.0001  0.3368  0.3888  *** \n#&gt; \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Meta-analysis and publication bias</span>"
    ]
  },
  {
    "objectID": "chapters/chapter5.html#r2-using-simulations",
    "href": "chapters/chapter5.html#r2-using-simulations",
    "title": "\n5  Meta-analysis and publication bias\n",
    "section": "\n12.29 \\(R^2\\) using simulations",
    "text": "12.29 \\(R^2\\) using simulations\nThe results from López-López et al. (2014) (and also our previous simulation) suggested that we need a large number of studies for precise \\(R^2\\) estimations. Let’s check using simulations the sampling distribution of \\(R^2\\) using a plausible meta-analysis scenario.\n\nk &lt;- 40 # number of studies\nn &lt;- 10 + rpois(k, 40 - 10) # sample size\ntau2 &lt;- 0.05 # tau ~ 0.22\nR2 &lt;- 0.3\nb0 &lt;- 0.1\nb1_2 &lt;- tau2 * R2\nb1 &lt;- sqrt(b1_2)\ntau2r &lt;- tau2 - b1_2\nnsim &lt;- 1e3\n\nR2i &lt;- rep(NA, nsim)\n\nfor(i in 1:nsim){\n  x &lt;- rnorm(k)\n  dat &lt;- sim_studies(k = k, es = b0 + b1*x, tau2 = tau2r, n1 = n, add = list(x))\n  fit &lt;- rma(yi, vi, data = dat, mods = ~x)\n  R2i[i] &lt;- fit$R2\n}",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Meta-analysis and publication bias</span>"
    ]
  },
  {
    "objectID": "chapters/chapter5.html#r2-using-simulations-1",
    "href": "chapters/chapter5.html#r2-using-simulations-1",
    "title": "\n5  Meta-analysis and publication bias\n",
    "section": "\n12.30 \\(R^2\\) using simulations",
    "text": "12.30 \\(R^2\\) using simulations\nWe estimated the true \\(R^2\\) correctly but there is a lot of uncertainty with a plausible meta-analysis scenario. There are a lot of meta-analysis also with lower \\(k\\) worsening the results.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Meta-analysis and publication bias</span>"
    ]
  },
  {
    "objectID": "chapters/chapter5.html#publication-bias-pb-1",
    "href": "chapters/chapter5.html#publication-bias-pb-1",
    "title": "\n5  Meta-analysis and publication bias\n",
    "section": "\n14.1 Publication Bias (PB)",
    "text": "14.1 Publication Bias (PB)\nThe PB is a very critical most problematic aspects of meta-analysis. Essentially the probability of publishing a paper (~and thus including into the meta-analysis) is not the same regardless of the result.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Meta-analysis and publication bias</span>"
    ]
  },
  {
    "objectID": "chapters/chapter5.html#publication-bias-disclaimer",
    "href": "chapters/chapter5.html#publication-bias-disclaimer",
    "title": "\n5  Meta-analysis and publication bias\n",
    "section": "\n14.2 Publication Bias Disclaimer!",
    "text": "14.2 Publication Bias Disclaimer!\nWe cannot (completely) solve the PB using statistical tools. The PB is a problem related to the publishing process and publishing incentives\n. . .\n\n\npre-registrations, multi-lab studies, etc. can (almost) completely solve the problem filling the literature with unbiased studies\n\n. . .\n\nthere are statistical tools to detect, estimate and correct for the publication bias. As every statistical method, they are influenced by statistical assumptions, number of studies and sample size, heterogeneity, etc.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Meta-analysis and publication bias</span>"
    ]
  },
  {
    "objectID": "chapters/chapter5.html#publication-bias-pb---the-big-picture",
    "href": "chapters/chapter5.html#publication-bias-pb---the-big-picture",
    "title": "\n5  Meta-analysis and publication bias\n",
    "section": "\n14.3 Publication Bias (PB) - The Big Picture9\n",
    "text": "14.3 Publication Bias (PB) - The Big Picture9",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Meta-analysis and publication bias</span>"
    ]
  },
  {
    "objectID": "chapters/chapter5.html#pb-under-an-ee-model",
    "href": "chapters/chapter5.html#pb-under-an-ee-model",
    "title": "\n5  Meta-analysis and publication bias\n",
    "section": "\n14.4 PB under an EE model",
    "text": "14.4 PB under an EE model\nThe easiest way to understand the PB is by simulating what happen without the PB. Let’s simulate a lot of studies (under a EE model) keeping all the results without selection (the ideal world).\n\nset.seed(2023)\nk &lt;- 1e3\nn &lt;- round(runif(k, 10, 100))\ntheta &lt;- 0.3\ndat &lt;- sim_studies(k = k, es = theta, tau2 = 0, n1 = n)\ndat &lt;- summary(dat)\n# compute 1 tail pvalue\ndat$pval1 &lt;- 1 - pnorm(dat$zi)\nht(dat)\n#&gt; \n#&gt;        id      yi     vi n1 n2    sei      zi   pval   ci.lb  ci.ub       pval1 \n#&gt; 1       1  0.3483 0.0406 52 52 0.2015  1.7287 0.0839 -0.0466 0.7431 0.041927744 \n#&gt; 2       2  0.3762 0.0605 40 40 0.2460  1.5291 0.1262 -0.1060 0.8585 0.063119366 \n#&gt; 3       3  0.0634 0.0911 25 25 0.3018  0.2101 0.8336 -0.5281 0.6549 0.416800171 \n#&gt; 4       4  0.4101 0.0487 46 46 0.2206  1.8588 0.0631 -0.0223 0.8426 0.031530757 \n#&gt; 5       5 -0.0476 0.1160 13 13 0.3405 -0.1398 0.8888 -0.7151 0.6199 0.555581669 \n#&gt; 995   995  0.3584 0.0950 24 24 0.3083  1.1625 0.2450 -0.2458 0.9625 0.122511729 \n#&gt; 996   996  0.3939 0.1697 17 17 0.4120  0.9562 0.3390 -0.4135 1.2014 0.169484321 \n#&gt; 997   997  0.5205 0.0486 35 35 0.2204  2.3610 0.0182  0.0884 0.9525 0.009112230 \n#&gt; 998   998  0.4206 0.0815 29 29 0.2854  1.4737 0.1406 -0.1388 0.9801 0.070283435 \n#&gt; 999   999  0.0625 0.0244 83 83 0.1562  0.3999 0.6893 -0.2438 0.3687 0.344631179 \n#&gt; 1000 1000  0.5547 0.0416 37 37 0.2039  2.7201 0.0065  0.1550 0.9544 0.003263493",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Meta-analysis and publication bias</span>"
    ]
  },
  {
    "objectID": "chapters/chapter5.html#pb-under-an-ee-model-1",
    "href": "chapters/chapter5.html#pb-under-an-ee-model-1",
    "title": "\n5  Meta-analysis and publication bias\n",
    "section": "\n14.5 PB under an EE model",
    "text": "14.5 PB under an EE model\n\npar(mfrow = c(1, 3))\nhist(dat$yi, breaks = 50, col = \"dodgerblue\", main = \"Effect Size\")\nplot(dat$yi, dat$pval1, pch = 19, col = ifelse(dat$pval1 &lt;= 0.05, scales::alpha(\"firebrick\", 0.5), scales::alpha(\"black\", 0.5)),\n     main = \"P value (one tail) ~ Effect size\")\nabline(h = 0.05)\nplot(dat$yi, dat$pval, pch = 19, col = ifelse(dat$pval &lt;= 0.05, scales::alpha(\"firebrick\", 0.5), scales::alpha(\"black\", 0.5)),\n     main = \"P value (two tails) ~ Effect size\")\nabline(h = 0.05)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Meta-analysis and publication bias</span>"
    ]
  },
  {
    "objectID": "chapters/chapter5.html#pb-under-an-ee-model-2",
    "href": "chapters/chapter5.html#pb-under-an-ee-model-2",
    "title": "\n5  Meta-analysis and publication bias\n",
    "section": "\n14.6 PB under an EE model",
    "text": "14.6 PB under an EE model\nThen, let’s assume that our publishing system is very strict (extreme). You can publish only if \\(p \\leq 0.05\\) on the expected direction. Then the true population of effect sizes will be truncated. Essentially we are assuming that \\(P(1|p \\leq 0.05) = 1\\) and \\(P(1|p \\leq 0.05) = 0\\).\n\n# selecting\nsign &lt;- dat$pval1 &lt;= 0.05 & dat$zi &gt; 0\ndat_pb &lt;- dat[sign, ]\ndat_un &lt;- dat[sample(1:nrow(dat), sum(sign)), ]\n\n# fitting EE model for the full vs selected (ignore k differences)\nfit &lt;- rma(yi, vi, method = \"EE\", data = dat_un)\nfit_pb &lt;- rma(yi, vi, method = \"EE\", data = dat_pb)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Meta-analysis and publication bias</span>"
    ]
  },
  {
    "objectID": "chapters/chapter5.html#pb-under-an-ee-model-3",
    "href": "chapters/chapter5.html#pb-under-an-ee-model-3",
    "title": "\n5  Meta-analysis and publication bias\n",
    "section": "\n14.7 PB under an EE model",
    "text": "14.7 PB under an EE model\nThen, let’s assume that our publishing system is very strict (extreme). You can publish only if \\(p \\leq 0.05\\) on the expected direction. Then the true population of effect sizes will be truncated. Essentially we are assuming that \\(P(1|p \\leq 0.05) = 1\\) and \\(P(1|p \\leq 0.05) = 0\\).\n\nround(compare_rma(fit, fit_pb), 3)\n#&gt; fit: rma(yi = yi, vi = vi, data = dat_un, method = \"EE\")\n#&gt; fit_pb: rma(yi = yi, vi = vi, data = dat_pb, method = \"EE\")\n#&gt;                fit fit_pb\n#&gt; b (intrcpt)  0.289  0.436\n#&gt; se           0.009  0.008\n#&gt; zval        32.003 51.811\n#&gt; pval         0.000  0.000\n#&gt; ci.lb        0.271  0.420\n#&gt; ci.ub        0.307  0.453\n#&gt; I2           0.955  0.000\n#&gt; tau2         0.000  0.000",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Meta-analysis and publication bias</span>"
    ]
  },
  {
    "objectID": "chapters/chapter5.html#pb-under-an-ee-model-4",
    "href": "chapters/chapter5.html#pb-under-an-ee-model-4",
    "title": "\n5  Meta-analysis and publication bias\n",
    "section": "\n14.8 PB under an EE model",
    "text": "14.8 PB under an EE model\nThe situation is even worse when we simulate a null effect. This strict selection results in committing type-1 error:\n\nset.seed(2023)\nk &lt;- 1e3\nn &lt;- round(runif(k, 10, 100))\ndat0 &lt;- sim_studies(k = k, es = 0, tau2 = 0, n1 = n)\ndat0 &lt;- summary(dat0)\n# compute 1 tail pvalue\ndat0$pval1 &lt;- 1 - pnorm(dat0$zi)\n# selecting\nsign &lt;- dat0$pval1 &lt;= 0.05 & dat0$zi &gt; 0\ndat_pb0 &lt;- dat0[sign, ]\ndat_un0 &lt;- dat0[sample(1:nrow(dat0), sum(sign)), ]\n\n# fitting EE model for the full vs selected (ignore k differences)\nfit0 &lt;- rma(yi, vi, method = \"EE\", data = dat_un0)\nfit_pb0 &lt;- rma(yi, vi, method = \"EE\", data = dat_pb0)\nround(compare_rma(fit0, fit_pb0), 3)\n#&gt; fit0: rma(yi = yi, vi = vi, data = dat_un0, method = \"EE\")\n#&gt; fit_pb0: rma(yi = yi, vi = vi, data = dat_pb0, method = \"EE\")\n#&gt;               fit0 fit_pb0\n#&gt; b (intrcpt) -0.006   0.363\n#&gt; se           0.027   0.026\n#&gt; zval        -0.220  14.015\n#&gt; pval         0.826   0.000\n#&gt; ci.lb       -0.059   0.312\n#&gt; ci.ub        0.047   0.414\n#&gt; I2           0.000   0.000\n#&gt; tau2         0.000   0.000",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Meta-analysis and publication bias</span>"
    ]
  },
  {
    "objectID": "chapters/chapter5.html#pb-under-an-ee-model-5",
    "href": "chapters/chapter5.html#pb-under-an-ee-model-5",
    "title": "\n5  Meta-analysis and publication bias\n",
    "section": "\n14.9 PB under an EE model",
    "text": "14.9 PB under an EE model\nAssuming to pick a very precise (\\(n = 1000\\)) and a very unprecise (\\(n = 20\\)) study, which one is more likely to have an effect size close to the true value?\n. . .\nThe precise study has a lower \\(\\epsilon_i\\) thus is closer to \\(\\theta\\). This relationship create a very insightful visual representation.\n. . .\nWhat could be the shape of the plot when plotting the precision (e.g., the sample size or the inverse of the variance) as a function of the effect size?",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Meta-analysis and publication bias</span>"
    ]
  },
  {
    "objectID": "chapters/chapter5.html#pb-under-an-ee-model-6",
    "href": "chapters/chapter5.html#pb-under-an-ee-model-6",
    "title": "\n5  Meta-analysis and publication bias\n",
    "section": "\n14.10 PB under an EE model",
    "text": "14.10 PB under an EE model\n\n#|code-fold: true\nplot(dat$yi, sqrt(dat$vi), ylim=rev(range(dat$vi)), pch = 19, col = scales::alpha(\"black\", 0.5), cex = 1.5)\nabline(v = theta, lwd = 3, col = \"firebrick\")",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Meta-analysis and publication bias</span>"
    ]
  },
  {
    "objectID": "chapters/chapter5.html#publication-bias-pb---funnel-plot",
    "href": "chapters/chapter5.html#publication-bias-pb---funnel-plot",
    "title": "\n5  Meta-analysis and publication bias\n",
    "section": "\n14.11 Publication Bias (PB) - Funnel Plot",
    "text": "14.11 Publication Bias (PB) - Funnel Plot\nWe created a funnel plot. This is a visual tool to check the presence of asymmetry that could be caused by publication bias. If meta-analysis assumptions are respected, and there is no publication bias:\n\neffects should be normally distributed around the average effect\nmore precise studies should be closer to the average effect\nless precise studies could be equally distributed around the average effect",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Meta-analysis and publication bias</span>"
    ]
  },
  {
    "objectID": "chapters/chapter5.html#publication-bias-pb---funnel-plot-1",
    "href": "chapters/chapter5.html#publication-bias-pb---funnel-plot-1",
    "title": "\n5  Meta-analysis and publication bias\n",
    "section": "\n14.12 Publication Bias (PB) - Funnel Plot",
    "text": "14.12 Publication Bias (PB) - Funnel Plot\nThe plot assume the typical funnel shape and there are not missing spots on the at the bottom. The presence of missing spots is a potential index of publication bias.\n\nCodefit &lt;- rma(yi, vi, method = \"EE\", data = dat)\ndat$pb &lt;- dat$pval &lt;= 0.05\n\nwith(dat[dat$pb, ],\n     plot(yi, sei,\n          ylim = rev(range(dat$sei)),\n          xlab = latex2exp::TeX(\"$y_i$\"),\n          ylab = latex2exp::TeX(\"$\\\\sqrt{\\\\sigma_i^2}$\"),\n          xlim = range(dat$yi),\n          pch = 19,\n          col = scales::alpha(\"firebrick\", 0.5))\n)\n\nwith(dat[!dat$pb, ],\n     points(yi, sei, col = scales::alpha(\"black\", 0.5), pch = 19)\n)\n\nabline(v = fit$b[[1]], col = \"black\", lwd = 1.2)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Meta-analysis and publication bias</span>"
    ]
  },
  {
    "objectID": "chapters/chapter5.html#publication-bias-pb---funnel-plot-2",
    "href": "chapters/chapter5.html#publication-bias-pb---funnel-plot-2",
    "title": "\n5  Meta-analysis and publication bias\n",
    "section": "\n14.13 Publication Bias (PB) - Funnel Plot",
    "text": "14.13 Publication Bias (PB) - Funnel Plot\nThe plot assume the typical funnel shape and there are not missing spots on the at the bottom. The presence of missing spots is a potential index of publication bias.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Meta-analysis and publication bias</span>"
    ]
  },
  {
    "objectID": "chapters/chapter5.html#robustness-to-pb---fail-safe-n",
    "href": "chapters/chapter5.html#robustness-to-pb---fail-safe-n",
    "title": "\n5  Meta-analysis and publication bias\n",
    "section": "\n14.14 Robustness to PB - Fail-safe N",
    "text": "14.14 Robustness to PB - Fail-safe N\nThe Fail-safe N (Rosenthal, 1979) idea is very simple. Given a meta-analysis with a significant result (i.e., \\(p \\leq \\alpha\\)). How many null studies (i.e., \\(\\hat \\theta = 0\\)) do I need to obtain \\(p &gt; \\alpha\\)?\n\nmetafor::fsn(yi, vi, data = dat)\n#&gt; \n#&gt; Fail-safe N Calculation Using the Rosenthal Approach\n#&gt; \n#&gt; Observed Significance Level: &lt;.0001\n#&gt; Target Significance Level:   0.05\n#&gt; \n#&gt; Fail-safe N: 832741",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Meta-analysis and publication bias</span>"
    ]
  },
  {
    "objectID": "chapters/chapter5.html#robustness-to-pb---fail-safe-n-1",
    "href": "chapters/chapter5.html#robustness-to-pb---fail-safe-n-1",
    "title": "\n5  Meta-analysis and publication bias\n",
    "section": "\n14.15 Robustness to PB - Fail-safe N",
    "text": "14.15 Robustness to PB - Fail-safe N\nThere are several criticism to the Fail-safe N procedure:\n. . .\n\nis not actually detecting the PB but suggesting the required PB size to remove the effect. A very large N suggest that even with PB, it is unlikely that the results could be completely changed by actually reporting null studies\n\n. . .\n\n\nOrwin (1983) proposed a new method calculating the number of studies required to reduce the effect size to a given target\n\n. . .\n\n\nRosenberg (2005) proposed a method similar to Rosenthal (1979) but combining the (weighted) effect sizes and not the p-values.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Meta-analysis and publication bias</span>"
    ]
  },
  {
    "objectID": "chapters/chapter5.html#detecting-pb---egger-regression",
    "href": "chapters/chapter5.html#detecting-pb---egger-regression",
    "title": "\n5  Meta-analysis and publication bias\n",
    "section": "\n14.16 Detecting PB - Egger Regression",
    "text": "14.16 Detecting PB - Egger Regression\nA basic method to test the funnel plot asymmetry is using an the Egger regression test. Basically we calculate the relationship between \\(y_i\\) and \\(\\sqrt{\\sigma^2_i}\\). In the absence of asimmetry, the line slope should be not different from 0.\nWe can use the metafor::regtest() function:\n\negger &lt;- regtest(fit)\negger\n#&gt; \n#&gt; Regression Test for Funnel Plot Asymmetry\n#&gt; \n#&gt; Model:     fixed-effects meta-regression model\n#&gt; Predictor: standard error\n#&gt; \n#&gt; Test for Funnel Plot Asymmetry: z = -0.3621, p = 0.7173\n#&gt; Limit Estimate (as sei -&gt; 0):   b =  0.3061 (CI: 0.2601, 0.3520)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Meta-analysis and publication bias</span>"
    ]
  },
  {
    "objectID": "chapters/chapter5.html#publication-bias-pb---egger-regression",
    "href": "chapters/chapter5.html#publication-bias-pb---egger-regression",
    "title": "\n5  Meta-analysis and publication bias\n",
    "section": "\n14.17 Publication Bias (PB) - Egger Regression",
    "text": "14.17 Publication Bias (PB) - Egger Regression\n\n\n\n\n\n\n\n\nThis is a standard (meta) regression thus the number of studies, the precision of each study and heterogeneity influence the reliability (power, type-1 error rate, etc.) of the procedure.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Meta-analysis and publication bias</span>"
    ]
  },
  {
    "objectID": "chapters/chapter5.html#correcting-pb---trim-and-fill",
    "href": "chapters/chapter5.html#correcting-pb---trim-and-fill",
    "title": "\n5  Meta-analysis and publication bias\n",
    "section": "\n14.18 Correcting PB - Trim and Fill",
    "text": "14.18 Correcting PB - Trim and Fill\nThe Trim and Fill method (Duval & Tweedie, 2000) is used to impute the hypothetical missing studies according to the funnel plot and recomputing the meta-analysis effect. Shi and Lin (Shi & Lin, 2019) provide an updated overview of the method with some criticisms.\n. . .\n\nset.seed(2023)\nk &lt;- 100 # we increased k to better show the effect\ntheta &lt;- 0.5\ntau2 &lt;- 0.1\nn &lt;- runif(k, 10, 100)\ndat &lt;- sim_studies(k, theta, tau2, n)\ndat &lt;- summary(dat)\ndatpb &lt;- dat[dat$pval &lt;= 0.1 & dat$zi &gt; 0, ]\nfit &lt;- rma(yi, vi, data = datpb, method = \"REML\")",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Meta-analysis and publication bias</span>"
    ]
  },
  {
    "objectID": "chapters/chapter5.html#correcting-pb---trim-and-fill-1",
    "href": "chapters/chapter5.html#correcting-pb---trim-and-fill-1",
    "title": "\n5  Meta-analysis and publication bias\n",
    "section": "\n14.19 Correcting PB - Trim and Fill",
    "text": "14.19 Correcting PB - Trim and Fill\nNow we can use the metafor::trimfill() function:\n\ntaf &lt;- metafor::trimfill(fit)\ntaf\n#&gt; \n#&gt; Estimated number of missing studies on the left side: 15 (SE = 5.4670)\n#&gt; \n#&gt; Random-Effects Model (k = 84; tau^2 estimator: REML)\n#&gt; \n#&gt; tau^2 (estimated amount of total heterogeneity): 0.0562 (SE = 0.0146)\n#&gt; tau (square root of estimated tau^2 value):      0.2371\n#&gt; I^2 (total heterogeneity / total variability):   61.61%\n#&gt; H^2 (total variability / sampling variability):  2.60\n#&gt; \n#&gt; Test for Heterogeneity:\n#&gt; Q(df = 83) = 210.3501, p-val &lt; .0001\n#&gt; \n#&gt; Model Results:\n#&gt; \n#&gt; estimate      se     zval    pval   ci.lb   ci.ub      \n#&gt;   0.5934  0.0339  17.5084  &lt;.0001  0.5270  0.6598  *** \n#&gt; \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nThe trim-and-fill estimates that 15 are missing. The new effect size after including the studies is reduced and closer to the simulated value (but in this case still significant).",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Meta-analysis and publication bias</span>"
    ]
  },
  {
    "objectID": "chapters/chapter5.html#correcting-pb---trim-and-fill-2",
    "href": "chapters/chapter5.html#correcting-pb---trim-and-fill-2",
    "title": "\n5  Meta-analysis and publication bias\n",
    "section": "\n14.20 Correcting PB - Trim and Fill",
    "text": "14.20 Correcting PB - Trim and Fill\nWe can also visualize the funnel plot highligting the points that are included by the method.\n\nfunnel(taf)\n\n\nCodefunnel(taf)\negg &lt;- regtest(fit)\negg_npb &lt;- regtest(taf)\nse &lt;- seq(0,1.8,length=100)\nlines(coef(egg$fit)[1] + coef(egg$fit)[2]*se, se, lwd=3, col = \"black\")\nlines(coef(egg_npb$fit)[1] + coef(egg_npb$fit)[2]*se, se, lwd=3, col = \"firebrick\")\nlegend(\"topleft\", legend = c(\"Original\", \"Corrected\"), fill = c(\"black\", \"firebrick\"))",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Meta-analysis and publication bias</span>"
    ]
  },
  {
    "objectID": "chapters/chapter5.html#why-the-funnel-plot-can-be-misleading",
    "href": "chapters/chapter5.html#why-the-funnel-plot-can-be-misleading",
    "title": "\n5  Meta-analysis and publication bias\n",
    "section": "\n14.21 Why the funnel plot can be misleading?",
    "text": "14.21 Why the funnel plot can be misleading?\nThis funnel plot show an evident asymmetry on the left side. Is there evidence of publication bias? What do you think?\n\nset.seed(2024)\nk &lt;- 50\nn1 &lt;- round(runif(k, 10, 200))\nn2 &lt;- round(runif(k, 10, 50))\ndat1 &lt;- sim_studies(k, 0, 0, n1, add = list(x = 0))\ndat2 &lt;- sim_studies(k, 0.5, 0.05, n2, add = list(x = 1))\ndat &lt;- rbind(dat1, dat2)\nfit &lt;- rma(yi, vi, dat = dat)\nfunnel(fit)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Meta-analysis and publication bias</span>"
    ]
  },
  {
    "objectID": "chapters/chapter5.html#why-the-funnel-plot-can-be-misleading-1",
    "href": "chapters/chapter5.html#why-the-funnel-plot-can-be-misleading-1",
    "title": "\n5  Meta-analysis and publication bias\n",
    "section": "\n14.22 Why the funnel plot can be misleading?",
    "text": "14.22 Why the funnel plot can be misleading?\nThe data are of course simulated and this is the code. What do you think now?\n\nset.seed(2024)\nk &lt;- 50\nn1 &lt;- round(runif(k, 10, 200))\nn2 &lt;- round(runif(k, 10, 50))\ndat1 &lt;- sim_studies(k, 0, 0, n1, add = list(x = 0))\ndat2 &lt;- sim_studies(k, 0.5, 0.05, n2, add = list(x = 1))\ndat &lt;- rbind(dat1, dat2)\nfit &lt;- rma(yi, vi, dat = dat)\nfunnel(fit)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Meta-analysis and publication bias</span>"
    ]
  },
  {
    "objectID": "chapters/chapter5.html#why-the-funnel-plot-can-be-misleading-2",
    "href": "chapters/chapter5.html#why-the-funnel-plot-can-be-misleading-2",
    "title": "\n5  Meta-analysis and publication bias\n",
    "section": "\n14.23 Why the funnel plot can be misleading?",
    "text": "14.23 Why the funnel plot can be misleading?\nIn fact, these are two unbiased population of effect sizes. Extra source of heterogeneity could create asymmetry not related to PB.\n\npar(mfrow = c(1, 2))\nfunnel(fit)\nfunnel(fit, col = dat$x + 1)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Meta-analysis and publication bias</span>"
    ]
  },
  {
    "objectID": "chapters/chapter5.html#why-the-funnel-plot-can-be-misleading-3",
    "href": "chapters/chapter5.html#why-the-funnel-plot-can-be-misleading-3",
    "title": "\n5  Meta-analysis and publication bias\n",
    "section": "\n14.24 Why the funnel plot can be misleading?",
    "text": "14.24 Why the funnel plot can be misleading?\nAlso the methods to detect/correct for PB are committing a false alarm:\n\nregtest(fit)\n#&gt; \n#&gt; Regression Test for Funnel Plot Asymmetry\n#&gt; \n#&gt; Model:     mixed-effects meta-regression model\n#&gt; Predictor: standard error\n#&gt; \n#&gt; Test for Funnel Plot Asymmetry: z =  6.2569, p &lt; .0001\n#&gt; Limit Estimate (as sei -&gt; 0):   b = -0.2067 (CI: -0.3460, -0.0675)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Meta-analysis and publication bias</span>"
    ]
  },
  {
    "objectID": "chapters/chapter5.html#why-the-funnel-plot-can-be-misleading-4",
    "href": "chapters/chapter5.html#why-the-funnel-plot-can-be-misleading-4",
    "title": "\n5  Meta-analysis and publication bias\n",
    "section": "\n14.25 Why the funnel plot can be misleading?",
    "text": "14.25 Why the funnel plot can be misleading?\nAlso the methods to detect/correct for PB are committing a false alarm:\n\ntrimfill(fit)\n#&gt; \n#&gt; Estimated number of missing studies on the left side: 32 (SE = 6.3553)\n#&gt; \n#&gt; Random-Effects Model (k = 132; tau^2 estimator: REML)\n#&gt; \n#&gt; tau^2 (estimated amount of total heterogeneity): 0.2183 (SE = 0.0337)\n#&gt; tau (square root of estimated tau^2 value):      0.4672\n#&gt; I^2 (total heterogeneity / total variability):   86.77%\n#&gt; H^2 (total variability / sampling variability):  7.56\n#&gt; \n#&gt; Test for Heterogeneity:\n#&gt; Q(df = 131) = 639.7238, p-val &lt; .0001\n#&gt; \n#&gt; Model Results:\n#&gt; \n#&gt; estimate      se    zval    pval    ci.lb   ci.ub    \n#&gt;   0.0281  0.0457  0.6142  0.5391  -0.0615  0.1177    \n#&gt; \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Meta-analysis and publication bias</span>"
    ]
  },
  {
    "objectID": "chapters/chapter5.html#why-the-funnel-plot-can-be-misleading-5",
    "href": "chapters/chapter5.html#why-the-funnel-plot-can-be-misleading-5",
    "title": "\n5  Meta-analysis and publication bias\n",
    "section": "\n14.26 Why the funnel plot can be misleading?",
    "text": "14.26 Why the funnel plot can be misleading?\nThe regtest can be applied also with moderators. The idea should be to take into account the moderators effects and then check for asymmetry.\n\nfitm &lt;- rma(yi, vi, mods = ~x, data = dat)\nregtest(fitm)\n#&gt; \n#&gt; Regression Test for Funnel Plot Asymmetry\n#&gt; \n#&gt; Model:     mixed-effects meta-regression model\n#&gt; Predictor: standard error\n#&gt; \n#&gt; Test for Funnel Plot Asymmetry: z = -1.0277, p = 0.3041",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Meta-analysis and publication bias</span>"
    ]
  },
  {
    "objectID": "chapters/chapter5.html#why-the-funnel-plot-can-be-misleading-6",
    "href": "chapters/chapter5.html#why-the-funnel-plot-can-be-misleading-6",
    "title": "\n5  Meta-analysis and publication bias\n",
    "section": "\n14.27 Why the funnel plot can be misleading?",
    "text": "14.27 Why the funnel plot can be misleading?\nIn fact, the funnel plot on the raw dataset and on residuals looks quite different because the asymmetry was caused by the moderator.\n\nCodedat$ri &lt;- residuals(fitm)\nfitr &lt;- rma(ri, vi, data = dat)\npar(mfrow = c(1, 2))\nplot_regtest(fit, main = \"Full model\")\nplot_regtest(fitr, main = \"Residuals\")",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Meta-analysis and publication bias</span>"
    ]
  },
  {
    "objectID": "chapters/chapter5.html#correcting-pb---selection-models-sm",
    "href": "chapters/chapter5.html#correcting-pb---selection-models-sm",
    "title": "\n5  Meta-analysis and publication bias\n",
    "section": "\n14.28 Correcting PB - Selection Models (SM)",
    "text": "14.28 Correcting PB - Selection Models (SM)\nSM are more than a tool for correcting for the PB. SM are formal models of PB that can help us understanding and simulating the PB.\nThe SM are composed by two parts:\n\n\nEffect size model: the unbiased data generation process. In our case basically the sim_studies() function.\n\nSelection model: the assumed process generating the biased selection of effect sizes\n\nSelection models can be based on the p-value (e.g., p-curve or p-uniform) and/or the effect size and variance (Copas model). We will see only models based on the p-value.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Meta-analysis and publication bias</span>"
    ]
  },
  {
    "objectID": "chapters/chapter5.html#correcting-pb---selection-models-sm-1",
    "href": "chapters/chapter5.html#correcting-pb---selection-models-sm-1",
    "title": "\n5  Meta-analysis and publication bias\n",
    "section": "\n14.29 Correcting PB - Selection Models (SM)",
    "text": "14.29 Correcting PB - Selection Models (SM)\nFormally, the random-effect meta-analysis probability density function (PDF) can be written as (e.g., Citkowicz & Vevea, 2017):\n\\[\nf\\left(y_i \\mid \\beta, \\tau^2 ; \\sigma_i^2\\right)=\\frac{\\phi\\left(\\frac{y_i-\\Delta_i}{\\sqrt{\\sigma_i^2+\\tau^2}}\\right)}{\\int_{-\\infty}^{\\infty}  \\phi\\left(\\frac{Y_i-\\Delta_i}{\\sqrt{\\sigma_i^2+\\tau^2}}\\right) d y_i}\n\\]\nWithout going into details, this is the PDF without any selection process (i.e., the effect sizes model).",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Meta-analysis and publication bias</span>"
    ]
  },
  {
    "objectID": "chapters/chapter5.html#correcting-pb---selection-models-sm-2",
    "href": "chapters/chapter5.html#correcting-pb---selection-models-sm-2",
    "title": "\n5  Meta-analysis and publication bias\n",
    "section": "\n14.30 Correcting PB - Selection Models (SM)",
    "text": "14.30 Correcting PB - Selection Models (SM)\nIf we have a function linking the p-value with the probability of publishing (a weight function) \\(w(p_i)\\) we can include it in the previous PDF, creating a weighted PDF.\n\\[\nf\\left(y_i \\mid \\beta, \\tau^2 ; \\sigma_i^2\\right)=\\frac{\\mathrm{w}\\left(p_i\\right) \\phi\\left(\\frac{y_i-\\Delta_i}{\\sqrt{\\sigma_i^2+\\tau^2}}\\right)}{\\int_{-\\infty}^{\\infty} \\mathrm{w}\\left(p_i\\right) \\phi\\left(\\frac{Y_i-\\Delta_i}{\\sqrt{\\sigma_i^2+\\tau^2}}\\right)d y_i}\n\\]\nEssentially, this new model take into account the selection process (the weight function) to estimate a new meta-analysis. In case of no selection (all weigths are the same) the model is the standard random-effects meta-analysis.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Meta-analysis and publication bias</span>"
    ]
  },
  {
    "objectID": "chapters/chapter5.html#sm---weigth-function",
    "href": "chapters/chapter5.html#sm---weigth-function",
    "title": "\n5  Meta-analysis and publication bias\n",
    "section": "\n14.31 SM - Weigth function",
    "text": "14.31 SM - Weigth function\nThe weigth function is a simple function that links the p-value with the probability of publishing. The simple example at the beginning (publishing only significant p-values) is a step weigth function.\n\np &lt;- c(0, 0.05, 0.05, 1)\nw &lt;- c(1, 1, 0, 0)\n\nplot(p, w, type = \"l\", xlab = \"p value\", ylab = \"Probability of Publishing\")",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Meta-analysis and publication bias</span>"
    ]
  },
  {
    "objectID": "chapters/chapter5.html#sm---weigth-function-1",
    "href": "chapters/chapter5.html#sm---weigth-function-1",
    "title": "\n5  Meta-analysis and publication bias\n",
    "section": "\n14.32 SM - Weigth function",
    "text": "14.32 SM - Weigth function\nWe can add more steps to express a more complex selection process:\n\np &lt;- c(0.001, 0.01, 0.05, 0.1, 0.8, 1)\nw &lt;- c(1, 1, 0.9, 0.5, 0.1, 0.1)\nplot(p, w, type = \"s\", xlab = \"p value\", ylab = \"Probability of Publishing\")",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Meta-analysis and publication bias</span>"
    ]
  },
  {
    "objectID": "chapters/chapter5.html#sm---weigth-function-2",
    "href": "chapters/chapter5.html#sm---weigth-function-2",
    "title": "\n5  Meta-analysis and publication bias\n",
    "section": "\n14.33 SM - Weigth function",
    "text": "14.33 SM - Weigth function\nOr we can draw a smooth function assuming certain parameters:\n\nwbeta &lt;- function(p, a = 1, b = 1) p^(a - 1) * (1 - p)^(b - 1)\npval &lt;- seq(0, 1, 0.01)\n\nplot(pval, wbeta(pval, 1, 1), type = \"l\", ylim = c(0, 1), col = 1, lwd = 2,\n     xlab = \"p value\", ylab = \"Probability of Publishing\")\nlines(pval, wbeta(pval, 1, 2), col = 2, lwd = 2)\nlines(pval, wbeta(pval, 1, 5), col = 3, lwd = 2)\nlines(pval, wbeta(pval, 1, 50), col = 4, lwd = 2)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Meta-analysis and publication bias</span>"
    ]
  },
  {
    "objectID": "chapters/chapter5.html#sm---weigth-function-3",
    "href": "chapters/chapter5.html#sm---weigth-function-3",
    "title": "\n5  Meta-analysis and publication bias\n",
    "section": "\n14.34 SM - Weigth function",
    "text": "14.34 SM - Weigth function\nWhatever the function, the SM estimate the parameters of the function and the meta-analysis parameters taking into account the weigths.\nClearly, in the presence of no bias the two models (with and without weights) are the same while with PB the estimation is different, probably reducing the effect size.\nIf the SM is correct (not possible in reality), the SM estimate the true effect even in the presence of bias. This is the strenght and elegance of the SM.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Meta-analysis and publication bias</span>"
    ]
  },
  {
    "objectID": "chapters/chapter5.html#sm---weigth-functions",
    "href": "chapters/chapter5.html#sm---weigth-functions",
    "title": "\n5  Meta-analysis and publication bias\n",
    "section": "\n14.35 SM - Weigth functions",
    "text": "14.35 SM - Weigth functions\nThere are several weight functions:\n\nthe step model\nthe negative-exponential model\nthe beta model\n…\n\nFor an overview see the metafor documentation https://wviechtb.github.io/metafor/reference/selmodel.html",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Meta-analysis and publication bias</span>"
    ]
  },
  {
    "objectID": "chapters/chapter5.html#sm---step-model",
    "href": "chapters/chapter5.html#sm---step-model",
    "title": "\n5  Meta-analysis and publication bias\n",
    "section": "\n14.36 SM - Step model",
    "text": "14.36 SM - Step model\nThe step model approximate the selection process with thresholds \\(\\alpha\\) and the associated weight \\(w(p_i)\\). For example:\n\nsteps &lt;- c(0.005, 0.01, 0.05, 0.10, 0.25, 0.35, 0.50, 0.65, 0.75, 0.90, 0.95, 0.99, 0.995, 1)\nmoderate_pb &lt;- c(1, 0.99, 0.95, 0.80, 0.75, 0.65, 0.60, 0.55, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50)\nsevere_pb &lt;- c(1, 0.99, 0.90, 0.75, 0.60, 0.50, 0.40, 0.35, 0.30, 0.25, 0.10, 0.10, 0.10, 0.10)\n\npar(mfrow = c(1, 2))\nplot(steps, moderate_pb, type = \"s\", xlab = \"p value\", ylab = \"Probability of Selection\", main = \"Moderate PB\", ylim = c(0, 1))\nabline(v = steps, col = \"grey\")\nplot(steps, severe_pb, type = \"s\", xlab = \"p value\", ylab = \"Probability of Selection\", main = \"Severe PB\", ylim = c(0, 1))\nabline(v = steps, col = \"grey\")",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Meta-analysis and publication bias</span>"
    ]
  },
  {
    "objectID": "chapters/chapter5.html#sm---negative-exponential",
    "href": "chapters/chapter5.html#sm---negative-exponential",
    "title": "\n5  Meta-analysis and publication bias\n",
    "section": "\n14.37 SM - Negative-Exponential",
    "text": "14.37 SM - Negative-Exponential\nThe Negative-Exponential model is very simple and intuitive. The weight function is \\(e^{-\\delta p_i}\\) thus the single parameter \\(\\delta\\) is the amount of bias. When \\(\\delta = 0\\) there is no bias.\n\nwnegexp &lt;- function(p, delta){\n  exp((-delta)*p)\n}\n\n\ncurve(wnegexp(x, 0), ylim = c(0, 1), col = 1, lwd = 2, xlab = \"p value\", ylab = \"Probability of Selection\")\ncurve(wnegexp(x, 1), add = TRUE, col = 2, lwd = 2)\ncurve(wnegexp(x, 5), add = TRUE, col = 3, lwd = 2)\ncurve(wnegexp(x, 30), add = TRUE, col = 4, lwd = 2)\n\nlegend(\"topright\", legend = latex2exp::TeX(sprintf(\"$\\\\delta = %s$\", c(1, 2, 3, 4))), fill = 1:4)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Meta-analysis and publication bias</span>"
    ]
  },
  {
    "objectID": "chapters/chapter5.html#simulating-data-with-pb",
    "href": "chapters/chapter5.html#simulating-data-with-pb",
    "title": "\n5  Meta-analysis and publication bias\n",
    "section": "\n14.38 Simulating data with PB",
    "text": "14.38 Simulating data with PB\nThe strategy to simulate biased data is to sample from the sim_studies() function but to keep the studies using a probabilistic sampling based on the weight function.\n\nset.seed(2024)\n\nk &lt;- 500 # high number to check the results\nes &lt;- 0 # H0 true\ntau2 &lt;- 0.1\ndelta &lt;- 5\ndat &lt;- vector(mode = \"list\", k)\n\ni &lt;- 1\nwhile(i &lt;= k){\n  # generate data\n  n &lt;- runif(1, 10, 100)\n  d &lt;- summary(sim_studies(1, es, tau2, n))\n  # get one-tail p-value\n  pi &lt;- 1 - pnorm(d$zi)\n  # get wi\n  wpi &lt;- wnegexp(pi, delta)\n  keep &lt;- rbinom(1, 1, wpi) == 1\n  if(keep){\n    dat[[i]] &lt;- d\n    i &lt;- i + 1\n  }\n}\n\ndat &lt;- do.call(rbind, dat)\nfit &lt;- rma(yi, vi, data = dat)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Meta-analysis and publication bias</span>"
    ]
  },
  {
    "objectID": "chapters/chapter5.html#simulating-data-with-pb-1",
    "href": "chapters/chapter5.html#simulating-data-with-pb-1",
    "title": "\n5  Meta-analysis and publication bias\n",
    "section": "\n14.39 Simulating data with PB",
    "text": "14.39 Simulating data with PB\nLet’s see some plots:\n\nCodepar(mfrow = c(1, 3))\nhist(dat$yi, breaks = 50, col = \"dodgerblue\")\nhist(1 - pnorm(dat$zi), breaks = 50, col = \"dodgerblue\")\nplot(dat$yi, dat$sei, ylim = c(rev(range(dat$sei)[2]), 0), xlim = c(-1, 2))",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Meta-analysis and publication bias</span>"
    ]
  },
  {
    "objectID": "chapters/chapter5.html#simulating-data-with-pb-2",
    "href": "chapters/chapter5.html#simulating-data-with-pb-2",
    "title": "\n5  Meta-analysis and publication bias\n",
    "section": "\n14.40 Simulating data with PB",
    "text": "14.40 Simulating data with PB\nLet’s see the model result:\n\nfit &lt;- rma(yi, vi, data = dat)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Meta-analysis and publication bias</span>"
    ]
  },
  {
    "objectID": "chapters/chapter5.html#simulating-data-with-pb-3",
    "href": "chapters/chapter5.html#simulating-data-with-pb-3",
    "title": "\n5  Meta-analysis and publication bias\n",
    "section": "\n14.41 Simulating data with PB",
    "text": "14.41 Simulating data with PB\nLet’s see the Egger regression test and the trim-and-fill procedure:\n\nregtest(fit)\n#&gt; \n#&gt; Regression Test for Funnel Plot Asymmetry\n#&gt; \n#&gt; Model:     mixed-effects meta-regression model\n#&gt; Predictor: standard error\n#&gt; \n#&gt; Test for Funnel Plot Asymmetry: z = 4.4739, p &lt; .0001\n#&gt; Limit Estimate (as sei -&gt; 0):   b = 0.2017 (CI: 0.1235, 0.2799)\ntrimfill(fit)\n#&gt; \n#&gt; Estimated number of missing studies on the left side: 106 (SE = 14.6182)\n#&gt; \n#&gt; Random-Effects Model (k = 606; tau^2 estimator: REML)\n#&gt; \n#&gt; tau^2 (estimated amount of total heterogeneity): 0.0474 (SE = 0.0049)\n#&gt; tau (square root of estimated tau^2 value):      0.2176\n#&gt; I^2 (total heterogeneity / total variability):   56.94%\n#&gt; H^2 (total variability / sampling variability):  2.32\n#&gt; \n#&gt; Test for Heterogeneity:\n#&gt; Q(df = 605) = 1399.3991, p-val &lt; .0001\n#&gt; \n#&gt; Model Results:\n#&gt; \n#&gt; estimate      se     zval    pval   ci.lb   ci.ub      \n#&gt;   0.2927  0.0121  24.1497  &lt;.0001  0.2689  0.3164  *** \n#&gt; \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Meta-analysis and publication bias</span>"
    ]
  },
  {
    "objectID": "chapters/chapter5.html#simulating-data-with-pb-4",
    "href": "chapters/chapter5.html#simulating-data-with-pb-4",
    "title": "\n5  Meta-analysis and publication bias\n",
    "section": "\n14.42 Simulating data with PB",
    "text": "14.42 Simulating data with PB\nThe two methods are detecting the PB but not correcting it appropriately. Let’s see the SM using a negexp method:\n\nsel &lt;- selmodel(fit, type = \"negexp\", alternative = \"greater\")\nsel\n#&gt; \n#&gt; Random-Effects Model (k = 500; tau^2 estimator: ML)\n#&gt; \n#&gt; tau^2 (estimated amount of total heterogeneity): 0.0800 (SE = 0.0118)\n#&gt; tau (square root of estimated tau^2 value):      0.2828\n#&gt; \n#&gt; Test for Heterogeneity:\n#&gt; LRT(df = 1) = 115.9867, p-val &lt; .0001\n#&gt; \n#&gt; Model Results:\n#&gt; \n#&gt; estimate      se    zval    pval    ci.lb   ci.ub    \n#&gt;   0.0519  0.0381  1.3609  0.1735  -0.0228  0.1266    \n#&gt; \n#&gt; Test for Selection Model Parameters:\n#&gt; LRT(df = 1) = 46.6783, p-val &lt; .0001\n#&gt; \n#&gt; Selection Model Results:\n#&gt; \n#&gt; estimate      se     zval    pval   ci.lb   ci.ub      \n#&gt;   4.7526  0.3818  12.4494  &lt;.0001  4.0044  5.5008  *** \n#&gt; \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Meta-analysis and publication bias</span>"
    ]
  },
  {
    "objectID": "chapters/chapter5.html#simulating-data-with-pb-5",
    "href": "chapters/chapter5.html#simulating-data-with-pb-5",
    "title": "\n5  Meta-analysis and publication bias\n",
    "section": "\n14.43 Simulating data with PB",
    "text": "14.43 Simulating data with PB\nWe can also plot the results:\n\nplot(sel)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Meta-analysis and publication bias</span>"
    ]
  },
  {
    "objectID": "chapters/chapter5.html#pb-sensitivity-analysis",
    "href": "chapters/chapter5.html#pb-sensitivity-analysis",
    "title": "\n5  Meta-analysis and publication bias\n",
    "section": "\n14.44 PB Sensitivity analysis",
    "text": "14.44 PB Sensitivity analysis\n\nThe SM is correctly detecting, estimating and correcting the PB. But we simulated a pretty strong bias with \\(k = 500\\) studies. In reality meta-analyses have few studies.\n\nVevea & Woods (2005) proposed to fix the weight function parameters to certain values representing different degree of selection and check how the model changes.\nIf the model parameters are affected after taking into account the SM, this could be considered as an index of PB.\nThis approach is really interesting in general but especially when \\(k\\) is too small for estimating the SM\nsee ?selmodel for information about performing sensitivity analysis with pre-specified weight functions",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Meta-analysis and publication bias</span>"
    ]
  },
  {
    "objectID": "chapters/chapter5.html#more-on-sm-and-publication-bias",
    "href": "chapters/chapter5.html#more-on-sm-and-publication-bias",
    "title": "\n5  Meta-analysis and publication bias\n",
    "section": "\n14.45 More on SM and Publication Bias",
    "text": "14.45 More on SM and Publication Bias\n\nThe SM documentation of metafor::selmodel() https://wviechtb.github.io/metafor/reference/selmodel.html\n\nWolfgang Viechtbauer overview of PB https://www.youtube.com/watch?v=ucmOCuyCk-c\n\n\nHarrer et al. (2021) - Doing Meta-analysis in R - Chapter 9\n\n\nMcShane et al. (2016) for a nice introduction about publication bias and SM\nAnother good overview by Jin et al. (2015)\n\nSee also Guan & Vandekerckhove (2016), Maier et al. (2023) and Bartoš et al. (2022) for Bayesian approaches to PB",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Meta-analysis and publication bias</span>"
    ]
  },
  {
    "objectID": "chapters/chapter5.html#references-download-.bib-file",
    "href": "chapters/chapter5.html#references-download-.bib-file",
    "title": "\n5  Meta-analysis and publication bias\n",
    "section": "\n14.46 References  Download .bib file\n",
    "text": "14.46 References  Download .bib file\n\n\n\nAnderson, S. F., & Kelley, K. (2024). Sample size planning for\nreplication studies: The devil is in the design. Psychological\nMethods, 29, 844–867. https://doi.org/10.1037/met0000520\n\n\nBarba, L. A. (2018). Terminologies for reproducible research. arXiv\n[Cs.DL], arXiv:1802.03311. http://arxiv.org/abs/1802.03311\n\n\nBartoš, F., Maier, M., Quintana, D. S., & Wagenmakers, E.-J. (2022).\nAdjusting for publication bias in JASP and r: Selection models,\nPET-PEESE, and robust bayesian meta-analysis. Advances in Methods\nand Practices in Psychological Science, 5,\n251524592211092. https://doi.org/10.1177/25152459221109259\n\n\nBorenstein, M., Hedges, L. V., Higgins, J. P. T., & Rothstein, H. R.\n(2009). Introduction to Meta-Analysis. https://doi.org/10.1002/9780470743386\n\n\nBrandt, M. J., IJzerman, H., Dijksterhuis, A., Farach, F. J., Geller,\nJ., Giner-Sorolla, R., Grange, J. A., Perugini, M., Spies, J. R., &\nVeer, A. van ’t. (2014). The replication recipe: What makes for a\nconvincing replication? Journal of Experimental Social\nPsychology, 50, 217–224. https://doi.org/10.1016/j.jesp.2013.10.005\n\n\nBroman, K., Cetinkaya-Rundel, M., Nussbaum, A., Paciorek, C., Peng, R.,\nTurek, D., & Wickham, H. (2017, January 18). Recommendations to\nfunding agencies for supporting reproducible research. https://www.amstat.org/asa/files/pdfs/POL-ReproducibleResearchRecommendations.pdf\n\n\nBushman, B. J., & Wang, M. C. (2009). Vote-counting procedures in\nmeta-analysis. In The handbook of research synthesis and\nmeta-analysis (pp. 207–220). Russell Sage Foundation.\n\n\nCitkowicz, M., & Vevea, J. L. (2017). A parsimonious weight function\nfor modeling publication bias. Psychological Methods,\n22, 28–41. https://doi.org/10.1037/met0000119\n\n\nClark, T. S., & Linzer, D. A. (2015). Should i use fixed or random\neffects? Political Science Research and Methods, 3,\n399–408. https://doi.org/10.1017/psrm.2014.32\n\n\nCollaboration, O. S., & Open Science Collaboration. (2015).\nEstimating the reproducibility of psychological science. In\nScience (Vol. 349). https://doi.org/10.1126/science.aac4716\n\n\nCrandall, C. S., & Sherman, J. W. (2016). On the scientific\nsuperiority of conceptual replications for scientific progress.\nJournal of Experimental Social Psychology, 66, 93–99.\nhttps://doi.org/10.1016/j.jesp.2015.10.002\n\n\nCronbach, L. J., & Shapiro, K. (1982). Designing evaluations of\neducational and social programs. https://eduq.info/xmlui/handle/11515/9106\n\n\nDuval, S., & Tweedie, R. (2000). Trim and fill: A simple\nfunnel-plot-based method of testing and adjusting for publication bias\nin meta-analysis. Biometrics, 56, 455–463. https://doi.org/10.1111/j.0006-341x.2000.00455.x\n\n\nGelman, A., & Carlin, J. (2014). Beyond power calculations. In\nPerspectives on Psychological Science (Vol. 9, pp. 641–651). https://doi.org/10.1177/1745691614551642\n\n\nGelman, A., Hill, J., & Vehtari, A. (2020). Regression and other\nstories. Cambridge University Press. https://doi.org/10.1017/9781139161879\n\n\nGoeman, J. J., Solari, A., & Stijnen, T. (2010). Three-sided\nhypothesis testing: Simultaneous testing of superiority, equivalence and\ninferiority. Statistics in Medicine, 29, 2117–2125. https://doi.org/10.1002/sim.4002\n\n\nGollwitzer, M., & Schwabe, J. (2022). Context dependency as a\npredictor of replicability. Review of General Psychology: Journal of\nDivision 1, of the American Psychological Association, 26,\n241–249. https://doi.org/10.1177/10892680211015635\n\n\nGoodman, S. N., Fanelli, D., & Ioannidis, J. P. A. (2016). What does\nresearch reproducibility mean? Science Translational Medicine,\n8, 341ps12. https://doi.org/10.1126/scitranslmed.aaf5027\n\n\nGuan, M., & Vandekerckhove, J. (2016). A bayesian approach to\nmitigation of publication bias. Psychonomic Bulletin &\nReview, 23, 74–86. https://doi.org/10.3758/s13423-015-0868-6\n\n\nHarrer, M., Cuijpers, P., Furukawa, T., & Ebert, D. (2021).\nDoing meta-analysis with r: A hands-on guide (1st ed.). CRC\nPress.\n\n\nHarrington, D., D’Agostino, R. B., Sr, Gatsonis, C., Hogan, J. W.,\nHunter, D. J., Normand, S.-L. T., Drazen, J. M., & Hamel, M. B.\n(2019). New guidelines for statistical reporting in the journal. The\nNew England Journal of Medicine, 381, 285–286. https://doi.org/10.1056/NEJMe1906559\n\n\nHedges, L. V. (1989). An unbiased correction for sampling error in\nvalidity generalization studies. The Journal of Applied\nPsychology, 74, 469–477. https://doi.org/10.1037/0021-9010.74.3.469\n\n\nHedges, L. V., & Olkin, I. (1980). Vote-counting methods in research\nsynthesis. Psychological Bulletin, 88, 359–369. https://doi.org/10.1037/0033-2909.88.2.359\n\n\nHedges, L. V., & Schauer, J. M. (2019a). Consistency of effects is\nimportant in replication: Rejoinder to mathur and VanderWeele (2019).\nPsychological Methods, 24, 576–577. https://doi.org/10.1037/met0000237\n\n\nHedges, L. V., & Schauer, J. M. (2019b). More than one replication\nstudy is needed for unambiguous tests of replication. Journal of\nEducational and Behavioral Statistics: A Quarterly Publication Sponsored\nby the American Educational Research Association and the American\nStatistical Association, 44, 543–570. https://doi.org/10.3102/1076998619852953\n\n\nHedges, L. V., & Schauer, J. M. (2019c). Statistical analyses for\nstudying replication: Meta-analytic perspectives. Psychological\nMethods, 24, 557–570. https://doi.org/10.1037/met0000189\n\n\nHedges, L. V., & Schauer, J. M. (2021). The design of replication\nstudies. Journal of the Royal Statistical Society. Series A,\n(Statistics in Society), 184, 868–886. https://doi.org/10.1111/rssa.12688\n\n\nHeller, R., Bogomolov, M., & Benjamini, Y. (2014). Deciding whether\nfollow-up studies have replicated findings in a preliminary large-scale\nomics study. Proceedings of the National Academy of Sciences of the\nUnited States of America, 111, 16262–16267. https://doi.org/10.1073/pnas.1314814111\n\n\nHeyard, R., Pawel, S., Frese, J., Voelkl, B., Würbel, H., McCann, S.,\nHeld, L., Wever, K. E., Hartmann, H., Townsin, L., & Zellers, S.\n(2024). A scoping review on metrics to quantify reproducibility: A\nmultitude of questions leads to a multitude of metrics. https://scholar.google.com/citations?view_op=view_citation&hl=en&citation_for_view=JAb7P1QAAAAJ:ldfaerwXgEUC\n\n\nHiggins, J. P. T., & Thompson, S. G. (2002). Quantifying\nheterogeneity in a meta-analysis. Statistics in Medicine,\n21, 1539–1558. https://doi.org/10.1002/sim.1186\n\n\nInbar, Y. (2016). Association between contextual dependence and\nreplicability in psychology may be spurious. Proceedings of the\nNational Academy of Sciences of the United States of America,\n113, E4933–4. https://doi.org/10.1073/pnas.1608676113\n\n\nIntHout, J., Ioannidis, J. P. A., Rovers, M. M., & Goeman, J. J.\n(2016). Plea for routinely presenting prediction intervals in\nmeta-analysis. BMJ Open, 6, e010247. https://doi.org/10.1136/bmjopen-2015-010247\n\n\nIoannidis, J. P. A. (2005). Why most published research findings are\nfalse. PLoS Medicine, 2, e124. https://doi.org/10.1371/journal.pmed.0020124\n\n\nJager, L. R., & Leek, J. T. (2014). An estimate of the science-wise\nfalse discovery rate and application to the top medical literature.\nBiostatistics, 15, 1–12. https://academic.oup.com/biostatistics/article-abstract/15/1/1/244509\n\n\nJeffreys, H. (1973). Scientific inference. Cambridge University\nPress.\n\n\nJin, Z.-C., Zhou, X.-H., & He, J. (2015). Statistical methods for\ndealing with publication bias in meta-analysis. Statistics in\nMedicine, 34, 343–360. https://doi.org/10.1002/sim.6342\n\n\nKenett, R. S., & Shmueli, G. (2015). Clarifying the terminology that\ndescribes scientific reproducibility. Nature Methods,\n12, 699. https://doi.org/10.1038/nmeth.3489\n\n\nKruschke, J. K. (2015). Bayesian approaches to testing a point\n(“null”) hypothesis. In Doing bayesian data\nanalysis (pp. 335–358). Elsevier. https://doi.org/10.1016/b978-0-12-405888-0.00012-x\n\n\nKruschke, J. K., & Liddell, T. M. (2018). The bayesian new\nstatistics: Hypothesis testing, estimation, meta-analysis, and power\nanalysis from a bayesian perspective. Psychonomic Bulletin &\nReview, 25, 178–206. https://doi.org/10.3758/s13423-016-1221-4\n\n\nLakens, D., Scheel, A. M., & Isager, P. M. (2018). Equivalence\ntesting for psychological research: A tutorial. Advances in Methods\nand Practices in Psychological Science, 1, 259–269. https://doi.org/10.1177/2515245918770963\n\n\nLindley, D. V. (1972). Bayesian statistics a review. In Bayesian\nstatistics (pp. 1–74). https://doi.org/10.1137/1.9781611970654.ch1\n\n\nLópez-López, J. A., Marín-Martínez, F., Sánchez-Meca, J., Van den\nNoortgate, W., & Viechtbauer, W. (2014). Estimation of the\npredictive power of the model in mixed-effects meta-regression: A\nsimulation study. The British Journal of Mathematical and\nStatistical Psychology, 67, 30–48. https://doi.org/10.1111/bmsp.12002\n\n\nLy, A., Etz, A., Marsman, M., & Wagenmakers, E.-J. (2019).\nReplication bayes factors from evidence updating. Behavior Research\nMethods, 51, 2498–2508. https://doi.org/10.3758/s13428-018-1092-x\n\n\nMachery, E. (2020). What is a replication? Philosophy of\nScience, 87, 545–567. https://doi.org/10.1086/709701\n\n\nMaier, M., Bartoš, F., & Wagenmakers, E.-J. (2023). Robust bayesian\nmeta-analysis: Addressing publication bias with model-averaging.\nPsychological Methods, 28, 107–122. https://doi.org/10.1037/met0000405\n\n\nMathur, M. B., & VanderWeele, T. J. (2019). Challenges and\nsuggestions for defining replication \"success\" when effects may be\nheterogeneous: Comment on hedges and schauer (2019). Psychological\nMethods, 24, 571–575. https://doi.org/10.1037/met0000223\n\n\nMathur, M. B., & VanderWeele, T. J. (2020). New statistical metrics\nfor multisite replication projects. Journal of the Royal Statistical\nSociety. Series A, (Statistics in Society), 183,\n1145–1166. https://doi.org/10.1111/rssa.12572\n\n\nMcShane, B. B., Böckenholt, U., & Hansen, K. T. (2016). Adjusting\nfor publication bias in meta-analysis: An evaluation of selection\nmethods and some cautionary notes: An evaluation of selection methods\nand some cautionary notes. Perspectives on Psychological Science: A\nJournal of the Association for Psychological Science, 11,\n730–749. https://doi.org/10.1177/1745691616662243\n\n\nMiller, J. (2009). What is the probability of replicating a\nstatistically significant effect? Psychonomic Bulletin &\nReview, 16, 617–640. https://doi.org/10.3758/PBR.16.4.617\n\n\nNational Academies of Sciences, Engineering, and Medicine, Policy and\nGlobal Affairs, Committee on Science, Engineering, Medicine, and Public\nPolicy, Board on Research Data and Information, Division on Engineering\nand Physical Sciences, Committee on Applied and Theoretical Statistics,\nBoard on Mathematical Sciences and Analytics, Division on Earth and Life\nStudies, Nuclear and Radiation Studies Board, & Division of\nBehavioral and Social Sciences and Education. (2019).\nReproducibility and replicability in science. National\nAcademies Press. https://doi.org/10.17226/25303\n\n\nNosek, B. A., & Errington, T. M. (2017). Making sense of\nreplications. eLife, 6, e23383. https://doi.org/10.7554/eLife.23383\n\n\nNosek, B. A., & Errington, T. M. (2020). What is replication?\nPLoS Biology, 18, e3000691. https://doi.org/10.1371/journal.pbio.3000691\n\n\nOrwin, R. G. (1983). A fail-SafeN for effect size in meta-analysis.\nJournal of Educational Statistics, 8, 157–159. https://doi.org/10.3102/10769986008002157\n\n\nParmigiani, G. (2023). Defining replicability of prediction rules.\nStatistical Science: A Review Journal of the Institute of\nMathematical Statistics, 38. https://doi.org/10.1214/23-sts891\n\n\nPatil, P., Peng, R. D., & Leek, J. T. (2016). What should\nresearchers expect when they replicate studies? A statistical view of\nreplicability in psychological science. Perspectives on\nPsychological Science: A Journal of the Association for Psychological\nScience, 11, 539–544. https://doi.org/10.1177/1745691616646366\n\n\nPawel, S., & Held, L. (2020). Probabilistic forecasting of\nreplication studies. PloS One, 15, e0231416. https://doi.org/10.1371/journal.pone.0231416\n\n\nPerugini, M., Gallucci, M., & Costantini, G. (2014). Safeguard power\nas a protection against imprecise power estimates. Perspectives on\nPsychological Science: A Journal of the Association for Psychological\nScience, 9, 319–332. https://doi.org/10.1177/1745691614528519\n\n\nRiley, R. D., Higgins, J. P. T., & Deeks, J. J. (2011).\nInterpretation of random effects meta-analyses. BMJ,\n342, d549. https://doi.org/10.1136/bmj.d549\n\n\nRosenberg, M. S. (2005). The file-drawer problem revisited: A general\nweighted method for calculating fail-safe numbers in meta-analysis.\nEvolution; International Journal of Organic Evolution,\n59, 464–468. https://doi.org/10.1111/j.0014-3820.2005.tb01004.x\n\n\nRosenthal, R. (1979). The file drawer problem and tolerance for null\nresults. Psychological Bulletin, 86, 638–641. https://doi.org/10.1037/0033-2909.86.3.638\n\n\nRosenthal, R. (1990). Replication in behavioral research. Journal of\nSocial Behavior and Personality. https://search.proquest.com/openview/5ccc3a267139968c18c0d7ce1a1c09f6/1?pq-origsite=gscholar&cbl=1819046\n\n\nRouder, J. N., Speckman, P. L., Sun, D., Morey, R. D., & Iverson, G.\n(2009). Bayesian t tests for accepting and rejecting the null\nhypothesis. Psychonomic Bulletin & Review, 16,\n225–237. https://doi.org/10.3758/PBR.16.2.225\n\n\nSchad, D. J., Vasishth, S., Hohenstein, S., & Kliegl, R. (2020). How\nto capitalize on a priori contrasts in linear (mixed) models: A\ntutorial. Journal of Memory and Language, 110, 104038.\nhttps://doi.org/10.1016/j.jml.2019.104038\n\n\nSchauer, J. M. (2022). Replicability and meta-analysis. In W. O’Donohue,\nA. Masuda, & S. Lilienfeld (Eds.), Avoiding questionable\nresearch practices in applied psychology (pp. 301–342). Springer\nInternational Publishing. https://doi.org/10.1007/978-3-031-04968-2_14\n\n\nSchauer, J. M., & Hedges, L. V. (2020). Assessing heterogeneity and\npower in replications of psychological experiments. Psychological\nBulletin, 146, 701–719. https://doi.org/10.1037/bul0000232\n\n\nSchauer, J. M., & Hedges, L. V. (2021). Reconsidering statistical\nmethods for assessing replication. Psychological Methods,\n26, 127–139. https://doi.org/10.1037/met0000302\n\n\nSchmidt, F. L., & Hunter, J. E. (2014). Methods of\nmeta-analysis: Correcting error and bias in research findings (3rd\ned.). SAGE Publications. https://doi.org/10.4135/9781483398105\n\n\nSchmidt, S. (2009). Shall we really do it again? The powerful concept of\nreplication is neglected in the social sciences. Review of General\nPsychology: Journal of Division 1, of the American Psychological\nAssociation, 13, 90–100. https://doi.org/10.1037/a0015108\n\n\nShi, L., & Lin, L. (2019). The trim-and-fill method for publication\nbias: Practical guidelines and recommendations based on a large database\nof meta-analyses: Practical guidelines and recommendations based on a\nlarge database of meta-analyses. Medicine, 98, e15987.\nhttps://doi.org/10.1097/MD.0000000000015987\n\n\nSimons, D. J. (2014). The value of direct replication. Perspectives\non Psychological Science: A Journal of the Association for Psychological\nScience, 9, 76–80. https://doi.org/10.1177/1745691613514755\n\n\nSimonsohn, U. (2015). Small telescopes: Detectability and the evaluation\nof replication results: Detectability and the evaluation of replication\nresults. Psychological Science, 26, 559–569. https://doi.org/10.1177/0956797614567341\n\n\nSpence, J. R., & Stanley, D. J. (2016). Prediction interval: What to\nexpect when you’re expecting … a replication. PloS One,\n11, e0162874. https://doi.org/10.1371/journal.pone.0162874\n\n\nStan Development Team. (2021). Stan: A c++ library for probability\nand sampling, version 2.26.0. https://mc-stan.org/\n\n\nSteegen, S., Tuerlinckx, F., Gelman, A., & Vanpaemel, W. (2016).\nIncreasing transparency through a multiverse analysis. Perspectives\non Psychological Science: A Journal of the Association for Psychological\nScience, 11, 702–712. https://doi.org/10.1177/1745691616658637\n\n\nSteiger, J. H. (2004). Beyond the f test: Effect size confidence\nintervals and tests of close fit in the analysis of variance and\ncontrast analysis. Psychological Methods, 9, 164–182.\nhttps://doi.org/10.1037/1082-989X.9.2.164\n\n\nTal, E. (2020). Measurement in science. Metaphysics Research\nLab, Stanford University. https://plato.stanford.edu/archives/fall2020/entries/measurement-science/\n\n\nUlrich, R., & Miller, J. (2020). Questionable research practices may\nhave little effect on replicability. eLife, 9, e58237.\nhttps://doi.org/10.7554/eLife.58237\n\n\nValentine, J. C., Biglan, A., Boruch, R. F., Castro, F. G., Collins, L.\nM., Flay, B. R., Kellam, S., Mościcki, E. K., & Schinke, S. P.\n(2011). Replication in prevention science. Prevention Science: The\nOfficial Journal of the Society for Prevention Research,\n12, 103–117. https://doi.org/10.1007/s11121-011-0217-6\n\n\nVan Bavel, J. J., Mende-Siedlecki, P., Brady, W. J., & Reinero, D.\nA. (2016). Contextual sensitivity in scientific reproducibility.\nProceedings of the National Academy of Sciences of the United States\nof America, 113, 6454–6459. https://doi.org/10.1073/pnas.1521897113\n\n\nVerhagen, J., & Wagenmakers, E.-J. (2014). Bayesian tests to\nquantify the result of a replication attempt. Journal of\nExperimental Psychology. General, 143, 1457–1475. https://doi.org/10.1037/a0036731\n\n\nVeroniki, A. A., Jackson, D., Viechtbauer, W., Bender, R., Bowden, J.,\nKnapp, G., Kuss, O., Higgins, J. P. T., Langan, D., & Salanti, G.\n(2016). Methods to estimate the between-study variance and its\nuncertainty in meta-analysis. Research Synthesis Methods,\n7, 55–79. https://doi.org/10.1002/jrsm.1164\n\n\nVevea, J. L., & Woods, C. M. (2005). Publication bias in research\nsynthesis: Sensitivity analysis using a priori weight functions.\nPsychological Methods, 10, 428–443. https://doi.org/10.1037/1082-989X.10.4.428\n\n\nViechtbauer, W. (2005). Bias and efficiency of meta-analytic variance\nestimators in the random-effects model. Journal of Educational and\nBehavioral Statistics: A Quarterly Publication Sponsored by the American\nEducational Research Association and the American Statistical\nAssociation, 30, 261–293. https://doi.org/10.3102/10769986030003261\n\n\nWagenmakers, E.-J., Lodewyckx, T., Kuriyal, H., & Grasman, R.\n(2010). Bayesian hypothesis testing for psychologists: A tutorial on the\nsavage–dickey method. Cognitive Psychology, 60,\n158–189. https://doi.org/10.1016/j.cogpsych.2009.12.001\n\n\nWasserstein, R. L., & Lazar, N. A. (2016). The ASA statement on\np-values: Context, process, and purpose. The American\nStatistician, 70, 129–133. https://doi.org/10.1080/00031305.2016.1154108\n\n\nWilliams, D. R., Rast, P., & Bürkner, P.-. C. (2018). Bayesian\nmeta-analysis with weakly informative prior distributions.\nPsyArXiv. https://doi.org/10.31234/osf.io/7tbrm\n\n\nWilson, B. M., & Wixted, J. T. (2018). The prior odds of testing a\ntrue effect in cognitive and social psychology. Advances in Methods\nand Practices in Psychological Science, 1, 186–197. https://doi.org/10.1177/2515245918767122\n\n\nYarkoni, T. (2020). The generalizability crisis. The Behavioral and\nBrain Sciences, 45, e1. https://doi.org/10.1017/S0140525X20001685",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Meta-analysis and publication bias</span>"
    ]
  },
  {
    "objectID": "chapters/chapter5.html#footnotes",
    "href": "chapters/chapter5.html#footnotes",
    "title": "\n5  Meta-analysis and publication bias\n",
    "section": "",
    "text": "There is a confusion about the fixed-effects vs fixed-effect (no s) and equal-effects models. See https://wviechtb.github.io/metafor/reference/misc-models.html↩︎\nThe filor::compare_rma() function is similar to the car::compareCoefs() function↩︎\nSee Harrer et al. (2021) (Chapter 5) and Hedges & Schauer (2019) for an overview about the Q statistics↩︎\nsee https://www.meta-analysis-workshops.com/download/common-mistakes1.pdf↩︎\nNotice that the equation, in particular the usage of \\(t\\) vs \\(z\\) depends on assuming \\(s_x\\) to be known or estimated. See https://online.stat.psu.edu/stat501/lesson/3/3.3, https://en.wikipedia.org/wiki/Prediction_interval and https://www.bryanshalloway.com/2021/03/18/intuition-on-uncertainty-of-predictions-introduction-to-prediction-intervals/↩︎\nWhen a \\(t\\) distribution is assumed, the quantiles are calculated using \\(k - 2\\) degrees of freedom↩︎\nThe functions is made for numerical variables thus is less appropriate for categorical variables↩︎\nsee https://www.metafor-project.org/doku.php/tips:models_with_or_without_intercept on removing the intercept↩︎\nThanks to the Wolfgang Viechtbauer’s course https://www.wvbauer.com/doku.php/course_ma↩︎",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Meta-analysis and publication bias</span>"
    ]
  },
  {
    "objectID": "chapters/chapter4.html#replication-studies-as-meta-analysis",
    "href": "chapters/chapter4.html#replication-studies-as-meta-analysis",
    "title": "\n4  Statistical methods for replication assessment\n",
    "section": "\n4.3 Replication studies as meta-analysis",
    "text": "4.3 Replication studies as meta-analysis\nAs introduced by Hedges & Schauer (2019) replication studies can be seen from a statistical point of view as meta-analyses. There is a single initial study and one or more attempts to replicate this initial finding. For the simple one-to-one replication design we have a meta-analysis with two studies while in the one-to-many design we have a meta-analysis with \\(k\\) studies. When comes to evaluate the results of the replication studies we can choose between several methods (see Heyard et al., 2024) and some of them are specifically meta-analysis based trying to pool togheter evidence from original and replication studies. We consider the meta-analytic thinking crucial to understand the replication studies and methods but not all methods are strictly meta-analyes in the usual sense.\nIn fact, while the methodology of the initial or replication studies is important when comes to evaluate the single result, at the aggregated level we evaluate how a certain focal parameter (e.g., the difference between two groups or conditions) vary across replications and id there is evidence, whatever the criteria, for a successful replication. Whatever is the model used in the original studies we can essentially think at the aggregated level without loss of generality loss.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Statistical methods for replication assessment</span>"
    ]
  },
  {
    "objectID": "chapters/chapter4.html#confidenceprediction-interval-methods",
    "href": "chapters/chapter4.html#confidenceprediction-interval-methods",
    "title": "\n4  Statistical methods for replication assessment\n",
    "section": "\n4.6 Confidence/prediction interval methods",
    "text": "4.6 Confidence/prediction interval methods\nThe main improvement of confidence/prediction interval methods is taking into account the size of the effect and the precision. The confidence interval represent the sampling uncertainty around the estimated value. It is interpreted as the percentage of confidence intervals under repetition of the same sampling procedure that contains the true value. \nConfidence interval around an estimated effect \\(\\theta_r\\) can be written as:\n\\[\n95\\%\\;\\mbox{CI} = \\hat\\theta_r \\pm \\mbox{SE}_{r} t_{\\alpha}\n\\]\nWhere \\(t_{\\alpha}\\) is the critical value for the test statistics with a given \\(\\alpha\\). Clearly, this version of the confidence interval is symmetric around the estimated value and the width is a function of the estimation precision.\n\n\n\n\n\n\n\n\n\n4.6.1 Replication effect within the original CI\n\\[\n\\theta_{orig} - \\Phi(\\alpha/2) \\sqrt{\\sigma^2_{orig}} &lt; \\theta_{rep} &lt; \\theta_{orig} + \\Phi(\\alpha/2) \\sqrt{\\sigma^2_{orig}}\n\\]\n::{.pros} - Take into account the size of the effect and the precision of \\(\\theta_{orig}\\) ::: :::{.cons} - The original study is assumed to be a reliable estimation - No extension for many-to-one designs - Low precise original studies lead to higher success rate :::\n\n\n\n\n\n\n\n\nOne potential problem of this method regards that low precise original studies are “easier” to replicate due to larger confidence intervals.\n\n\n\n\n\n\n\n\n\n4.6.2 Original study within replication CI\nThe same approach can be applied checking if the original effect size is contained within the replication confidence interval. Clearly these methods depends on the precision of studies. Formally:\n\\[\n\\theta_{rep} - \\Phi(\\alpha/2) \\sqrt{\\sigma_{rep}^2} &lt; \\theta_{orig} &lt; \\theta_{rep} + \\Phi(\\alpha/2) \\sqrt{\\sigma_{rep}^2}\n\\]\nThe method has the same pros and cons of the previous approach. One advantage is that usually replication studies are more precise (higher sample size) thus the parameter and the % CI is more reliable.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Statistical methods for replication assessment</span>"
    ]
  },
  {
    "objectID": "chapters/chapter4.html#meta-analytic-methods",
    "href": "chapters/chapter4.html#meta-analytic-methods",
    "title": "\n4  Statistical methods for replication assessment\n",
    "section": "\n4.8 Meta-analytic methods",
    "text": "4.8 Meta-analytic methods",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Statistical methods for replication assessment</span>"
    ]
  },
  {
    "objectID": "chapters/chapter4.html#prediction-interval-pi",
    "href": "chapters/chapter4.html#prediction-interval-pi",
    "title": "\n4  Statistical methods for replication assessment\n",
    "section": "\n4.7 Prediction interval PI",
    "text": "4.7 Prediction interval PI\nThere is still one missing information from the CI method that is considering the uncertainty only from the original or the replication study. The prediction interval PI express the uncertainty of a future observation (in this case a study) and not on the estimated parameter.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Statistical methods for replication assessment</span>"
    ]
  }
]