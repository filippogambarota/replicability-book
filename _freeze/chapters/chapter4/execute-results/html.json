{
  "hash": "42fd4b67b82d06fb6bb4bf9c33621982",
  "result": {
    "engine": "knitr",
    "markdown": "# Statistical methods for replication assessment {#sec-statmethodrep}\n\n\n\n\n\n\n\n\n\n\n\n## How to assess the replication outcome?\n\nIn the previous chapters we introduced the idea of replication from a statistical and theoretical point of view. We purposely omitted defining the outcome of a replication study to dedicate an entire chapter about this topic. The problem of having multiple definitions of replication extends to the various ways of assessing the outcomes of replication studies.\n\nIn addition, the field of replication metrics keeps growing. @Heyard2024-hv published an extensive systematic review, supported by an online and keep up-to-date database with more than 50 replication measures organized and classified according to a common scheme.\n\nThe database is available at the following link [http://rachelheyard.com/reproducibility_metrics/](http://rachelheyard.com/reproducibility_metrics/). We decided to make a selection of methods from the database according to the following criteria:\n\n- for similar methods we keep the most recent and general version\n- we included both bayesian and frequentists\n- we included methods for answering to the question *\"Is the study $x$ replicated?\"* but also methods to combine evidence and estimate the size of the effect without an inferential decision\n- we selected methods that are not linked to a specific research area or topic. Some methods are specific for a given area of research or type of experiment such as replicating fMRI studies at the voxel level.\n\nIn general, despite this heterogeneity in methods and definitions choosing a single measure is probably not the best solution. In fact, large-scale replication projects [e.g., @Collaboration2015-ep; @Errington2021-fb; @The-Brazilian-Reproducibility-Initiative2025-iw] always evaluates the results across different metrics.\n\n## Replication studies as meta-analysis\n\nAs introduced by @Hedges2019-ry replication studies can be seen from a statistical point of view as meta-analyses. There is a single initial study and one or more attempts to replicate this initial finding. For the simple one-to-one replication design we have a meta-analysis with two studies while in the one-to-many design we have a meta-analysis with $R$ studies. When it comes to evaluating the results of the replication studies we can choose between several methods [see @Heyard2024-hv] and some of them are specifically meta-analysis based trying to pool together evidence from original and replication studies. We consider meta-analytic thinking crucial to understand the replication studies and methods but not all methods are strictly meta-analyses in the usual sense.\n\nIn fact, while the methodology of the initial or replication studies is important when comes to evaluate the single result, at the aggregated level we evaluate how a certain focal parameter (e.g., the difference between two groups or conditions) vary across replications and id there is evidence, whatever the criteria, for a successful replication. Whatever is the model used in the original studies we can essentially think at the aggregated level without loss of generality loss.\n\n## Simulating data\n\nWhile real-world examples are important, to understand the replication methods from a statistical point of view, simulating simplified examples is a good strategy. Furthermore, simulating data is nowadays considered an important tool to teach and understand statistical methods [@Hardin2015-ss]. Monte Carlo simulations are also necessary to estimate statistical properties (e.g., statistical power or type-1 error rate) of complex models.\n\nFor the simulated examples we can define the following simulation approach:\n\n- primary studies always compare two independent groups on a certain response variable\n- within a study, the two groups are assumed to come from two normal distributions with unit variance. For one group $y_0$ the mean is centered on zero and for the other group $y_1$ the mean is centered on the value representing the effect size $\\theta_r$ for that specific study.\n- the sample size can vary between the two groups\n\nFor example we can simulate a single study:\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntheta <- 0.3\nn0 <- 30\nn1 <- 30\n\ny0 <- rnorm(n0, 0, 1)\ny1 <- rnorm(n1, theta, 1)\n\nmean(y1) - mean(y0) # this is the effect size\n## [1] 0.6006209\nvar(y1)/n1 + var(y0)/n0 # this is the sampling variability\n## [1] 0.07598655\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndd <- data.frame(y = c(y0, y1),\n                 x = factor(rep(0:1, c(n0, n1))))\n\nggplot(dd, aes(x = x, y = y)) +\n    geom_boxplot(aes(fill = x))\n```\n\n::: {.cell-output-display}\n![](chapter4_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\n\n\n\n\nNow we can simply iterate the process for $R$ studies to create a series of replications with a common true parameter $\\mu_{\\theta}$. We can also generate a more realistic set of sample sizes sampling from a probability distribution (e.g., Poisson). Then, if we want to include variability in the true effects as in the extension framework we can simply sample $R$ effects from a normal distribution with variance $\\tau^2$.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nR <- 10\nmu <- 0.3 # average true effect\ntau2 <- 0.1 # heterogeneity\ndeltai <- rnorm(R, mu, sqrt(tau2)) # true effects for R studies\n\nyi <- vi <- rep(NA, R)\nn0 <- n1 <- 10 + rpois(R, 40 - 10)\n\nfor(i in 1:R){\n    y0 <- rnorm(n0[i], 0, 1)\n    y1 <- rnorm(n1[i], deltai[i], 1)\n    yi[i] <- mean(y1) - mean(y0)\n    vi[i] <- var(y1)/n1[i] + var(y0)/n0[i]\n}\n\nsim <- data.frame(id = 1:R, yi, vi, n0, n1)\nsim\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   id          yi         vi n0 n1\n1   1  0.48512788 0.04274283 50 50\n2   2  0.27339358 0.03453032 39 39\n3   3  0.82169855 0.06445129 29 29\n4   4 -0.03499302 0.03082677 51 51\n5   5  0.81318359 0.04637697 42 42\n6   6  0.73877894 0.04780514 44 44\n7   7  0.58633072 0.06484594 39 39\n8   8  0.25708146 0.05066051 37 37\n9   9  0.36428355 0.05013147 39 39\n10 10  0.29304233 0.03588685 43 43\n```\n\n\n:::\n:::\n\n\n\n\n\nThen we can put everything in a function that can be used to simulate different scenarios.\n\n\n\n\n\n```r\nsim_study <- function(theta, n0, n1 = NULL, id = NULL){\n    if(is.null(n1)) n1 <- n0\n    y0 <- rnorm(n0, 0, 1)\n    y1 <- rnorm(n1, theta, 1)\n    sim <- data.frame(\n        yi = mean(y1) - mean(y0),\n        vi = var(y1)/n1 + var(y0)/n0\n    )\n    sim$sei <- sqrt(sim$vi)\n    sim$n0 <- n0\n    sim$n1 <- n1\n    if(!is.null(id)){\n        sim <- cbind(id = id, sim)\n    }\n    class(sim) <- c(\"rep3data\", class(sim))\n    return(sim)\n}\nsim_studies <- function(R, mu = 0, tau2 = 0, n0, n1 = NULL){\n    # check input parameters consistency\n    if(length(mu) == 1) mu <- rep(mu, R)\n    if(length(n0) == 1) n0 <- rep(n0, R)\n    if(length(n1) == 1) n1 <- rep(n1, R)\n    if(is.null(n1)) n1 <- n0\n\n    deltai <- rnorm(R, 0, sqrt(tau2))\n    mu <- mu + deltai\n\n    sim <- mapply(sim_study, mu, n0, n1, SIMPLIFY = FALSE)\n    sim <- do.call(rbind, sim)\n    sim <- cbind(id = 1:R, sim)\n    class(sim) <- c(\"rep3data\", class(sim))\n    return(sim)\n}\n```\n\n::: {.cell}\n\n```{.r .cell-code}\nsim_studies(R = 10, mu = 0.5, tau2 = 0, n0 = 30)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   id         yi         vi       sei n0 n1\n1   1 0.80626169 0.06757911 0.2599598 30 30\n2   2 0.07120694 0.07427562 0.2725355 30 30\n3   3 0.35172580 0.08255055 0.2873161 30 30\n4   4 0.57126023 0.07388580 0.2718194 30 30\n5   5 0.45379331 0.07732052 0.2780657 30 30\n6   6 0.56832139 0.05782110 0.2404602 30 30\n7   7 0.57729675 0.07727588 0.2779854 30 30\n8   8 0.32115946 0.05785645 0.2405337 30 30\n9   9 0.02710467 0.06642620 0.2577328 30 30\n10 10 0.61524812 0.07488992 0.2736602 30 30\n```\n\n\n:::\n:::\n\n\n\n\n\n## Methods\n\nThe database is available at the following link [http://rachelheyard.com/reproducibility_metrics/](http://rachelheyard.com/reproducibility_metrics/) can be consulted for a more complete overview. Among the all available methods we selected the following metrics^[The name of the method corresponds to the first column of the online table] that will be discussed more in detail with examples.\n\n- Prediction interval: replication effect in original 95% prediction interval\n- Proportion of population effects agreeing in direction with the original (vote counting)\n- Confidence interval (CI): original effect in replication 95% CI\n- Confidence interval (CI): replication effect in original 95% CI\n- Prediction interval (CI): replication effect in original 95% PI\n- Replication Bayes Factor (BF)\n- Small Telescopes\n- Consistency of original with replications, $P_{\\mbox{orig}}$\n- Proportion of population effects agreeing in direction with the original, $\\hat{P}_{>0}$\n- Meta-analysis and $Q$ statistics\n\n## Vote Counting based on significance or direction\n\nThe simplest method is called **vote counting** [@Valentine2011-yq; @Hedges1980-gd]. A replication attempt $\\theta_{r}$ is considered successful if the result has the same direction of the original study $\\theta_{0}$ and is statistically significant i.e., $p_{\\theta_{r}} \\leq \\alpha$. Similarly we can count the number of replication with the *same sign* as the original study.\n\n:::{.pros}\n- Easy to understand, communicate and compute\n:::\n\n:::{.cons}\n- Did not consider the size of the effect\n- Depends on the power of $\\theta_{rep}$\n:::\n\nLet's simulate a replication:\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## original study\nn_orig <- 30\ntheta_orig <- theta_from_z(2, n_orig)\n\norig <- data.frame(\n  yi = theta_orig,\n  vi = 4/(n_orig*2)\n)\n\norig$sei <- sqrt(orig$vi)\norig <- summary_es(orig)\n\norig\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n         yi         vi       sei zi       pval      ci.lb    ci.ub\n1 0.5163978 0.06666667 0.2581989  2 0.04550026 0.01033725 1.022458\n```\n\n\n:::\n\n```{.r .cell-code}\n## replications\n\nR <- 10\nreps <- sim_studies(R = R, \n                    mu = theta_orig, \n                    tau2 = 0, \n                    n_orig)\n\nreps <- summary_es(reps)\n\nhead(reps)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  id        yi         vi       sei n0 n1       zi        pval         ci.lb\n1  1 0.6156174 0.09402554 0.3066358 30 30 2.007650 0.044680510  0.0146222059\n2  2 0.6065386 0.07744783 0.2782945 30 30 2.179485 0.029295672  0.0610914399\n3  3 0.8976386 0.07469491 0.2733037 30 30 3.284400 0.001021998  0.3619732038\n4  4 0.4881870 0.06182757 0.2486515 30 30 1.963338 0.049606882  0.0008390206\n5  5 0.3736972 0.09661308 0.3108264 30 30 1.202270 0.229259099 -0.2355114611\n6  6 0.7501443 0.07471205 0.2733351 30 30 2.744413 0.006061917  0.2144174895\n      ci.ub\n1 1.2166126\n2 1.1519858\n3 1.4333040\n4 0.9755350\n5 0.9829058\n6 1.2858712\n```\n\n\n:::\n:::\n\n\n\n\n\nLet's compute the proportions of replication studies are statistically significant:\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmean(reps$pval <= 0.05)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.9\n```\n\n\n:::\n:::\n\n\n\n\n\nLet's compute the proportions of replication studies with the same sign as the original:\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmean(sign(orig$yi) == sign(reps$yi))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 1\n```\n\n\n:::\n:::\n\n\n\n\n\nWe could also perform some statistical tests. See @Bushman2009-zv and @Hedges1980-gd for vote-counting methods in meta-analysis.\n\nAn extreme example:\n\n\n\n\n\n\n\n\n\n\n\nLet's imagine an original experiment with $n_{0} = 30$ and $\\hat \\theta_{0} = 0.5$ that is statistically significant $p \\approx 0.045$. Now a direct replication (thus assuming $\\tau^2 = 0$) study with $n_{1} = 350$ found $\\hat \\theta_{1} = 0.15$, that is statistically significant $p\\approx 0.047$.\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](chapter4_files/figure-html/unnamed-chunk-11-1.png){width=672}\n:::\n:::\n\n\n\n\n\nThe most problematic aspect of using only the information from the sign of the effect or the p value is completely losing the information about the **size of the effect** and the **precision**.\n\n## Confidence/prediction interval methods\n\nThe main improvement of confidence/prediction interval methods is taking into account the size of the effect and the precision. The confidence interval represent the sampling uncertainty around the estimated value. It is interpreted as the percentage of confidence intervals under repetition of the same sampling procedure that contains the true value.\n\nConfidence interval around an estimated effect $\\theta_r$ can be written as:\n\n$$\n95\\%\\;\\mbox{CI} = \\hat\\theta_r \\pm t_{\\alpha}\\sqrt{\\sigma^2_{r}}\n$$\n\nWhere $t_{\\alpha}$ is the critical test statistics for significance level $\\alpha$. Clearly, this version of the confidence interval is symmetric around the estimated value and the width is a function of the estimation precision.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nalpha <- 0.05\nn <- c(10, 50, 100, 500)\ntheta <- rep(0.3, length(n))\nse <- sqrt(1/n + 1/n)\ntc <- abs(qnorm(alpha/2))\nlb <- theta - se * tc\nub <- theta + se * tc\n\nplot(n, theta, ylim = c(min(lb), max(ub)), type = \"b\", pch = 19,\n     xlab = \"Sample Size\",\n     ylab = latex2exp::TeX(\"$\\\\theta\\\\;_{r}$\"))\nlines(n, ub, lty = \"dashed\")\nlines(n, lb, lty = \"dashed\")\n```\n\n::: {.cell-output-display}\n![](chapter4_files/figure-html/unnamed-chunk-12-1.png){width=672}\n:::\n:::\n\n\n\n\n\n### Replication effect within the original CI\n\n$$\n\\theta_{0} - t_{\\alpha} \\sqrt{\\sigma^2_{0}} < \\theta_{r} < \\theta_{0} + t_{\\alpha} \\sqrt{\\sigma^2_{0}}\n$$\n\n:::{.pros}\n- Take into account the size of the effect and the precision of $\\theta_{0}$\n:::\n\n:::{.cons}\n- The original study is assumed to be a reliable estimation\n- No extension for *many-to-one* designs\n- Low precise original studies lead to higher success rate\n:::\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nz_orig <- 2\nn_orig <- 30\ntheta_orig <- theta_from_z(z_orig, n_orig)\nn_rep <- 350\ntheta_rep <- 0.15\nse_orig <- sqrt(4 / (2 * n_orig))\nci_orig <- theta_orig + qnorm(c(0.025, 0.975)) * se_orig\ncurve(dnorm(x, theta_orig, se_orig), \n      -1, 2, \n      ylab = \"Density\", \n      xlab = latex2exp::TeX(\"$\\\\theta$\"))\nabline(v = ci_orig, lty = \"dashed\")\npoints(theta_orig, 0, pch = 19, cex = 2)\npoints(theta_rep, 0, pch = 19, cex = 2, col = \"firebrick\")\nlegend(\"topleft\", \n       legend = c(\"Original\", \"Replication\"), \n       fill = c(\"black\", \"firebrick\"))\n```\n\n::: {.cell-output-display}\n![](chapter4_files/figure-html/unnamed-chunk-13-1.png){width=672}\n:::\n:::\n\n\n\n\n\nOne potential problem of this method regards that low precise original studies are \"easier\" to replicate due to larger confidence intervals. \n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](chapter4_files/figure-html/unnamed-chunk-14-1.png){width=672}\n:::\n:::\n\n\n\n\n\n### Original study within replication CI\n\nThe same approach can be applied checking if the original effect size is contained within the replication confidence interval. Clearly these methods depends on the precision of studies. Formally:\n\n$$\n\\theta_{r} - t_{\\alpha} \\sqrt{\\sigma_{r}^2} < \\theta_{0} < \\theta_{r} + t_{\\alpha} \\sqrt{\\sigma_{r}^2}\n$$\n\nThe method has the same pros and cons of the previous approach. One advantage is that usually replication studies are more precise (higher sample size) thus the parameter and the % CI is more reliable.\n\n## Prediction interval PI\n\nThere is still one missing information from the CI method that is considering the uncertainty only from the original or the replication study. The prediction interval PI express the uncertainty of a future observation (in this case a study) and not on the estimated parameter.\n\nIf we compute the difference between the original and the replication study, the sampling distribution of the difference express the range of values that are expected *in absence of other source of variability*. In other terms, assuming that $\\tau^2 = 0$.\n\nIf the original and replication studies comes from the same population, the sampling distribution of the difference is centered on zero with a certain standard error $\\theta_{0} - \\theta_{r} \\sim \\mathcal{N}\\left(0, \\sqrt{\\sigma^2_{\\hat \\theta_{0} - \\hat \\theta_{r}}} \\right)$ (subscript $0$ to indicate that is expected to be sampled from the same population as $\\theta_{0}$)\n\n$$\n\\hat \\theta_{0} \\pm t_{\\alpha} \\sqrt{\\sigma^2_{\\theta_{0} - \\theta_{r}}}\n$$\n\nThe new standard error is the sum (assuming that the two studies are independent) of the two standard errors. Thus we are taking into account the estimation uncertainty of the original and replication studies.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(2025)\n\no1 <- rnorm(50, 0.5, 1) # group 1\no2 <- rnorm(50, 0, 1) # group 2\nod <- mean(o1) - mean(o2) # effect size\nse_o <- sqrt(var(o1)/50 + var(o2)/50) # standard error of the difference\n\nn_r <- 100 # sample size replication\n\nse_o_r <- sqrt(se_o^2 + (var(o1)/100 + var(o2)/100))\n\nod + qnorm(c(0.025, 0.975)) * se_o_r\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.325520 1.298378\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\npar(mar = c(4, 4, 0.1, 0.1))  \ncurve(dnorm(x, od, se_o_r), od - se_o_r*4, od + se_o_r*4, lwd = 2, xlab = latex2exp::TeX(\"$\\\\theta$\"), ylab = \"Density\")\nabline(v = od + qnorm(c(0.025, 0.975)) * se_o_r, lty = \"dashed\")\n```\n\n::: {.cell-output-display}\n![](chapter4_files/figure-html/unnamed-chunk-16-1.png){width=70%}\n:::\n:::\n\n\n\n\n\n:::{.pros}\n- Take into account uncertainty of both studies\n- We can plan a replication using the standard deviation of the original study and the expected sample size\n:::\n\n:::{.cons}\n- Low precision original studies lead to wider PI. For a replication study is difficult to fall outside the PI\n- Mainly for *one-to-one* replications design\n:::\n\n## Mathur & VanderWeele [-@Mathur2020-nw] $p_{orig}$\n\nMathur & VanderWeele [-@Mathur2020-nw] proposed a new method based on the prediction interval to calculate a p value $p_{orig}$ representing the probability that $\\theta_{0}$ is consistent with the replications. This method is suited for *many-to-one* replication designs. Formally:\n\n$$\nP_{orig} = 2 \\left[ 1 - \\Phi \\left( \\frac{|\\hat \\theta_{0} - \\hat \\mu_{\\theta_{R}}|}{\\sqrt{\\hat \\tau^2 + \\sigma^2_{\\theta_{0}} + \\hat{SE}^2_{\\hat \\mu_{\\theta_{R}}}}} \\right) \\right]\n$$\n\n- $\\hat \\mu_{\\theta_{R}}$ is the pooled (i.e., meta-analytic) estimation of the $k$ replications\n- $\\tau^2$ is the variance among replications (i.e., heterogeneity)\n\nIt is interpreted as the probability that $\\theta_{0}$ is equal or more extreme than what is observed. A very low $p_{orig}$ suggests that the original study is inconsistent with replications.\n\n:::{.pros}\n- Suited for *many-to-one* designs\n- We take into account all sources of uncertainty\n- We have a p-value\n:::\n\nThe code is implemented in the `Replicate` and `MetaUtility` R packages:\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntau2 <- 0.05\ntheta_rep <- 0.2\ntheta_orig <- 0.7\n\nn_orig <- 30\nn_rep <- 100\nk <- 20\n\nreplications <- sim_studies(k, theta_rep, tau2, n_rep, n_rep)\noriginal <- sim_studies(1, theta_orig, 0, n_orig, n_orig)\n\nfit_rep <- metafor::rma(yi, vi, data = replications) # random-effects meta-analysis\n\nReplicate::p_orig(original$yi, original$vi, fit_rep$b[[1]], fit_rep$tau2, fit_rep$se^2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.06692786\n```\n\n\n:::\n:::\n\n\n\n\n\n### Simulation\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# standard errors assuming same n and variance 1\nse_orig <- sqrt(4/(n_orig * 2))\nse_rep <- sqrt(4/(n_rep * 2))\nse_theta_rep <- sqrt(1/((1/(se_rep^2 + tau2)) * k)) # standard error of the random-effects estimate\n\nsep <- sqrt(tau2 + se_orig^2 + se_theta_rep^2) # z of p-orig denominator\n\ncurve(dnorm(x, theta_rep, sep), theta_rep - 4*sep, theta_rep + 4*sep, ylab = \"Density\", xlab = latex2exp::TeX(\"\\\\theta\"))\npoints(theta_orig, 0.02, pch = 19, cex = 2)\nabline(v = qnorm(c(0.025, 0.975), theta_rep, sep), lty = \"dashed\", col = \"firebrick\")\n```\n\n::: {.cell-output-display}\n![](chapter4_files/figure-html/unnamed-chunk-18-1.png){width=672}\n:::\n:::\n\n\n\n\n\n\n\n### Mathur & VanderWeele [-@Mathur2020-nw] $\\hat P_{> 0}$\n\nAnother related metric is the $\\hat P_{> 0}$, representing the proportion of replications following the same direction as the original effect. Before simply computing the proportions we need to adjust the estimated $\\theta_{r}$ ($r = 1, \\dots, R$) with a shrinkage factor:\n\n$$\n\\tilde{\\theta}_{r} = (\\theta_{r} - \\mu_{\\theta_{r}}) \\sqrt{\\frac{\\hat \\tau^2}{\\hat \\tau^2 + \\sigma^2_{r}}}\n$$\n\nWhere $\\sigma^2_r$ is the sampling variance for the replication effect size $r$. This method is somehow similar to the vote counting but we are adjusting the effects taking with a shrinkage factor based on $\\tau^2$.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\n# compute calibrated estimation for the replications\n# use restricted maximum likelihood to estimate tau2 under the hood\ntheta_sh <- MetaUtility::calib_ests(replications$yi, replications$sei, method = \"REML\")\nmean(theta_sh > 0)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.85\n```\n\n\n:::\n:::\n\n\n\n\n\nThe authors suggest a bootstrapping approach for making inference on $\\hat P_{> 0}$\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nnboot <- 1e3\ntheta_boot <- matrix(0, nrow = nboot, ncol = k)\n\nfor(i in 1:nboot){\n  idx <- sample(1:nrow(replications), nrow(replications), replace = TRUE)\n  replications_boot <- replications[idx, ]\n  theta_cal <- MetaUtility::calib_ests(replications_boot$yi, \n                                       replications_boot$sei, \n                                       method = \"REML\")\n  theta_boot[i, ] <- theta_cal\n}\n\n# calculate\np_greater_boot <- apply(theta_boot, 1, function(x) mean(x > 0))\n```\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![](chapter4_files/figure-html/unnamed-chunk-22-1.png){width=672}\n:::\n:::\n\n\n\n\n\n### Mathur & VanderWeele [-@Mathur2020-nw] $\\hat P_{\\gtrless q*}$\n\nInstead of using 0 as threshold, we can use meaningful effect size to be considered as low but different from 0. $\\hat P_{\\gtrless q*}$ is the proportion of (calibrated) replications greater or lower than the $q*$ value. This framework is similar to equivalence and minimum effect size testing [@Lakens2018-ri].\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nq <- 0.2 # minimum non zero effect\n\nfit <- metafor::rma(yi, vi, data = replications)\n\n# see ?MetaUtility::prop_stronger\nMetaUtility::prop_stronger(q = q,\n                           M = fit$b[[1]],\n                           t2 = fit$tau2,\n                           tail = \"above\",\n                           estimate.method = \"calibrated\",\n                           ci.method = \"calibrated\",\n                           dat = replications,\n                           yi.name = \"yi\",\n                           vi.name = \"vi\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   est        se lo  hi   bt.mn shapiro.pval\n1 0.35 0.1756669  0 0.6 0.35375    0.8137044\n```\n\n\n:::\n:::\n\n\n\n\n\n## Meta-analytic methods\n\nAnother approach is to combine the original and replication results (both *one-to-one* and *many-to-one*) using a meta-analysis model. Then we can test if the pooled estimate is different from 0 or another meaningful value.\n\n:::{.pros}\n- Use all the available information, especially when fitting a random-effects model\n- Take into account the precision by inverse-variance weighting\n:::\n\n:::{.cons}\n- Did not consider the publication bias\n- For *one-to-one* designs only a fixed-effects model can be used\n:::\n\n### Fixed-effects Model\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# fixed-effects\nfit_fixed <- rma(yi, vi, method = \"FE\")\nsummary(fit_fixed)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nFixed-Effects Model (k = 20)\n\n  logLik  deviance       AIC       BIC      AICc   \n 20.7415   -0.0000  -39.4829  -38.4872  -39.2607   \n\nI^2 (total heterogeneity / total variability):   0.00%\nH^2 (total variability / sampling variability):  0.00\n\nTest for Heterogeneity:\nQ(df = 19) = 0.0000, p-val = 1.0000\n\nModel Results:\n\nestimate      se    zval    pval   ci.lb   ci.ub      \n  0.2000  0.0316  6.3246  <.0001  0.1380  0.2620  *** \n\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\n\n\n\n\n### Random-Effects model\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# fixed-effects\nfit_random <- rma(yi, vi, method = \"REML\")\nsummary(fit_random)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nRandom-Effects Model (k = 20; tau^2 estimator: REML)\n\n  logLik  deviance       AIC       BIC      AICc   \n 19.7044  -39.4088  -35.4088  -33.5199  -34.6588   \n\ntau^2 (estimated amount of total heterogeneity): 0 (SE = 0.0065)\ntau (square root of estimated tau^2 value):      0\nI^2 (total heterogeneity / total variability):   0.00%\nH^2 (total variability / sampling variability):  1.00\n\nTest for Heterogeneity:\nQ(df = 19) = 0.0000, p-val = 1.0000\n\nModel Results:\n\nestimate      se    zval    pval   ci.lb   ci.ub      \n  0.2000  0.0316  6.3246  <.0001  0.1380  0.2620  *** \n\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\n\n\n\n\n### Pooling replications\n\nThe previous approach can be also implemented combining replications into a single effect and then compare the original with the combined replication study.\n\nThis is similar to using the CI or PI approaches but the replication effect will probably by very precise due to pooling multiple studies.\n\n### Q Statistics \n\nAn interesting proposal is using the Q statistics [@Hedges2019-pr; @Hedges2019-ar; @Hedges2021-of; @Schauer2022-mj; @Schauer2021-ja; @Schauer2020-tw; @Hedges2019-ry], commonly used in meta-analysis to assess the presence of heterogeneity. Formally:\n\n$$\nQ = \\sum_{r = 0}^{R} \\frac{(\\theta_r - \\bar \\theta_R)^2}{\\sigma^2_r}\n$$\n\nWhere $\\bar \\theta_R$ is the inverse-variance weighted average (i.g., fixed-effect model). The $Q$ statistics is essentially a weighted sum of squares. Under the null hypothesis where all studies are equal $\\theta_0 = \\theta_1, ... = \\theta_R$ the $Q$ statistics has a $\\chi^2$ distribution with $R - 1$ degrees of freedom. Under the alternative hypothesis the distribution is a non-central $\\chi^2$ with non centrality parameter $\\lambda$. The expected value of the $Q$ is $E(Q) = \\nu + \\lambda$, where $\\nu$ are the degrees of freedom.\n\nHedges & Schauer proposed to use the $Q$ statistics to evaluate the consistency of a series of replications:\n\n- In case of a **replication**, $\\lambda = 0$ because $\\theta_0 = \\theta_1, ... = \\theta_R$.\n- In case of an **extension**, $\\lambda < \\lambda_0$ where $\\lambda_0$ is the maximum value considered equivalent to zero. In other terms, $\\lambda_0$ is the threshold above which the variability is too high for being considered a neglegible quantity for a replication [in @Machery2020-sv terms].\n\nThis approach is testing the consistency (i.e., homogeneity) of replications. A successful replication should minimize the heterogeneity and the presence of a significant $Q$ statistics would suggest a failed replication attempt. @Hedges2019-pr and @Mathur2019-vh The discussed extensively this approach.\n\n- to cover different replication setup (burden of proof on replicating vs non-replicating, many-to-one and one-to-one, etc.)\n- interpreting and choosing an appropriate $\\lambda_0$ parameter\n- evaluating the power and statistical properties under different replication scenarios\n\nIn the case of evaluating a replication we can use the `Qrep()` function that simply calculates the p-value based on the Q sampling distribution.\n\n\n\n\n\n```r\nQrep <- function(yi, vi, lambda0 = 0, alpha = 0.05){\n  fit <- metafor::rma(yi, vi)\n  k <- fit$k\n  Q <- fit$QE\n  df <- k - 1\n  Qp <- pchisq(Q, df = df, ncp = lambda0, lower.tail = FALSE)\n  pval <- ifelse(Qp < 0.001, \"p < 0.001\", sprintf(\"p = %.3f\", Qp))\n  lambda <- ifelse((Q - df) < 0, 0, (Q - df))\n  res <- list(Q = Q, lambda = lambda, pval = Qp, df = df, k = k, alpha = alpha, lambda0 = lambda0)\n  H0 <- ifelse(lambda0 != 0, paste(\"H0: lambda <\", lambda0), \"H0: lambda = 0\")\n  title <- ifelse(lambda0 != 0, \"Q test for Approximate Replication\", \"Q test for Exact Replication\")\n  cli::cli_rule()\n  cat(cli::col_blue(cli::style_bold(title)), \"\\n\\n\")\n  cat(sprintf(\"Q = %.3f (df = %s), lambda = %.3f, %s\", res$Q, res$df, lambda, pval), \"\\n\")\n  cat(H0, \"\\n\")\n  cli::cli_rule()\n  class(res) <- \"Qrep\"\n  invisible(res)\n}\n```\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\nQ test for Exact Replication \n\nQ = 509.539 (df = 99), lambda = 410.539, p < 0.001 \nH0: lambda = 0 \n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nQres <- Qrep(dat$yi, dat$vi)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nQ test for Exact Replication \n\nQ = 509.539 (df = 99), lambda = 410.539, p < 0.001 \nH0: lambda = 0 \n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nplot.Qrep(Qres)\n```\n\n::: {.cell-output-display}\n![](chapter4_files/figure-html/unnamed-chunk-29-1.png){width=672}\n:::\n:::\n\n\n\n\n\n#### Q Statistics for an extension\n\nIn case of an extension we need to set $\\lambda_0$ to a meaningful value but the overall test is the same. The critical $Q$ is no longer evaluated with a central $\\chi^2$ but a non-central $\\chi^2$ with $\\lambda_0$ as non-centrality parameter.\n\n@Hedges2019-ry provides different strategies to choose $\\lambda_0$. They found that under some assumptions, $\\lambda = (R - 1) \\frac{\\tau^2}{\\tilde{v}}$\n\nGiven that we introduced the $I^2$ statistics we can derive a $\\lambda_0$ based on $I^2$. @Schmidt2014-kw proposed that when $\\tilde{v}$ is at least 75% of total variance $\\tilde{v} + \\tau^2$ thus $\\tau^2$ could be considered negligible. This corresponds to a $I^2 = 25%$ and a ratio $\\frac{\\tau^2}{\\tilde{v}} = 1/3$ thus $\\lambda_0 = \\frac{(R - 1)}{3}$ can be considered a neglegible heterogeneity.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nk <- 100\ndat <- sim_studies(k, 0.5, 0, 50, 50)\nQrep(dat$yi, dat$vi, lambda0 = (k - 1)/3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nQ test for Approximate Replication \n\nQ = 93.881 (df = 99), lambda = 0.000, p = 0.989 \nH0: lambda < 33 \n```\n\n\n:::\n:::\n\n\n\n\n\n## Small Telescopes [@Simonsohn2015-kg]\n\nThe idea is simple but quite powerful and insightful. Let's assume that an original study found an effect of $\\theta_{0} = 0.7$ on a two-sample design with $n = 20$ per group.\n\n- we define a threshold as the effect size that is associated with a certain low statistical power level e.g., $33\\%$ given the sample size i.e. $\\theta_{s} = 0.5$\n- the replication study found an effect of $y_{rep} = 0.2$ with $n = 100$ subjects\n\nIf the $\\theta_{r}$ is lower (i.e., the upper bound of the confidence interval) than the *small effect* ($\\theta_{s} = 0.5$) we conclude that the effect is probably so small that could not have been detected by the original study. Thus there is no evidence for a replication.\n    \nWe can use the custom `small_telescope()` function on simulated data:\n\n\n\n\n\n```r\nsmall_telescope <- function(or_d,\n                            or_se,\n                            rep_d,\n                            rep_se,\n                            small,\n                            ci = 0.95){\n  # quantile for the ci\n  qs <- c((1 - ci)/2, 1 - (1 - ci)/2)\n\n  # original confidence interval\n  or_ci <- or_d + qnorm(qs) * or_se\n\n  # replication confidence interval\n  rep_ci <- rep_d + qnorm(qs) * rep_se\n\n  # small power\n  is_replicated <- rep_ci[2] > small\n\n  msg_original <- sprintf(\"Original Study: d = %.3f %s CI = [%.3f, %.3f]\",\n                          or_d, ci, or_ci[1], or_ci[2])\n\n  msg_replicated <- sprintf(\"Replication Study: d = %.3f %s CI = [%.3f, %.3f]\",\n                            rep_d, ci, rep_ci[1], rep_ci[2])\n\n\n  if(is_replicated){\n    msg_res <- sprintf(\"The replicated effect is not smaller than the small effect (%.3f), (probably) replication!\", small)\n    msg_res <- cli::col_green(msg_res)\n  }else{\n    msg_res <- sprintf(\"The replicated effect is smaller than the small effect (%.3f), no replication!\", small)\n    msg_res <- cli::col_red(msg_res)\n  }\n\n  out <- data.frame(id = c(\"original\", \"replication\"),\n                    d = c(or_d, rep_d),\n                    lower = c(or_ci[1], rep_ci[1]),\n                    upper = c(or_ci[2], rep_ci[2]),\n                    small = small\n  )\n\n  # nice message\n  cat(\n    msg_original,\n    msg_replicated,\n    cli::rule(),\n    msg_res,\n    sep = \"\\n\"\n  )\n\n  invisible(out)\n\n}\n```\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nset.seed(2025)\n\nd <- 0.2 # real effect\n\n# original study\nor_n <- 20\nor_d <- 0.7\nor_se <- sqrt(1/20 + 1/20)\nd_small <- pwr::pwr.t.test(or_n, power = 0.33)$d\n\n# replication\nrep_n <- 100 # sample size of replication study\ng0 <- rnorm(rep_n, 0, 1)\ng1 <- rnorm(rep_n, d, 1)\n\nrep_d <- mean(g1) - mean(g0)\nrep_se <- sqrt(var(g1)/rep_n + var(g0)/rep_n)\n```\n:::\n\n\n\n\n\nHere we are using the `pwr::pwr.t.test()` to compute the effect size $\\theta_{small}$ (in code `d`) associated with 33% power.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsmall_telescope(or_d, or_se, rep_d, rep_se, d_small, ci = 0.95)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nOriginal Study: d = 0.700 0.95 CI = [0.080, 1.320]\nReplication Study: d = 0.214 0.95 CI = [-0.061, 0.490]\n────────────────────────────────────────────────────────────────────────────────\nThe replicated effect is smaller than the small effect (0.493), no replication!\n```\n\n\n:::\n:::\n\n\n\n\n\nAnd a (quite over-killed) plot:\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](chapter4_files/figure-html/unnamed-chunk-34-1.png){width=672}\n:::\n:::\n\n\n\n\n\n## Replication Bayes factor\n\n@Verhagen2014-tx proposed a method to estimate the evidence of a *replication* study. The core topics to understand the method are:\n\n- Bayesian hypothesis testing using the Bayes Factor [see, @Rouder2009-jh]\n- Bayes Factor using the Savage-Dickey density ratio [SDR, @Wagenmakers2010-fj]\n\n### Bayesian inference\n\nBayesian inference is the statistical procedure where **prior beliefs** about a phenomenon are combined, using the Bayes theorem, with **evidence from data** to obtain the **posterior beliefs**. \n\nThe interesting part is that the researcher expresses the prior beliefs in probabilistic terms. Then after collecting data, evidence from the experiment is combined, increasing or decreasing the plausibility of prior beliefs.\n\nLet's make an (not very innovative :smile:) example. We need to evaluate the fairness of a coin. The crucial parameter is $\\pi$ that is the probability of success (e.g., head). We have our prior belief about the coin (e.g., fair but with some uncertainty). We toss the coin $k$ times and we observe $x$ heads. What are my conclusions?\n\n$$\np(\\pi|D) = \\frac{p(D|\\pi) \\; p(\\pi)}{p(D)}\n$$\nWhere $\\pi$ is our parameter and $D$ the data. $p(\\pi|D)$ is the posterior distribution that is the product between the likelihood $p(D|\\pi)$ and the prior $p(\\pi)$. $p(D)$ is the probability of the data (aka marginal likelihood) and is necessary only for the posterior to be a proper probability distribution.\n\nWe can \"read\" the formula as: *The probability of the parameter given the data is the product between the likelihood of the data given the parameter and the prior probability of the parameter*.\n\nLet's express our **prior** belief in probabilistic terms:\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](chapter4_files/figure-html/unnamed-chunk-35-1.png){width=672}\n:::\n:::\n\n\n\n\n\nNow we collect data and we observe $x = 40$ tails out of $k = 50$ trials thus $\\hat{\\pi} = 0.8$ and compute the *likelihood*:\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](chapter4_files/figure-html/unnamed-chunk-36-1.png){width=672}\n:::\n:::\n\n\n\n\n\nFinally we combine, using the Bayes rule, **prior** and **likelihood** to obtain the **posterior** distribution:\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](chapter4_files/figure-html/unnamed-chunk-37-1.png){width=672}\n:::\n:::\n\n\n\n\n\nThe idea of the Bayes Factor is computing the evidence of the data under two competing hypotheses, $H_0$ and $H_1$ (~ $\\pi$ in our previous example):\n\n$$\n\\frac{p(H_0|D)}{p(H_1|D)} = \\frac{f(D|H_0)}{f(D|H_1)} \\times \\frac{p(H_0)}{p(H_1)}\n$$\n\nWhere $f$ is the likelihood function, $y$ are the data. The $\\frac{p(H_0)}{p(H_1)}$ is the prior odds of the two hypotheses. The Bayes Factor is the ratio between the likelihood of the data under the two hypotheses.\n\n### Bayes Factor using the SDR\n\nCalculating the BF can be problematic in some conditions. The SDR is a convenient shortcut to calculate the Bayes Factor [@Wagenmakers2010-fj]. The idea is that the ratio between the prior and posterior density distribution for the $H_1$ is an estimate of the Bayes factor calculated in the standard way.\n    \n$$\nBF_{01} = \\frac{p(D|H_0)}{p(D|H_1)} = \\frac{p(\\pi = x|D, H_1)}{p(\\pi = x, H_1)}\n$$\n\nWhere $\\pi$ is the parameter of interest and $x$ is the null value under $H_0$ e.g., 0. and $D$ are the data.\n\nFollowing the previous example $H_0: \\pi = 0.5$. Under $H_1$ we use a completely uninformative prior by setting $\\pi \\sim Beta(1, 1)$.\n\nWe flipped the coin 20 times and we found that $\\hat \\pi = 0.75$.\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](chapter4_files/figure-html/unnamed-chunk-38-1.png){width=672}\n:::\n:::\n\n\n\n\n\nThe ratio between the two black dots is the Bayes Factor.\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](chapter4_files/figure-html/unnamed-chunk-39-1.png){width=672}\n:::\n:::\n\n\n\n\n\n### @Verhagen2014-tx model^[see also @Ly2019-ow for an improvement]\n\nThe idea is using the posterior distribution of the original study as prior for a Bayesian hypothesis testing where:\n\n- $H_0: \\theta_{r} = 0$ thus there is no effect in the replication study\n- $H_1: \\theta_{r} \\neq 0$ and in particular is distributed as $\\delta \\sim \\mathcal{N}(\\theta_{0}, \\sigma^2_{0})$ where $\\theta_{0}$ and $\\sigma^2_{0}$ are the mean and standard error of the original study\n\nIf $H_0$ is more likely after seeing the data, there is evidence against the replication (i.e., $BF_{r0} > 1$) otherwise there is evidence for a successful replication ($BF_{r1} > 1$).\n\n::: {.callout-warning}\n**Disclaimer:** The actual implementation of @Verhagen2014-tx is different (they use the $t$ statistics). We implemented a similar model using a Bayesian linear model with `rstanarm`.\n:::\n\nLet's assume that the original study ($n = 30$) estimates a $\\theta_{0} = 0.4$ and a standard error $\\sigma^2/n$.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\n# original study\nn <- 30\nyorig <- 0.4\nse <- sqrt(1/n)\n```\n:::\n\n\n\n\n\n::: {.callout-note}\nThe assumption of @Verhagen2014-tx is that the original study performed a Bayesian analysis with a completely flat prior. Thus the confidence interval is the same as the Bayesian credible interval.\n:::\n\nFor this reason, the posterior distribution of the original study can be approximated as:\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](chapter4_files/figure-html/unnamed-chunk-41-1.png){width=672}\n:::\n:::\n\n\n\n\n\nLet's imagine that a new study tried to replicate the original one. They collected $n = 100$ participants with the same protocol and found an effect of $y_{rep} = 0.1$.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnrep <- 100\nyrep <- MASS::mvrnorm(nrep, mu = 0.1, Sigma = 1, empirical = TRUE)[, 1]\ndat <- data.frame(y = yrep)\nhist(yrep, main = \"Replication Study (n1 = 100)\", xlab = latex2exp::TeX(\"$\\\\theta_{r}$\"))\nabline(v = mean(yrep), lwd = 2, col = \"firebrick\")\n```\n\n::: {.cell-output-display}\n![](chapter4_files/figure-html/unnamed-chunk-42-1.png){width=672}\n:::\n:::\n\n\n\n\n\nWe can analyze these data with an *intercept-only regression model* setting as prior the posterior distribution of the original study:\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# setting the prior on the intercept parameter\nprior <- rstanarm::normal(location = yorig,\n                          scale = se)\n\n# fitting the bayesian linear regression\nfit <- stan_glm(y ~ 1, \n                data = dat, \n                prior_intercept = prior,\n                refresh = FALSE)\n\nsummary(fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nModel Info:\n function:     stan_glm\n family:       gaussian [identity]\n formula:      y ~ 1\n algorithm:    sampling\n sample:       4000 (posterior sample size)\n priors:       see help('prior_summary')\n observations: 100\n predictors:   1\n\nEstimates:\n              mean   sd   10%   50%   90%\n(Intercept) 0.2    0.1  0.1   0.2   0.3  \nsigma       1.0    0.1  0.9   1.0   1.1  \n\nFit Diagnostics:\n           mean   sd   10%   50%   90%\nmean_PPD 0.2    0.1  0.0   0.2   0.3  \n\nThe mean_ppd is the sample average posterior predictive distribution of the outcome variable (for details see help('summary.stanreg')).\n\nMCMC diagnostics\n              mcse Rhat n_eff\n(Intercept)   0.0  1.0  2609 \nsigma         0.0  1.0  2608 \nmean_PPD      0.0  1.0  3185 \nlog-posterior 0.0  1.0  1693 \n\nFor each parameter, mcse is Monte Carlo standard error, n_eff is a crude measure of effective sample size, and Rhat is the potential scale reduction factor on split chains (at convergence Rhat=1).\n```\n\n\n:::\n:::\n\n\n\n\n\nWe can use the `bayestestR::bayesfactor_pointnull()` to calculate the BF using the Savage-Dickey density ratio.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbf <- bayestestR::bayesfactor_pointnull(fit, null = 0)\nprint(bf)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nBayes Factor (Savage-Dickey density ratio)\n\nParameter   |    BF\n-------------------\n(Intercept) | 0.265\n\n* Evidence Against The Null: 0\n```\n\n\n:::\n\n```{.r .cell-code}\nplot(bf)\n```\n\n::: {.cell-output-display}\n![](chapter4_files/figure-html/unnamed-chunk-44-1.png){width=672}\n:::\n:::\n\n\n\n\n\nYou can also use the `bf_replication()` function:\n\n\n\n\n\n```r\nbf_replication <- function(mu_original,\n                           se_original,\n                           replication){\n\n  # prior based on the original study\n  prior <- rstanarm::normal(location = mu_original, scale = se_original)\n\n  # to dataframe\n  replication <- data.frame(y = replication)\n\n  fit <- rstanarm::stan_glm(y ~ 1,\n                            data = replication,\n                            prior_intercept = prior,\n                            refresh = 0) # avoid printing\n\n  bf <- bayestestR::bayesfactor_pointnull(fit, null = 0, verbose = FALSE)\n\n  title <- \"Bayes Factor Replication Rate\"\n  posterior <- \"Posterior Distribution ~ Mean: %.3f, SE: %.3f\"\n  replication <- \"Evidence for replication: %3f (log %.3f)\"\n  non_replication <- \"Evidence for non replication: %3f (log %.3f)\"\n\n  if(bf$log_BF > 0){\n    replication <- cli::col_green(sprintf(replication, exp(bf$log_BF), bf$log_BF))\n    non_replication <- sprintf(non_replication, 1/exp(bf$log_BF), -bf$log_BF)\n  }else{\n    replication <- sprintf(replication, exp(bf$log_BF), bf$log_BF)\n    non_replication <- cli::col_red(sprintf(non_replication, 1/exp(bf$log_BF), -bf$log_BF))\n  }\n\n  outlist <- list(\n    fit = fit,\n    bf = bf\n  )\n\n  cat(\n    cli::col_blue(title),\n    cli::rule(),\n    sprintf(posterior, fit$coefficients, fit$ses),\n    \"\\n\",\n    replication,\n    non_replication,\n    sep = \"\\n\"\n  )\n\n  invisible(outlist)\n\n}\n```\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nbf_replication(mu_original = yorig, se_original = se, replication = yrep)\n```\n:::\n\n\n\n\n\nA better custom plot:\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbfplot <- data.frame(\n  prior = rnorm(1e5, yorig, se),\n  posterior = rnorm(1e5, fit$coefficients, fit$ses)\n)\n \nggplot() +\n  stat_function(geom = \"line\", \n                aes(color = \"Original Study (Prior)\"),\n                linewidth = 1,\n                alpha = 0.3,\n                fun = dnorm, args = list(mean = yorig, sd = se)) +\n  stat_function(geom = \"line\",\n                linewidth = 1,\n                aes(color = \"Replication Study (Posterior)\"),\n                fun = dnorm, args = list(mean = fit$coefficients, sd = fit$ses)) +\n  xlim(c(-0.5, 1.2)) +\n  geom_point(aes(x = c(0, 0), y = c(dnorm(0, yorig, sd = se),\n                                    dnorm(0, fit$coefficients, sd = fit$ses))),\n             size = 3) +\n  xlab(latex2exp::TeX(\"\\\\delta\")) +\n  ylab(\"Density\") +\n  theme(legend.position = \"bottom\",\n        legend.title = element_blank())\n```\n\n::: {.cell-output-display}\n![](chapter4_files/figure-html/unnamed-chunk-47-1.png){width=672}\n:::\n:::\n\n\n\n\n\n## Conclusions\n\nWe believe this selection of methods is an appropriate overview of statistical approaches to different definitions of replication. The meta-analytic and $Q$ methods are more focused on accumulating evidence combining original and replication studies. The CI and PI methods are more interested in comparing the original and replication(s) study. The BF method can be considered as an hybrid approach evaluating the evidence of null effect from the posterior distribution of the replication study. \n\n## References {.unnumbered}\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}