{
  "hash": "09855d61950f8c45c7dc3496685e7903",
  "result": {
    "engine": "knitr",
    "markdown": "<!-- TODO:  table with method (rows), (colums)properties, id exact or extension, bayesian or not, open source code, citation -->\n\n# Statistical methods for replication assessment\n\n\n\n\n\n::: {.cell}\n\n:::\n\n\n\n\n\n@Anderson2024-cx suggests that also the perspective of supporting the null hypothesis should be taken into account especially when we are really skeptical about the initial results. We want to replicate an effect to see the reliability, maybe better estimating the true effect with a larger sample (taking into account the inflation of the original effect) but we could also think that the original effect is a false positive and we want to do the replication to see if the null hypothesis is supported. The bayes factor approach by @Ly2019-ow @Verhagen2014-tx is exactly about this. also equivalence testing is useful\n\n<!-- TODO check the final table and the overall structure of the paper because it's great also the references are amazing -->\n\n\n\n\n\n```{mermaid}\ngraph TD\n    A[Statistical Methods]\n    A --> B[Sign/Direction]\n    B --> C[Vote Counting]\n    A --> D[Confidence/Prediction Interval]\n    D --> E[CI/PI original/replication]\n    D --> F[Small Telescope]\n    A --> G[Meta-analysis]\n    G --> H[Equal and Random-effects]\n    G --> h[cumulative meta analysis]\n    G --> I[Q Statistics]\n    A --> J[Bayesian Methods]\n    J --> K[Replication Bayes Factor]\n    J --> z[Pawel model]\n    J --> a[Skeptical p value]\n    J --> b[Skeptical prior etz]\n```\n\n\n\n\n\n<!-- TODO the idea here is also to keep track of the methods providing a clear overview -->\n\n## Introduction\n\nIn the previous chapters we introduced replication from a statistical and theoretical point of view. We purposely omitted defining the outcome of a replication study to dedicate an entire chapter about this part. Similarly to the multiple definitions problem there are several statistical measures for the replication assessment. There are also multiple ways to propose a classification but we could indentify:\n\n<!-- \npropose some kind of classification/taxonomy here. for example:\n\n- linked or not to a definition\n- aim of the metric (e.g., testing or estimating)\n- statistical approach (bayesian, frequentists, etc.)\n- practical limitations (only for one-to-one/many design, only with raw data, etc.)\n-->\n\nIn addition to the multitude of methods, the field of replication measures is relatively new and keep growing proposing new metrics, simulation studies, and comparisons between methods. \n\nA review work could be in principle the best option but this is not feasible and also useful for a very active research area. There is an amazing project proposed by @Heyard2024-hv where an extensive systematic review is supported by an online and keep up-to-date database with all replication measures organized and classified according to a common methodology. The authors reviewed roughly 50 different measures. The database is avaliable at the following link [http://rachelheyard.com/reproducibility_metrics/](http://rachelheyard.com/reproducibility_metrics/).\n\nWe decided to make a selection of methods from the database according to the following criteria:\n\n- for similar methods we keep the most recent and general version. For example, if a method $x$ originally developed for *one-to-one* replication designs has been extended to cover also *one-to-many* designs we will present the latter.\n- we select both bayesian and frequentist methods\n- we selected methods more related to inference and also methods more focused on estimation of the effect size\n- we selected methods that are not linked to a specific research area or topic. For example in the database there are methods for *voxels* in fMRI research or methods for metrology.\n\nAn important point is related to the choice of the method when evaluating a replication result. The @Heyard2024-hv work shows the amount of different methods and definitions of replication success without formally comparing them. This is not a problematic aspect because in practice, large scale replication projects such as the <!-- TODO add the references --> used different methods for the same dataset. In this context is important clearly use the method, and the related replication definition, closest to our aim.\n\n## Methods\n\n- [x] Prediction interval: replication effect in original 95% prediction interval\n- [x] Proportion of population effects agreeing in direction with the original\n- [x] Bayes Factor: Independent Jeffreys-Zellner-Siow BF test (default BF)\n- Bayes Factor: Equality-of-effect-size BF test\n- Bayes Factor: Fixed-effect meta-analysis BF Test (Meta-analytic BF) [this probably just a bayesian meta-analysis]\n- Bayesian Evidence Synthesis (variant: Meta-Analysis Model-based Assessment of replicability (MAMBA))\n- Bayesian mixture model for reproducibility rate\n- Confidence interval: original effect in replication 95% CI (Coverage)\n- Confidence interval: replication effect in original 95%CI (Capture probability)\n- Continuously cumulating meta-analytic approach\n- Correspondence test\n- Credibility analysis (Reverse-Bayes, probability of credibility, probability of replicating an effect)\n- Design analysis [type-m/s]\n- Equivalence testing (TOST (two one-sided tests))\n- Likelihood-based approach for reproducibility (Likelihood-ratio) [similar to bf?]\n- Minimum effect testing [similar to small telescope?]\n- P interval\n- Prediction interval: replication effect in original 95% prediction interval\n- Replication Bayes factor [already included]\n- Sceptical $p$-value (versions: nominal sceptical $p$-value, golden sceptical $p$-value, controlled sceptical $p$-value)\n- Sceptical Bayes Factor (Reverse-Bayes)\n- Small Telescopes\n- Snapshot hybrid (Bayesian meta-analysis)\n- Z-curve (Exact replication rate, p-curves)\n- Consistency of original with replications, $P_{\\mbox{orig}}$\n- I squared - $I^2$ (Estimation of effect variance) [this can be togheter with Q and meta-analysis]\n- Proportion of population effects agreeing in direction with the original, $\\hat{P}_{>0}$\n- Bland-Altman Plot (Agreement measures)\n- Correlation between effects\n- Difference in effect size (Q-statistic, (meta-analytic) Q-test, difference test, Tukeyâ€™s post-hoc honest significant difference test)\n- Externally standardized residuals [idea di calcolare tipo m su effetto stimato]\n- Meta-analysis\n- Significance criterion (vote counting, two-trials rule, regulatory agreement)\n\n## Replication studies as meta-analysis\n\nAs introduced by @Hedges2019-ry replication studies can be seen from a statistical point of view as meta-analyses. There is a single initial study and one or more attempts to replicate this initial finding. For the simple one-to-one replication design we have a meta-analysis with two studies while in the one-to-many design we have a meta-analysis with $k$ studies. When comes to evaluate the results of the replication studies we can choose between several methods [see @Heyard2024-hv] and some of them are specifically meta-analysis based trying to pool togheter evidence from original and replication studies. We consider the meta-analytic thinking crucial to understand the replication studies and methods but not all methods are strictly meta-analyes in the usual sense.\n\nIn fact, while the methodology of the initial or replication studies is important when comes to evaluate the single result, at the aggregated level we evaluate how a certain focal parameter (e.g., the difference between two groups or conditions) vary across replications and id there is evidence, whatever the criteria, for a successful replication. Whatever is the model used in the original studies we can essentially think at the aggregated level without loss of generality loss.\n\n## Simulating data\n\nWhile real-world examples are important, to understand the replication methods from a statistical point of view, simulating simplified examples is a good strategies. Furthermore, simulating data is nowadays considered an important tool to teach and understand statistical methods <!-- TODO add reference about simulating for learning -->. In additions, Monte Carlo simulations are necessary to estimate statistical properties (e.g., statistical power or type-1 error rate) of complex models.\n\nFor the simulated examples we can define the following simulation approach:\n\n- primary studies always compare two independent groups on a certain response variable\n- within a study, the two groups are assumed to comes from two normal distributions with unit variance. For one group the mean is centered on zero and for the other group the mean is centered on the value representing the effect size for that specific study.\n- the sample size can vary between the two groups\n\nUsing the same notation as @sec-prob-replication we can define:\n\n- $y_0$: as the reference group\n- $y_1$: as the treated group\n\n<!-- TODO add more notation here -->\n\nFor example we can simulate a single study:\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nes <- 0.3\nn0 <- 30\nn1 <- 30\n\ny0 <- rnorm(n0, 0, 1)\ny1 <- rnorm(n1, es, 1)\n\nmean(y1) - mean(y0) # this is the effect size\n## [1] 0.1770251\nvar(y1)/n1 + var(y0)/n0 # this is the sampling variability\n## [1] 0.06015214\n```\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![](chapter4_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n\n\n\n\n\nNow we can simply iterate the process for $R$ studies to create a series of replications with a common true parameter $\\theta$. We can also generate a more realistic set of sample sizes sampling from a probability distribution (e.g., Poisson). Then, if we want to include variability in the true effects as in the extension framework we can simply sample $k$ $\\theta$'s from a normal distribution with variability $\\tau^2$.\n\n\n\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n   id           yi         vi n0 n1\n1   1 -0.309182883 0.04721305 40 40\n2   2 -0.007160845 0.05244029 38 38\n3   3  0.400803196 0.02992522 58 58\n4   4  0.598311744 0.05014373 43 43\n5   5  0.261480295 0.06766628 41 41\n6   6  0.449684404 0.05287953 37 37\n7   7  0.346506342 0.05851893 33 33\n8   8  0.317175615 0.05576734 41 41\n9   9  0.640812151 0.04657143 48 48\n10 10  0.347284159 0.06991821 34 34\n```\n\n\n:::\n:::\n\n\n\n\n\nThen we can put everything in a function that can be used to simulate different scenarios.\n\n\n\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n   id        yi         vi n0 n1\n1   1 0.7165347 0.05813802 30 30\n2   2 0.6992119 0.04220137 30 30\n3   3 0.4546156 0.05948486 30 30\n4   4 0.9265450 0.05913646 30 30\n5   5 0.2003159 0.06838246 30 30\n6   6 0.7966482 0.07168206 30 30\n7   7 0.5775708 0.05543920 30 30\n8   8 0.3816547 0.05839979 30 30\n9   9 0.3395412 0.06251267 30 30\n10 10 0.3649269 0.06040433 30 30\n```\n\n\n:::\n:::\n\n\n\n\n\n## References\n",
    "supporting": [
      "chapter4_files/figure-html"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}