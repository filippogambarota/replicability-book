{
  "hash": "d21e453ab463558e80e4d58d6598f388",
  "result": {
    "engine": "knitr",
    "markdown": "<!-- TODO:  table with method (rows), (colums)properties, id exact or extension, bayesian or not, open source code, citation -->\n\n# Statistical methods for replication assessment\n\n\n\n\n\n::: {.cell}\n\n:::\n\n\n\n\n\n@Anderson2024-cx suggests that also the perspective of supporting the null hypothesis should be taken into account especially when we are really skeptical about the initial results. We want to replicate an effect to see the reliability, maybe better estimating the true effect with a larger sample (taking into account the inflation of the original effect) but we could also think that the original effect is a false positive and we want to do the replication to see if the null hypothesis is supported. The bayes factor approach by @Ly2019-ow @Verhagen2014-tx is exactly about this. also equivalence testing is useful\n\n<!-- TODO check the final table and the overall structure of the paper because it's great also the references are amazing -->\n\n\n\n\n\n```{mermaid}\ngraph TD\n    A[Statistical Methods]\n    A --> B[Sign/Direction]\n    B --> C[Vote Counting]\n    A --> D[Confidence/Prediction Interval]\n    D --> E[CI/PI original/replication]\n    D --> F[Small Telescope]\n    A --> G[Meta-analysis]\n    G --> H[Equal and Random-effects]\n    G --> h[cumulative meta analysis]\n    G --> I[Q Statistics]\n    A --> J[Bayesian Methods]\n    J --> K[Replication Bayes Factor]\n    J --> z[Pawel model]\n    J --> a[Skeptical p value]\n    J --> b[Skeptical prior etz]\n```\n\n\n\n\n\n<!-- TODO the idea here is also to keep track of the methods providing a clear overview -->\n\n## Introduction\n\nIn the previous chapters we introduced replication from a statistical and theoretical point of view. We purposely omitted defining the outcome of a replication study to dedicate an entire chapter about this part. Similarly to the multiple definitions problem there are several statistical measures for the replication assessment. There are also multiple ways to propose a classification but we could indentify:\n\n<!-- \npropose some kind of classification/taxonomy here. for example:\n\n- linked or not to a definition\n- aim of the metric (e.g., testing or estimating)\n- statistical approach (bayesian, frequentists, etc.)\n- practical limitations (only for one-to-one/many design, only with raw data, etc.)\n-->\n\nIn addition to the multitude of methods, the field of replication measures is relatively new and keep growing proposing new metrics, simulation studies, and comparisons between methods. \n\nA review work could be in principle the best option but this is not feasible and also useful for a very active research area. There is an amazing project proposed by @Heyard2024-hv where an extensive systematic review is supported by an online and keep up-to-date database with all replication measures organized and classified according to a common methodology. The authors reviewed roughly 50 different measures. The database is avaliable at the following link [http://rachelheyard.com/reproducibility_metrics/](http://rachelheyard.com/reproducibility_metrics/).\n\nWe decided to make a selection of methods from the database according to the following criteria:\n\n- for similar methods we keep the most recent and general version. For example, if a method $x$ originally developed for *one-to-one* replication designs has been extended to cover also *one-to-many* designs we will present the latter.\n- we select both bayesian and frequentist methods\n- we selected methods more related to inference and also methods more focused on estimation of the effect size\n- we selected methods that are not linked to a specific research area or topic. For example in the database there are methods for *voxels* in fMRI research or methods for metrology.\n\nAn important point is related to the choice of the method when evaluating a replication result. The @Heyard2024-hv work shows the amount of different methods and definitions of replication success without formally comparing them. This is not a problematic aspect because in practice, large scale replication projects such as the <!-- TODO add the references --> used different methods for the same dataset. In this context is important clearly use the method, and the related replication definition, closest to our aim.\n\n## Methods\n\n- [x] Prediction interval: replication effect in original 95% prediction interval\n- [x] Proportion of population effects agreeing in direction with the original\n- [x] Bayes Factor: Independent Jeffreys-Zellner-Siow BF test (default BF)\n- Bayes Factor: Equality-of-effect-size BF test\n- Bayes Factor: Fixed-effect meta-analysis BF Test (Meta-analytic BF) [this probably just a bayesian meta-analysis]\n- Bayesian Evidence Synthesis (variant: Meta-Analysis Model-based Assessment of replicability (MAMBA))\n- Bayesian mixture model for reproducibility rate\n- Confidence interval: original effect in replication 95% CI (Coverage)\n- Confidence interval: replication effect in original 95%CI (Capture probability)\n- Continuously cumulating meta-analytic approach\n- Correspondence test\n- Credibility analysis (Reverse-Bayes, probability of credibility, probability of replicating an effect)\n- Design analysis [type-m/s]\n- Equivalence testing (TOST (two one-sided tests))\n- Likelihood-based approach for reproducibility (Likelihood-ratio) [similar to bf?]\n- Minimum effect testing [similar to small telescope?]\n- P interval\n- Prediction interval: replication effect in original 95% prediction interval\n- Replication Bayes factor [already included]\n- Sceptical $p$-value (versions: nominal sceptical $p$-value, golden sceptical $p$-value, controlled sceptical $p$-value)\n- Sceptical Bayes Factor (Reverse-Bayes)\n- Small Telescopes\n- Snapshot hybrid (Bayesian meta-analysis)\n- Z-curve (Exact replication rate, p-curves)\n- Consistency of original with replications, $P_{\\mbox{orig}}$\n- I squared - $I^2$ (Estimation of effect variance) [this can be togheter with Q and meta-analysis]\n- Proportion of population effects agreeing in direction with the original, $\\hat{P}_{>0}$\n- Bland-Altman Plot (Agreement measures)\n- Correlation between effects\n- Difference in effect size (Q-statistic, (meta-analytic) Q-test, difference test, Tukeyâ€™s post-hoc honest significant difference test)\n- Externally standardized residuals [idea di calcolare tipo m su effetto stimato]\n- Meta-analysis\n- Significance criterion (vote counting, two-trials rule, regulatory agreement)\n\n## Replication studies as meta-analysis\n\nAs introduced by @Hedges2019-ry replication studies can be seen from a statistical point of view as meta-analyses. There is a single initial study and one or more attempts to replicate this initial finding. For the simple one-to-one replication design we have a meta-analysis with two studies while in the one-to-many design we have a meta-analysis with $k$ studies. When comes to evaluate the results of the replication studies we can choose between several methods [see @Heyard2024-hv] and some of them are specifically meta-analysis based trying to pool togheter evidence from original and replication studies. We consider the meta-analytic thinking crucial to understand the replication studies and methods but not all methods are strictly meta-analyes in the usual sense.\n\nIn fact, while the methodology of the initial or replication studies is important when comes to evaluate the single result, at the aggregated level we evaluate how a certain focal parameter (e.g., the difference between two groups or conditions) vary across replications and id there is evidence, whatever the criteria, for a successful replication. Whatever is the model used in the original studies we can essentially think at the aggregated level without loss of generality loss.\n\n## Simulating data\n\nWhile real-world examples are important, to understand the replication methods from a statistical point of view, simulating simplified examples is a good strategies. Furthermore, simulating data is nowadays considered an important tool to teach and understand statistical methods <!-- TODO add reference about simulating for learning -->. In additions, Monte Carlo simulations are necessary to estimate statistical properties (e.g., statistical power or type-1 error rate) of complex models.\n\nFor the simulated examples we can define the following simulation approach:\n\n- primary studies always compare two independent groups on a certain response variable\n- within a study, the two groups are assumed to comes from two normal distributions with unit variance. For one group the mean is centered on zero and for the other group the mean is centered on the value representing the effect size for that specific study.\n- the sample size can vary between the two groups\n\nUsing the same notation as @sec-prob-replication we can define:\n\n- $y_0$: as the reference group\n- $y_1$: as the treated group\n\n<!-- TODO add more notation here -->\n\nFor example we can simulate a single study:\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nes <- 0.3\nn0 <- 30\nn1 <- 30\n\ny0 <- rnorm(n0, 0, 1)\ny1 <- rnorm(n1, es, 1)\n\nmean(y1) - mean(y0) # this is the effect size\n## [1] 0.5053206\nvar(y1)/n1 + var(y0)/n0 # this is the sampling variability\n## [1] 0.06662781\n```\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![](chapter4_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n\n\n\n\n\nNow we can simply iterate the process for $R$ studies to create a series of replications with a common true parameter $\\theta$. We can also generate a more realistic set of sample sizes sampling from a probability distribution (e.g., Poisson). Then, if we want to include variability in the true effects as in the extension framework we can simply sample $k$ $\\theta$'s from a normal distribution with variability $\\tau^2$.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nk <- 10\nmu <- 0.3 # average true effect\ntau2 <- 0.1 # heterogeneity\ntheta <- rnorm(k, mu, sqrt(tau2)) # true effects for R studies\n\nyi <- vi <- rep(NA, k)\nn0 <- n1 <- 10 + rpois(k, 40 - 10)\n\nfor(i in 1:k){\n    y0 <- rnorm(n0[i], 0, 1)\n    y1 <- rnorm(n1[i], theta[i], 1)\n    yi[i] <- mean(y1) - mean(y0)\n    vi[i] <- var(y1)/n1[i] + var(y0)/n0[i]\n}\n\nsim <- data.frame(id = 1:k, yi, vi, n0, n1)\nsim\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   id          yi         vi n0 n1\n1   1  0.52909205 0.05088875 41 41\n2   2  0.02704910 0.04171950 44 44\n3   3 -0.39499518 0.04754986 44 44\n4   4  0.65187235 0.06602157 38 38\n5   5  0.28071535 0.04203564 44 44\n6   6  0.76716325 0.05185400 35 35\n7   7  0.20233509 0.05050333 39 39\n8   8  0.07102992 0.04261760 36 36\n9   9 -0.35075220 0.06125763 35 35\n10 10 -0.29547640 0.04586728 44 44\n```\n\n\n:::\n:::\n\n\n\n\n\nThen we can put everything in a function that can be used to simulate different scenarios.\n\n\n\n\n\n\n```{.r .cell-code}\nfilor::print_fun(c(funs$sim_study, funs$sim_studies))\n```\n\n```r\n```\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n   id        yi         vi       sei n0 n1\n1   1 0.4358275 0.08427709 0.2903052 30 30\n2   2 0.3306699 0.05005760 0.2237356 30 30\n3   3 0.4205573 0.07437415 0.2727162 30 30\n4   4 0.6171794 0.05300495 0.2302280 30 30\n5   5 0.8807895 0.07827960 0.2797849 30 30\n6   6 0.3317133 0.07050544 0.2655286 30 30\n7   7 0.4252083 0.05187646 0.2277640 30 30\n8   8 0.4956010 0.08486217 0.2913111 30 30\n9   9 0.2790063 0.05735929 0.2394980 30 30\n10 10 0.4875873 0.07091980 0.2663077 30 30\n```\n\n\n:::\n:::\n\n\n\n\n\n## Vote Counting based on significance or direction\n\nThe simplest method is called **vote counting** [@Valentine2011-yq; @Hedges1980-gd]. A replication attempt $\\theta_{rep}$ is considered successful if the result has the same direction of the original study $\\theta_{orig}$ and it is statistically significant i.e., $p_{\\theta_{rep}} \\leq \\alpha$. Similarly we can count the number of replication with the *same sign* as the original study.\n\n:::{.pros}\n- Easy to understand, communicate and compute\n:::\n\n:::{.cons}\n- Did not consider the size of the effect\n- Depends on the power of $\\theta_{rep}$\n:::\n\nLet's simulate an exact replication:\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## original study\nn_orig <- 30\ntheta_orig <- theta_from_z(2, n_orig)\n\norig <- data.frame(\n  yi = theta_orig,\n  vi = 4/(n_orig*2)\n)\n\norig$sei <- sqrt(orig$vi)\norig <- summary_es(orig)\n\norig\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n         yi         vi       sei zi       pval      ci.lb    ci.ub\n1 0.5163978 0.06666667 0.2581989  2 0.04550026 0.01033725 1.022458\n```\n\n\n:::\n\n```{.r .cell-code}\n## replications\n\nR <- 10\nreps <- sim_studies(R = R, \n                    mu = theta_orig, \n                    tau2 = 0, \n                    n_orig)\n\nreps <- summary_es(reps)\n\nhead(reps)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  id        yi         vi       sei n0 n1        zi         pval       ci.lb\n1  1 0.6161224 0.07563193 0.2750126 30 30 2.2403425 0.0250686936  0.07710763\n2  2 0.5268250 0.05593417 0.2365041 30 30 2.2275515 0.0259104357  0.06328554\n3  3 0.1664346 0.07094800 0.2663607 30 30 0.6248468 0.5320715805 -0.35562269\n4  4 0.2749253 0.06959962 0.2638174 30 30 1.0421046 0.2973632062 -0.24214729\n5  5 0.8341384 0.05538511 0.2353404 30 30 3.5443906 0.0003935218  0.37287962\n6  6 0.5963507 0.05393098 0.2322304 30 30 2.5679266 0.0102308809  0.14118743\n      ci.ub\n1 1.1551372\n2 0.9903644\n3 0.6884919\n4 0.7919979\n5 1.2953971\n6 1.0515140\n```\n\n\n:::\n:::\n\n\n\n\n\nLet's compute the proportions of replication studies are statistically significant:\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmean(reps$pval <= 0.05)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.7\n```\n\n\n:::\n:::\n\n\n\n\n\nLet's compute the proportions of replication studies with the same sign as the original:\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmean(sign(orig$yi) == sign(reps$yi))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 1\n```\n\n\n:::\n:::\n\n\n\n\n\nWe could also perform some statistical tests. See @Bushman2009-zv and @Hedges1980-gd for vote-counting methods in meta-analysis.\n\nAn extreme example:\n\n\n\n\n\n\n\n\n\n\n\nLet's imagine an original experiment with $n_{orig} = 30$ and $\\hat \\theta_{orig} = 0.5$ that is statistically significant $p \\approx 0.045$. Now a direct replication (thus assuming $\\tau^2 = 0$) study with $n_{rep} = 350$ found $\\hat \\theta_{rep_1} = 0.15$, that is statistically significant $p\\approx 0.047$.\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](chapter4_files/figure-html/unnamed-chunk-12-1.png){width=672}\n:::\n:::\n\n\n\n\n\nThe most problematic aspect of using only the information from the sign of the effect or the p value is completely losing the information about the **size of the effect** and the **precision**.\n\n## Confidence/prediction interval methods\n\nThe main improvement of confidence/prediction interval methods is taking into account the size of the effect and the precision. The confidence interval represent the sampling uncertainty around the estimated value. It is interpreted as the percentage of confidence intervals under repetition of the same sampling procedure that contains the true value. <!-- TODO check better definition -->\n\nConfidence interval around an estimated effect $\\theta_r$ can be written as:\n\n$$\n95\\%\\;\\mbox{CI} = \\hat\\theta_r \\pm \\mbox{SE}_{r} t_{\\alpha} \n$$\n\nWhere $t_{\\alpha}$ is the critical value for the test statistics with a given $\\alpha$. Clearly, this version of the confidence interval is symmetric around the estimated value and the width is a function of the estimation precision.\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](chapter4_files/figure-html/unnamed-chunk-13-1.png){width=672}\n:::\n:::\n\n\n\n\n\n### Replication effect within the original CI\n\n$$\n\\theta_{orig} - \\Phi(\\alpha/2) \\sqrt{\\sigma^2_{orig}} < \\theta_{rep} < \\theta_{orig} + \\Phi(\\alpha/2) \\sqrt{\\sigma^2_{orig}}\n$$\n\n::{.pros}\n- Take into account the size of the effect and the precision of $\\theta_{orig}$\n:::\n:::{.cons}\n- The original study is assumed to be a reliable estimation\n- No extension for *many-to-one* designs\n- Low precise original studies lead to higher success rate\n:::\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](chapter4_files/figure-html/unnamed-chunk-14-1.png){width=672}\n:::\n:::\n\n\n\n\n\nOne potential problem of this method regards that low precise original studies are \"easier\" to replicate due to larger confidence intervals. \n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](chapter4_files/figure-html/unnamed-chunk-15-1.png){width=672}\n:::\n:::\n\n\n\n\n\n### Original study within replication CI\n\nThe same approach can be applied checking if the original effect size is contained within the replication confidence interval. Clearly these methods depends on the precision of studies. Formally:\n\n$$\n\\theta_{rep} - \\Phi(\\alpha/2) \\sqrt{\\sigma_{rep}^2} < \\theta_{orig} < \\theta_{rep} + \\Phi(\\alpha/2) \\sqrt{\\sigma_{rep}^2}\n$$\n\nThe method has the same pros and cons of the previous approach. One advantage is that usually replication studies are more precise (higher sample size) thus the parameter and the % CI is more reliable.\n\n## Prediction interval PI\n\nThere is still one missing information from the CI method that is considering the uncertainty only from the original or the replication study. The prediction interval PI is similar to the confidence interval but taking into account \n\n## Meta-analytic methods\n\n## Bayes factor\n\n## References\n",
    "supporting": [
      "chapter4_files/figure-html"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}