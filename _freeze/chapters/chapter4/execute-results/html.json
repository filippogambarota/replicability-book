{
  "hash": "08b41415ea7e51ab15d0a2e7d4cde9fc",
  "result": {
    "engine": "knitr",
    "markdown": "# Statistical methods for replication assessment {#sec-statmethodrep}\n\n\n\n\n\n\n\n\n\n## Suggested Readings {.unnumbered}\n\n- @Heyard2024-hv\n- @Hedges2019-ry\n\n## How to assess the replication outcome?\n\nIn earlier chapters, we introduced the concept of replication from both statistical and theoretical perspectives. We now focus on analyzing the outcomes of replication experiments. Just as there are many definitions of replication, there are also multiple ways of assessing the outcomes of replication studies.\n\nMuch of the discussion will focus on replication metrics, which is a burgeoning area of statistics. @Heyard2024-hv published an extensive systematic review, supported by an online catalog.  They also maintain an up-to-date database with more than 50 replication measures organized and classified according to a common scheme.\n\nThe database is available at the following link [http://rachelheyard.com/reproducibility_metrics/](http://rachelheyard.com/reproducibility_metrics/). Here, we have selected methods from the database, according to the following criteria:\n\n- we have included methods not only for answering the question *\"Has a study been replicated?\"*, but also methods which instead combine evidence and estimate the size of the effect \n- we have selected methods that are not linked to a specific research area or topic. (Some methods are specific for a given area of research or type of experiment such as replicating fMRI studies at the voxel level.)\n- we have included both bayesian and frequentist approaches\n- where methods are very similar, we have focused on the most recent or general version\n\nIn general, choosing a single \"measure of replicability\" is probably not the best approach Typically, large-scale replication projects [e.g., @Collaboration2015-ep; @Errington2021-fb; @The-Brazilian-Reproducibility-Initiative2025-iw]  evaluate results across different metrics.\n\n## Replication studies as meta-analysis\n\n@Hedges2019-ry point out that replication studies can be seen --- from a statistical point of view --- as meta-analyses. Typically, there is a single initial study and one or more attempts to replicate its initial findings. For the simple one-to-one replication design we have a meta-analysis with two studies while in the one-to-many design we have a meta-analysis with $R$ studies. When it comes to evaluating the results of replication studies, we can choose between several methods [see @Heyard2024-hv]. Some of these methods will explictly involve meta-analyses, which pool evidence from original and replication studies.  While we consider meta-analytic thinking to be crucial for understanding replication, not all methods are (strictly speaking) meta-analyses in the usual sense.\n\nSpecifically, the methodology of the initial and/or replication studies is important when it comes to evaluating a single result.  However, at the aggregate level,  we evaluate how a certain focal parameter (e.g., the difference between two groups or conditions) varies across replications with an eye toward determining whether  there is evidence --- whatever the criterion --- for a successful replication.  In this sense, whatever model happens to be used in the original study, we can (without loss of generality) fruitfully model the situation from the aggregative perspective.\n\n## Simulating data\n\nWhile real-world examples are important, simulating simplified examples (\"toy models\") is useful for understanding replication methods from a statistical point of view. Indeed, in general, simulating data is an important technique for teaching and understanding statistical methods [@Hardin2015-ss]. Moreover, in practice, Monte Carlo simulations are necessary for estimating statistical properties (e.g., statistical power or type-1 error rate) of complex models.\n\nFor our simulated examples, we adpot the following methodological assumptions:\n\n- primary studies always compare two independent groups on a certain response variable\n- within a study, the two groups are assumed to come from two normal distributions with unit variance. For one group $y_0$ the mean is zero and for the other group $y_1$ the mean is the value representing the effect size $\\theta_r$ for that specific study.\n- the sample size can vary between the two groups\n\nFor example we can simulate a single study:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntheta <- 0.3\nn0 <- 30\nn1 <- 30\n\ny0 <- rnorm(n0, 0, 1)\ny1 <- rnorm(n1, theta, 1)\n\nmean(y1) - mean(y0) # this is the effect size\n## [1] 0.02489806\nvar(y1)/n1 + var(y0)/n0 # this is the sampling variability\n## [1] 0.09229475\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndd <- data.frame(y = c(y0, y1),\n                 x = factor(rep(0:1, c(n0, n1))))\n\nggplot(dd, aes(x = x, y = y)) +\n    geom_boxplot(aes(fill = x))\n```\n\n::: {.cell-output-display}\n![](chapter4_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\n\n\n\nNow we can simply iterate the process for $R$ studies to create a series of replications with a common true parameter $\\mu_{\\theta}$. We can also generate a set of different sample sizes by sampling from a probability distribution (e.g., Poisson). Then, if we want to include variability in the true effects (as would be expected in the case of *extensions* of an experinent), we can sample $R$ effects from, say, a normal distribution with variance $\\tau^2$.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nR <- 10\nmu <- 0.3 # average true effect\ntau2 <- 0.1 # heterogeneity\ndeltai <- rnorm(R, mu, sqrt(tau2)) # true effects for R studies\n\nyi <- vi <- rep(NA, R)\nn0 <- n1 <- 10 + rpois(R, 40 - 10)\n\nfor(i in 1:R){\n    y0 <- rnorm(n0[i], 0, 1)\n    y1 <- rnorm(n1[i], deltai[i], 1)\n    yi[i] <- mean(y1) - mean(y0)\n    vi[i] <- var(y1)/n1[i] + var(y0)/n0[i]\n}\n\nsim <- data.frame(id = 1:R, yi, vi, n0, n1)\nsim\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   id          yi         vi n0 n1\n1   1  0.85048016 0.04778103 36 36\n2   2  0.30921378 0.04848062 35 35\n3   3 -0.08156708 0.06168057 39 39\n4   4 -0.29994880 0.04077748 44 44\n5   5  0.32756262 0.05728339 33 33\n6   6  0.79298676 0.04796314 39 39\n7   7  0.67216941 0.04605116 34 34\n8   8 -0.37006253 0.04260985 40 40\n9   9  1.06199067 0.04762220 45 45\n10 10  0.82797567 0.06092187 31 31\n```\n\n\n:::\n:::\n\n\n\n\nHere, we put everything into a single function, which can be used to simulate different scenarios.\n\n\n\n\n```r\nsim_study <- function(theta, n0, n1 = NULL, id = NULL){\n    if(is.null(n1)) n1 <- n0\n    y0 <- rnorm(n0, 0, 1)\n    y1 <- rnorm(n1, theta, 1)\n    sim <- data.frame(\n        yi = mean(y1) - mean(y0),\n        vi = var(y1)/n1 + var(y0)/n0\n    )\n    sim$sei <- sqrt(sim$vi)\n    sim$n0 <- n0\n    sim$n1 <- n1\n    if(!is.null(id)){\n        sim <- cbind(id = id, sim)\n    }\n    class(sim) <- c(\"rep3data\", class(sim))\n    return(sim)\n}\nsim_studies <- function(R, mu = 0, tau2 = 0, n0, n1 = NULL){\n    # check input parameters consistency\n    if(length(mu) == 1) mu <- rep(mu, R)\n    if(length(n0) == 1) n0 <- rep(n0, R)\n    if(length(n1) == 1) n1 <- rep(n1, R)\n    if(is.null(n1)) n1 <- n0\n\n    deltai <- rnorm(R, 0, sqrt(tau2))\n    mu <- mu + deltai\n\n    sim <- mapply(sim_study, mu, n0, n1, SIMPLIFY = FALSE)\n    sim <- do.call(rbind, sim)\n    sim <- cbind(id = 1:R, sim)\n    class(sim) <- c(\"rep3data\", class(sim))\n    return(sim)\n}\n```\n\n::: {.cell}\n\n```{.r .cell-code}\nsim_studies(R = 10, mu = 0.5, tau2 = 0, n0 = 30)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   id        yi         vi       sei n0 n1\n1   1 0.7637734 0.05646744 0.2376288 30 30\n2   2 0.4986094 0.06305786 0.2511132 30 30\n3   3 0.2429051 0.06739538 0.2596062 30 30\n4   4 0.4922899 0.07759289 0.2785550 30 30\n5   5 0.3290003 0.05849493 0.2418573 30 30\n6   6 0.6820730 0.06914787 0.2629598 30 30\n7   7 0.2040946 0.05499395 0.2345079 30 30\n8   8 0.5796687 0.05165193 0.2272706 30 30\n9   9 0.1460033 0.05915946 0.2432272 30 30\n10 10 0.6659032 0.08595197 0.2931757 30 30\n```\n\n\n:::\n:::\n\n\n\n\n## Methods\n\nThe database available at the following link [http://rachelheyard.com/reproducibility_metrics/](http://rachelheyard.com/reproducibility_metrics/) can be consulted for a more complete overview. Among all the available methods, we have selected the following metrics^[The name of the method corresponds to the first column of the online table], each of which will be illustrated below, with suitable examples.\n\n- Prediction interval: is the replication effect inside the original 95% prediction interval?\n- Proportion of population effects agreeing in direction with the original (vote counting)\n- Confidence interval (CI): original effect in replication 95% CI\n- Confidence interval (CI): replication effect in original 95% CI\n- Prediction interval (CI): replication effect in original 95% PI\n- Replication Bayes Factor (BF)\n- Small Telescopes\n- Consistency of original with replications, $P_{\\mbox{orig}}$\n- Proportion of population effects agreeing in direction with the original, $\\hat{P}_{>0}$\n- Meta-analysis and $Q$ statistics\n\n## Vote Counting based on significance or direction\n\nThe simplest method is called **vote counting** [@Valentine2011-yq; @Hedges1980-gd]. A replication attempt $\\theta_{r}$ is considered to be successful if the result is is statistically significant i.e., $p_{\\theta_{r}} \\leq \\alpha$ and has the same direction as the original study $\\theta_{0}$. Similarly, we can count the number of replications with the *same sign* as the original study.  This method has the following <span style=\"color: green;\">pros</span> and <span style=\"color: red;\">cons</span>.\n\n:::{.pros}\n- Easy to understand, communicate and compute\n:::\n\n:::{.cons}\n- Does not consider the size of the effect\n- Depends on the power of $\\theta_{rep}$\n:::\n\nHere is an example which illustrates the vote counting method  First, we simulate a replication:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## original study\nn_orig <- 30\ntheta_orig <- theta_from_z(2, n_orig)\n\norig <- data.frame(\n  yi = theta_orig,\n  vi = 4/(n_orig*2)\n)\n\norig$sei <- sqrt(orig$vi)\norig <- summary_es(orig)\n\norig\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n         yi         vi       sei zi       pval      ci.lb    ci.ub\n1 0.5163978 0.06666667 0.2581989  2 0.04550026 0.01033725 1.022458\n```\n\n\n:::\n\n```{.r .cell-code}\n## replications\n\nR <- 10\nreps <- sim_studies(R = R, \n                    mu = theta_orig, \n                    tau2 = 0, \n                    n_orig)\n\nreps <- summary_es(reps)\n\nhead(reps)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  id         yi         vi       sei n0 n1         zi         pval       ci.lb\n1  1  0.5417437 0.06415704 0.2532924 30 30  2.1388074 0.0324512712  0.04529967\n2  2  0.8204319 0.05502213 0.2345680 30 30  3.4976299 0.0004694121  0.36068715\n3  3 -0.1758015 0.08193435 0.2862418 30 30 -0.6141713 0.5391021068 -0.73682505\n4  4  0.6274413 0.05432717 0.2330819 30 30  2.6919351 0.0071038776  0.17060921\n5  5  0.4938945 0.06952231 0.2636708 30 30  1.8731478 0.0610479723 -0.02289088\n6  6  0.2908247 0.08020594 0.2832065 30 30  1.0268996 0.3044677335 -0.26424992\n      ci.ub\n1 1.0381876\n2 1.2801767\n3 0.3852221\n4 1.0842734\n5 1.0106798\n6 0.8458993\n```\n\n\n:::\n:::\n\n\n\n\nThen, we compute the proportions of replication studies that are statistically significant:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmean(reps$pval <= 0.05)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.5\n```\n\n\n:::\n:::\n\n\n\n\nAnd, the proportions of replication studies with the same sign as the original:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmean(sign(orig$yi) == sign(reps$yi))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.9\n```\n\n\n:::\n:::\n\n\n\n\nAt this stage, we could also perform some statistical tests in order to complement the vote counting approach. See @Bushman2009-zv and @Hedges1980-gd for vote-counting methods in meta-analysis.\n\nHere is an extreme example:\n\n\n\n\n\n\n\n\n\nImagine an original experiment with $n_{0} = 30$ and $\\hat \\theta_{0} = 0.5$ that is statistically significant $p \\approx 0.045$. Now, consider a direct replication study (thus assume $\\tau^2 = 0$) with $n_{1} = 350$ which found $\\hat \\theta_{1} = 0.15$, which is statistically significant at $p\\approx 0.047$.\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](chapter4_files/figure-html/unnamed-chunk-11-1.png){width=672}\n:::\n:::\n\n\n\n\nThe most problematic aspect of using only the information about the sign of the effect (or the p value) is that *we lose information* about the **size of the effect** and the **precision**.\n\n## Confidence/prediction interval methods\n\nAn advantage of confidence/prediction interval methods is that they take into account the size of the effect and the precision. The confidence interval represents the sampling uncertainty around the estimated value. It is interpreted as the proportion of confidence intervals (under hypothetical repetition of the same sampling procedure) which contain the true value.\n\nThe confidence interval around an estimated effect $\\theta_r$ can be written as follows:\n\n$$\n95\\%\\;\\mbox{CI} = \\hat\\theta_r \\pm t_{\\alpha}\\sqrt{\\sigma^2_{r}}\n$$\n\nHere, $t_{\\alpha}$ is the critical test statistic for significance level $\\alpha$. This version of the confidence interval is symmetric around the estimated value and its width is a function of the estimation precision.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nalpha <- 0.05\nn <- c(10, 50, 100, 500)\ntheta <- rep(0.3, length(n))\nse <- sqrt(1/n + 1/n)\ntc <- abs(qnorm(alpha/2))\nlb <- theta - se * tc\nub <- theta + se * tc\n\nplot(n, theta, ylim = c(min(lb), max(ub)), type = \"b\", pch = 19,\n     xlab = \"Sample Size\",\n     ylab = latex2exp::TeX(\"$\\\\theta\\\\;_{r}$\"))\nlines(n, ub, lty = \"dashed\")\nlines(n, lb, lty = \"dashed\")\n```\n\n::: {.cell-output-display}\n![](chapter4_files/figure-html/unnamed-chunk-12-1.png){width=672}\n:::\n:::\n\n\n\n\n### Replication effect within the original CI\n\nWe can check whether a replication effect falls within the original study's confidence interval.  That is, whether the following inequalities are satisfied.\n\n$$\n\\theta_{0} - t_{\\alpha} \\sqrt{\\sigma^2_{0}} < \\theta_{r} < \\theta_{0} + t_{\\alpha} \\sqrt{\\sigma^2_{0}}\n$$\n\nThe <span style=\"color: green;\">pros</span> and <span style=\"color: red;\">cons</span> of this approach include:\n\n:::{.pros}\n- Takes into account the size of the effect and the precision of $\\theta_{0}$\n:::\n\n:::{.cons}\n- Assumes the original study provides a reliable estimate\n- Has no extension for *many-to-one* designs\n- Original studies with limited precision will tend to lead to higher success rates (i.e., original studies with wide intervals will be easier to replicate)\n:::\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nz_orig <- 2\nn_orig <- 30\ntheta_orig <- theta_from_z(z_orig, n_orig)\nn_rep <- 350\ntheta_rep <- 0.15\nse_orig <- sqrt(4 / (2 * n_orig))\nci_orig <- theta_orig + qnorm(c(0.025, 0.975)) * se_orig\ncurve(dnorm(x, theta_orig, se_orig), \n      -1, 2, \n      ylab = \"Density\", \n      xlab = latex2exp::TeX(\"$\\\\theta$\"))\nabline(v = ci_orig, lty = \"dashed\")\npoints(theta_orig, 0, pch = 19, cex = 2)\npoints(theta_rep, 0, pch = 19, cex = 2, col = \"firebrick\")\nlegend(\"topleft\", \n       legend = c(\"Original\", \"Replication\"), \n       fill = c(\"black\", \"firebrick\"))\n```\n\n::: {.cell-output-display}\n![](chapter4_files/figure-html/unnamed-chunk-13-1.png){width=672}\n:::\n:::\n\n\n\n\nHere is an illustration of the last <span style=\"color: red;\">con</span> of this method (i.e., that original studies with wide intervals will be easier to replicate).\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](chapter4_files/figure-html/unnamed-chunk-14-1.png){width=672}\n:::\n:::\n\n\n\n\n### Original study within replication CI\n\nA similar approach is checking whether the original effect size is contained within the replication confidence interval, i.e., whether the following inequalities are satisfied.\n\n$$\n\\theta_{r} - t_{\\alpha} \\sqrt{\\sigma_{r}^2} < \\theta_{0} < \\theta_{r} + t_{\\alpha} \\sqrt{\\sigma_{r}^2}\n$$\n\nThis method has the same <span style=\"color: green;\">pros</span> and <span style=\"color: red;\">cons</span> as the previous approach. One advantage of this method is that replication studies are usually more precise (because they tend to have higher sample sizes).  As a result, the parameter and the %CI tends to be more reliable.\n\n## Prediction interval PI\n\nThe CI methods described so far only take into account uncertainty from either the original or the replication study. The prediction interval PI allows us to account for uncertainty regarding a future observation (in this case, a future study).\n\nIf we compute the difference between the original and the replication study, the sampling distribution of the difference represents the range of values that are expected *in the absence of other sources of variability*. In other words, assuming that $\\tau^2 = 0$.\n\nIf the original and replication studies come from the same population, the sampling distribution of the difference is centered on zero with a certain standard error $\\theta_{0} - \\theta_{r} \\sim \\mathcal{N}\\left(0, \\sqrt{\\sigma^2_{\\hat \\theta_{0} - \\hat \\theta_{r}}} \\right)$ (the subscript $0$ is to indicate that the effect is expected to be sampled from the same population as $\\theta_{0}$)\n\n$$\n\\hat \\theta_{0} \\pm t_{\\alpha} \\sqrt{\\sigma^2_{\\theta_{0} - \\theta_{r}}}\n$$\n\nThe new standard error is the sum (assuming that the two studies are independent) of the two standard errors. Thus, we are taking into account the estimation uncertainty of the original and replication studies.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(2025)\n\no1 <- rnorm(50, 0.5, 1) # group 1\no2 <- rnorm(50, 0, 1) # group 2\nod <- mean(o1) - mean(o2) # effect size\nse_o <- sqrt(var(o1)/50 + var(o2)/50) # standard error of the difference\n\nn_r <- 100 # sample size replication\n\nse_o_r <- sqrt(se_o^2 + (var(o1)/100 + var(o2)/100))\n\nod + qnorm(c(0.025, 0.975)) * se_o_r\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.325520 1.298378\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\npar(mar = c(4, 4, 0.1, 0.1))  \ncurve(dnorm(x, od, se_o_r), od - se_o_r*4, od + se_o_r*4, lwd = 2, xlab = latex2exp::TeX(\"$\\\\theta$\"), ylab = \"Density\")\nabline(v = od + qnorm(c(0.025, 0.975)) * se_o_r, lty = \"dashed\")\n```\n\n::: {.cell-output-display}\n![](chapter4_files/figure-html/unnamed-chunk-16-1.png){width=70%}\n:::\n:::\n\n\n\n\nThe <span style=\"color: green;\">pros</span> and <span style=\"color: red;\">cons</span> of this method include:\n\n:::{.pros}\n- Takes into account uncertainty in both studies\n- Allows us to plan a replication using the standard deviation of the original study and the expected sample size of the replication study\n:::\n\n:::{.cons}\n- Low precision in original studies leads to wider PI. For a replication study, it is difficult to fall outside the PI\n- Mainly useful for *one-to-one* replication designs\n:::\n\n## Mathur & VanderWeele [-@Mathur2020-nw] $p_{orig}$\n\nMathur & VanderWeele [-@Mathur2020-nw] proposed a new method based on the prediction interval to calculate a p value $p_{orig}$ representing the probability that $\\theta_{0}$ is consistent with the replications. This method is suited for *many-to-one* replication designs. Formally:\n\n$$\np_{orig} = 2 \\left[ 1 - \\Phi \\left( \\frac{|\\hat \\theta_{0} - \\hat \\mu_{\\theta_{R}}|}{\\sqrt{\\hat \\tau^2 + \\sigma^2_{\\theta_{0}} + \\hat{SE}^2_{\\hat \\mu_{\\theta_{R}}}}} \\right) \\right]\n$$\n\n- $\\hat \\mu_{\\theta_{R}}$ is the pooled (i.e., meta-analytic) estimation of the $k$ replications\n- $\\tau^2$ is the variance of effects across replications (i.e., heterogeneity)\n\n$p_{orig}$ is interpreted as the probability that $\\theta_{0}$ is at least as extreme as what is observed. A low $p_{orig}$ suggests that the original study is inconsistent with replications.  This method as three key <span style=\"color: green;\">pros</span>.\n\n:::{.pros}\n- Suited for *many-to-one* designs\n- Takes into account all sources of uncertainty\n- Provides a p-value (to gauge consistency of the original study with replications)\n:::\n\nThe code for this approach is implemented in the `Replicate` and `MetaUtility` R packages:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntau2 <- 0.05\ntheta_rep <- 0.2\ntheta_orig <- 0.7\n\nn_orig <- 30\nn_rep <- 100\nk <- 20\n\nreplications <- sim_studies(k, theta_rep, tau2, n_rep, n_rep)\noriginal <- sim_studies(1, theta_orig, 0, n_orig, n_orig)\n\nfit_rep <- metafor::rma(yi, vi, data = replications) # random-effects meta-analysis\n\nReplicate::p_orig(original$yi, original$vi, fit_rep$b[[1]], fit_rep$tau2, fit_rep$se^2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.06692786\n```\n\n\n:::\n:::\n\n\n\n\n### Simulation\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# standard errors assuming same n and variance 1\nse_orig <- sqrt(4/(n_orig * 2))\nse_rep <- sqrt(4/(n_rep * 2))\nse_theta_rep <- sqrt(1/((1/(se_rep^2 + tau2)) * k)) # standard error of the random-effects estimate\n\nsep <- sqrt(tau2 + se_orig^2 + se_theta_rep^2) # z of p-orig denominator\n\ncurve(dnorm(x, theta_rep, sep), theta_rep - 4*sep, theta_rep + 4*sep, ylab = \"Density\", xlab = latex2exp::TeX(\"\\\\theta\"))\npoints(theta_orig, 0.02, pch = 19, cex = 2)\nabline(v = qnorm(c(0.025, 0.975), theta_rep, sep), lty = \"dashed\", col = \"firebrick\")\n```\n\n::: {.cell-output-display}\n![](chapter4_files/figure-html/unnamed-chunk-18-1.png){width=672}\n:::\n:::\n\n\n\n\n\n\n### Mathur & VanderWeele [-@Mathur2020-nw] $\\hat P_{> 0}$\n\nAnother related metric is the $\\hat P_{> 0}$, which represents the proportion of replications having the same direction as the original effect. Note: before computing the proportions, they adjust the estimated $\\theta_{r}$ ($r = 1, \\dots, R$) with a shrinkage factor (to account for regression to the mean):\n\n$$\n\\tilde{\\theta}_{r} = (\\theta_{r} - \\mu_{\\theta_{r}}) \\sqrt{\\frac{\\hat \\tau^2}{\\hat \\tau^2 + \\sigma^2_{r}}}\n$$\n\nHere $\\sigma^2_r$ is the sampling variance for the replication effect size $r$. This method is  similar to vote counting; but, we are adjusting the effects with a shrinkage factor based on $\\tau^2$.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\n# compute calibrated estimation for the replications\n# use restricted maximum likelihood to estimate tau2 under the hood\ntheta_sh <- MetaUtility::calib_ests(replications$yi, replications$sei, method = \"REML\")\nmean(theta_sh > 0)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.85\n```\n\n\n:::\n:::\n\n\n\n\nThe authors suggest a bootstrapping approach for inference on $\\hat P_{> 0}$\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nnboot <- 1e3\ntheta_boot <- matrix(0, nrow = nboot, ncol = k)\n\nfor(i in 1:nboot){\n  idx <- sample(1:nrow(replications), nrow(replications), replace = TRUE)\n  replications_boot <- replications[idx, ]\n  theta_cal <- MetaUtility::calib_ests(replications_boot$yi, \n                                       replications_boot$sei, \n                                       method = \"REML\")\n  theta_boot[i, ] <- theta_cal\n}\n\n# calculate\np_greater_boot <- apply(theta_boot, 1, function(x) mean(x > 0))\n```\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![](chapter4_files/figure-html/unnamed-chunk-22-1.png){width=672}\n:::\n:::\n\n\n\n\n### Mathur & VanderWeele [-@Mathur2020-nw] $\\hat P_{\\gtrless q*}$\n\nInstead of using 0 as threshold, we can use a practically meaningful effect size, considered to be low but different from 0. $\\hat P_{\\gtrless q*}$ is the proportion of (calibrated) replications greater or lower than the $q*$ value. This framework is similar to equivalence and minimum effect size testing [@Lakens2018-ri].\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nq <- 0.2 # minimum non zero effect\n\nfit <- metafor::rma(yi, vi, data = replications)\n\n# see ?MetaUtility::prop_stronger\nMetaUtility::prop_stronger(q = q,\n                           M = fit$b[[1]],\n                           t2 = fit$tau2,\n                           tail = \"above\",\n                           estimate.method = \"calibrated\",\n                           ci.method = \"calibrated\",\n                           dat = replications,\n                           yi.name = \"yi\",\n                           vi.name = \"vi\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   est        se lo  hi   bt.mn shapiro.pval\n1 0.35 0.1756669  0 0.6 0.35375    0.8137044\n```\n\n\n:::\n:::\n\n\n\n\n## Meta-analytic methods\n\nAnother approach is to combine the original and replication results using a meta-analysis model. This is applicable to both *one-to-one* and *many-to-one* designs. In this case, we can test whether the pooled estimate is different from 0 (or another practically relevant value).  The meta-analytic approach has the following <span style=\"color: green;\">pros</span> and <span style=\"color: red;\">cons</span>.\n\n:::{.pros}\n- Uses all the available information, especially when fitting a random-effects model\n- Takes into account the precision of individual studies (e.g., via inverse-variance weighting)\n:::\n\n:::{.cons}\n- Does not take into account publication bias\n- For *one-to-one* designs, only a fixed-effects model can be used\n:::\n\n### Fixed-effects Model\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# fixed-effects\nfit_fixed <- rma(yi, vi, method = \"FE\")\nsummary(fit_fixed)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nFixed-Effects Model (k = 20)\n\n  logLik  deviance       AIC       BIC      AICc   \n 20.7415   -0.0000  -39.4829  -38.4872  -39.2607   \n\nI^2 (total heterogeneity / total variability):   0.00%\nH^2 (total variability / sampling variability):  0.00\n\nTest for Heterogeneity:\nQ(df = 19) = 0.0000, p-val = 1.0000\n\nModel Results:\n\nestimate      se    zval    pval   ci.lb   ci.ub      \n  0.2000  0.0316  6.3246  <.0001  0.1380  0.2620  *** \n\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\n\n\n\n### Random-Effects model\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# fixed-effects\nfit_random <- rma(yi, vi, method = \"REML\")\nsummary(fit_random)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nRandom-Effects Model (k = 20; tau^2 estimator: REML)\n\n  logLik  deviance       AIC       BIC      AICc   \n 19.7044  -39.4088  -35.4088  -33.5199  -34.6588   \n\ntau^2 (estimated amount of total heterogeneity): 0 (SE = 0.0065)\ntau (square root of estimated tau^2 value):      0\nI^2 (total heterogeneity / total variability):   0.00%\nH^2 (total variability / sampling variability):  1.00\n\nTest for Heterogeneity:\nQ(df = 19) = 0.0000, p-val = 1.0000\n\nModel Results:\n\nestimate      se    zval    pval   ci.lb   ci.ub      \n  0.2000  0.0316  6.3246  <.0001  0.1380  0.2620  *** \n\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\n\n\n\n### Pooling replications\n\nThe previous approach can also incorporate the combination/pooling of replications into a single effect (then comparing the original study and the combined/pooled replication study).\n\nThis is similar to using the CI or PI approaches; but, the replication effect is likely to be more precise, since we are pooling multiple replication studies.\n\n### The Q Statistic\n\nAn interesting proposal involves the use of the Q statistic [@Hedges2019-pr; @Hedges2019-ar; @Hedges2021-of; @Schauer2022-mj; @Schauer2021-ja; @Schauer2020-tw; @Hedges2019-ry], which is commonly applied in meta-analysis to assess the presence of heterogeneity. Formally:\n\n$$\nQ = \\sum_{r = 0}^{R} \\frac{(\\theta_r - \\bar \\theta_R)^2}{\\sigma^2_r}\n$$\n\nwhere $\\bar \\theta_R$ is the inverse-variance weighted average (e.g., from a fixed-effect model). The $Q$ statistic is a weighted sum of squares. Under the null hypothesis where all studies are equal $\\theta_0 = \\theta_1, ... = \\theta_R$, the $Q$ statistic has a $\\chi^2$ distribution with $R - 1$ degrees of freedom. Under the alternative hypothesis the distribution is a non-central $\\chi^2$ with non centrality parameter $\\lambda$. The expected value of $Q$ is $E(Q) = \\nu + \\lambda$, where $\\nu$ are the degrees of freedom.\n\nHedges & Schauer proposed the use of $Q$ to evaluate the consistency of a series of replications:\n\n- In case of a **replication**, $\\lambda = 0$ because $\\theta_0 = \\theta_1, ... = \\theta_R$.\n- In case of an **extension**, $\\lambda < \\lambda_0$ where $\\lambda_0$ is the maximum value considered equivalent to zero. In other words, $\\lambda_0$ is the threshold above which the variability is too high to be considered negligible for a replication [in the sense of @Machery2020-sv].\n\nThis approach is testing the consistency (i.e., homogeneity) of replications. A successful replication should minimize the heterogeneity and the presence of a significant $Q$ statistic would suggest a failed replication attempt. @Hedges2019-pr and @Mathur2019-vh discuss this approach in detail.  Their discussion addresses various issues surrounding the proper use of this method, including:\n\n- variations of the apporoach, which cover different replication setups (burden of proof on replicating vs non-replicating, many-to-one and one-to-one, etc.)\n- the choice (and interpretation) of the $\\lambda_0$ parameter\n- evaluating the power and statistical properties under different replication scenarios\n\nWhen evaluating a replication, we can use the `Qrep()` function which calculates the p-value based on the Q sampling distribution.\n\n\n\n\n```r\nQrep <- function(yi, vi, lambda0 = 0, alpha = 0.05){\n  fit <- metafor::rma(yi, vi)\n  k <- fit$k\n  Q <- fit$QE\n  df <- k - 1\n  Qp <- pchisq(Q, df = df, ncp = lambda0, lower.tail = FALSE)\n  pval <- ifelse(Qp < 0.001, \"p < 0.001\", sprintf(\"p = %.3f\", Qp))\n  lambda <- ifelse((Q - df) < 0, 0, (Q - df))\n  res <- list(Q = Q, lambda = lambda, pval = Qp, df = df, k = k, alpha = alpha, lambda0 = lambda0)\n  H0 <- ifelse(lambda0 != 0, paste(\"H0: lambda <\", lambda0), \"H0: lambda = 0\")\n  title <- ifelse(lambda0 != 0, \"Q test for Approximate Replication\", \"Q test for Exact Replication\")\n  cli::cli_rule()\n  cat(cli::col_blue(cli::style_bold(title)), \"\\n\\n\")\n  cat(sprintf(\"Q = %.3f (df = %s), lambda = %.3f, %s\", res$Q, res$df, lambda, pval), \"\\n\")\n  cat(H0, \"\\n\")\n  cli::cli_rule()\n  class(res) <- \"Qrep\"\n  invisible(res)\n}\n```\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\nQ test for Exact Replication \n\nQ = 509.539 (df = 99), lambda = 410.539, p < 0.001 \nH0: lambda = 0 \n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nQres <- Qrep(dat$yi, dat$vi)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nQ test for Exact Replication \n\nQ = 509.539 (df = 99), lambda = 410.539, p < 0.001 \nH0: lambda = 0 \n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nplot.Qrep(Qres)\n```\n\n::: {.cell-output-display}\n![](chapter4_files/figure-html/unnamed-chunk-29-1.png){width=672}\n:::\n:::\n\n\n\n\n#### Q Statistic for an extension study\n\nIn the case of an extension of an intial experiment, we will need to set $\\lambda_0$ to a meaningful value. The critical $Q$ is no longer evaluated with a central $\\chi^2$ but is now a non-central $\\chi^2$ with non-centrality parameter $\\lambda_0$.\n\n@Hedges2019-ry provide different strategies for choosing $\\lambda_0$. They found that (under some mild assumptions), $\\lambda = (R - 1) \\frac{\\tau^2}{\\tilde{v}}$ seems to be a suitable choice.\n\nSimilarly to a meta-analysis, in a many-to-one replication design we have mainly two sources of variability: the true heterogeneity $\\tau^2$ and the within-studies veriability $\\sigma_i^2$. The sum of these two quantities is the total variability and the proportion of total variability due to heterogeneity is a relative index of (in)consistency among effects. This proportion is called $I^2$ [@Higgins2002-fh] in the meta-analysis literature is calculated as: \n$$\nI^2 = 100\\% \\times \\frac{\\hat{\\tau}^2}{\\hat{\\tau}^2 + \\tilde{v}}\n$$\n\nWhere $\\tilde{v}$ can be considered as a typical within-study variance value.\n\n$$\n\\tilde{v} = \\frac{(k-1) \\sum w_i}{(\\sum w_i)^2 - \\sum w_i^2},\n$$\n\nHaving introduced the $I^2$ statistic, we can derive a $\\lambda_0$ based on $I^2$. @Schmidt2014-kw proposed that when $\\tilde{v}$ is at least 75% of the total variance $\\tilde{v} + \\tau^2$,  $\\tau^2$ can be considered to be negligible. This corresponds to a $I^2 = 25%$ and a ratio $\\frac{\\tau^2}{\\tilde{v}} = 1/3$.  Hence,  $\\lambda_0 = \\frac{(R - 1)}{3}$ can be considered to be a negligible amount of heterogeneity.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nk <- 100\ndat <- sim_studies(k, 0.5, 0, 50, 50)\nQrep(dat$yi, dat$vi, lambda0 = (k - 1)/3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nQ test for Approximate Replication \n\nQ = 93.881 (df = 99), lambda = 0.000, p = 0.989 \nH0: lambda < 33 \n```\n\n\n:::\n:::\n\n\n\n\n## Small Telescopes [@Simonsohn2015-kg]\n\nThe idea is simple but quite powerful and insightful. An original study $\\theta_{0}$ is conducted estimating a certain effect size with a certain sample size. This study has a certain statistical power level assuming a plausible effect size. Then we estimate the effect size that would have a certain low power level (e.g., 33%, $d_{33\\%}$). Then we run the replication study $\\theta_r$ testing if the replication effect size is lower (one-sided test) than $d_{33\\%}$. If upper bound of the confidence interval of the replication study is lower this means that the original study is probably seriously underpowered suggesting evidence of non-replication.\n\nAs a practical example, let's assume that an original study found an effect of $\\theta_{0} = 0.7$ on a two-sample design with $n = 20$ per group. We define a threshold as the effect size that is associated with a certain low statistical power level e.g., $33\\%$ given the sample size i.e. $d_{33\\%} = 0.5$. The replication study found an effect of $\\theta_{r} = 0.2$ with $n = 100$ subjects.\n\nIf the $\\theta_{r}$ is statistically significantly lower than $d_{33\\%} = 0.5$ we conclude that the effect is probably so small that could not have been detected by the original study.\n    \nWe can use the custom `small_telescope()` function on simulated data:\n\n\n\n\n```r\nsmall_telescope <- function(or_d,\n                            or_se,\n                            rep_d,\n                            rep_se,\n                            small,\n                            ci = 0.95){\n  # quantile for the ci\n  qs <- c((1 - ci)/2, 1 - (1 - ci)/2)\n\n  # original confidence interval\n  or_ci <- or_d + qnorm(qs) * or_se\n\n  # replication confidence interval\n  rep_ci <- rep_d + qnorm(qs) * rep_se\n\n  # small power\n  is_replicated <- rep_ci[2] > small\n\n  msg_original <- sprintf(\"Original Study: d = %.3f %s CI = [%.3f, %.3f]\",\n                          or_d, ci, or_ci[1], or_ci[2])\n\n  msg_replicated <- sprintf(\"Replication Study: d = %.3f %s CI = [%.3f, %.3f]\",\n                            rep_d, ci, rep_ci[1], rep_ci[2])\n\n\n  if(is_replicated){\n    msg_res <- sprintf(\"The replicated effect is not smaller than the small effect (%.3f), (probably) replication!\", small)\n    msg_res <- cli::col_green(msg_res)\n  }else{\n    msg_res <- sprintf(\"The replicated effect is smaller than the small effect (%.3f), no replication!\", small)\n    msg_res <- cli::col_red(msg_res)\n  }\n\n  out <- data.frame(id = c(\"original\", \"replication\"),\n                    d = c(or_d, rep_d),\n                    lower = c(or_ci[1], rep_ci[1]),\n                    upper = c(or_ci[2], rep_ci[2]),\n                    small = small\n  )\n\n  # nice message\n  cat(\n    msg_original,\n    msg_replicated,\n    cli::rule(),\n    msg_res,\n    sep = \"\\n\"\n  )\n\n  invisible(out)\n\n}\n```\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nset.seed(2025)\n\nd <- 0.2 # real effect\n\n# original study\nor_n <- 20\nor_d <- 0.7\nor_se <- sqrt(1/20 + 1/20)\nd_small <- pwr::pwr.t.test(or_n, power = 0.33)$d\n\n# replication\nrep_n <- 100 # sample size of replication study\ng0 <- rnorm(rep_n, 0, 1)\ng1 <- rnorm(rep_n, d, 1)\n\nrep_d <- mean(g1) - mean(g0)\nrep_se <- sqrt(var(g1)/rep_n + var(g0)/rep_n)\n```\n:::\n\n\n\n\nHere we are using the `pwr::pwr.t.test()` to compute the effect size $\\theta_{small}$ (in code `d`) associated with 33% power.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsmall_telescope(or_d, or_se, rep_d, rep_se, d_small, ci = 0.95)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nOriginal Study: d = 0.700 0.95 CI = [0.080, 1.320]\nReplication Study: d = 0.214 0.95 CI = [-0.061, 0.490]\n────────────────────────────────────────────────────────────────────────────────\nThe replicated effect is smaller than the small effect (0.493), no replication!\n```\n\n\n:::\n:::\n\n\n\n\nAnd a plot:\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](chapter4_files/figure-html/unnamed-chunk-34-1.png){width=672}\n:::\n:::\n\n\n\n\n## Replication Bayes factor\n\n@Verhagen2014-tx proposed a method to estimate the evidence of a *replication* study. The core topics to understand the method are:\n\n- Bayesian hypothesis testing using the Bayes Factor [see, @Rouder2009-jh]\n- Bayes Factor using the Savage-Dickey density ratio [SDR, @Wagenmakers2010-fj]\n\n### Bayesian inference\n\nBayesian inference is a statistical procedure where **prior beliefs** about a phenomenon are combined, using Bayes's theorem, with **evidence from data** to obtain  **posterior beliefs**. \n\nIf the researcher can express their prior beliefs in probabilistic terms, then evidence from the experiment is taken into account (via Bayesian conditionalization), thus increasing or decreasing the plausibility of prior beliefs.\n\nLet's consider a simple example. We need to evaluate the fairness of a coin. The parameter is the probability $\\pi$ of success (i.e., heads). We have our prior belief about the coin (e.g., fair but with some uncertainty). We toss the coin $k$ times, and we observe $x$ heads. How should we update our prior beliefs?  We apply Bayes's Theorem, which, in this case, is:\n\n$$\np(\\pi|D) = \\frac{p(D|\\pi) \\; p(\\pi)}{p(D)}\n$$\n\nHere, $\\pi$ is our parameter and $D$ is our data. $p(\\pi|D)$ is the posterior distribution, which is the normalized product of $p(D|\\pi)$ --- which is called the **likelihood** of the hypothesis $\\pi$, relative to the data $D$ ---  and the prior $p(\\pi)$. $p(D)$ is the prior probability of the data (aka marginal likelihood) and is necessary for the posterior to be a (proper) probability distribution.\n\nWe can \"read\" the formula as: *The probability of the parameter given the data is the product of the probability of the data given the parameter and the prior probability of the parameter*.\n\nLet's express our **prior** belief in probabilistic terms:\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](chapter4_files/figure-html/unnamed-chunk-35-1.png){width=672}\n:::\n:::\n\n\n\n\nNow we collect data and we observe $x = 40$ tails out of $k = 50$ trials thus $\\hat{\\pi} = 0.8$ and compute the *likelihood*:\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](chapter4_files/figure-html/unnamed-chunk-36-1.png){width=672}\n:::\n:::\n\n\n\n\nFinally we combine, using the Bayes rule, **prior** and **likelihood** to obtain the **posterior** distribution:\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](chapter4_files/figure-html/unnamed-chunk-37-1.png){width=672}\n:::\n:::\n\n\n\n\nThe **Bayes Factor** (BF) --- also called the **likelihood ratio**, for obvious reasons --- is a measure of the relative support that the evidence provides for two competing hypotheses, $H_0$ and $H_1$ (~ $\\pi$ in our previous example).  It plays a key role in the following *odds form* of Bayes's theorem.\n\n$$\n\\frac{p(H_0|D)}{p(H_1|D)} = \\frac{p(D|H_0)}{p(D|H_1)} \\times \\frac{p(H_0)}{p(H_1)}\n$$\n\nThe ratio of the priors $\\frac{p(H_0)}{p(H_1)}$ is called the **prior odds** of the hypotheses; and, the ratio of the poosteriors $\\frac{p(H_0| D)}{p(H_1 | D)}$ is called the **posterior odds** of the hypotheses. Thus, the above (odds form) of Bayes's Theorem can be paraphrased as follows\n\n$$\n\\text{posterior odds} = \\text{Bayes Factor} \\times \\text{prior odds}\n$$\nIn this way, it can be seen that the Bayes Factor captures the relative empirical support that the evidence $D$ provides in favor of $H_0$ over $H_1$.  If the Bayes Factor is greater than 1, then $D$ favors $H_0$ over $H_1$.  If the BF is less than one, then  $D$ favors $H_1$ over $H_0$.  If the BF is 1, then $D$ is neutral regarding $H_0$ vs $H_1$ [@Royall1997-ye].  For this reason, the BF is also sometimes called the **weight of evidence** in favor of $H_0$ (*vs* $H_1$) [@Good1950-go].\n\n### Calculating the Bayes Factor using the SDR\n\nThe Savage-Dickey density ratio (SDR) is a convenient shortcut to calculate the Bayes Factor [see @Wagenmakers2010-fj; @Marin2010-oh] in a situation where the null hypothesis ($H_0$) is a single parameter value (e.g., $H_0: \\theta = 0$). With SDR the Bayes Factor can be calculated as the ratio between the prior and posterior density distribution under the alternative hypothesis $H_1$.\n\n$$\nBF_{01} = \\frac{p(D|H_0)}{p(D|H_1)} \\approx \\frac{p(\\pi = x|D, H_1)}{p(\\pi = x | H_1)}\n$$\n\nWhere $\\pi$ is the parameter of interest and $x$ is the null value under $H_0$ (e.g., 0). and $D$ are the data. \n\nWhen there are multiple parameters in play and the null hypothesis is expressed in terms of only one of them, this representation is valid only under the assumption that the prior for any of the other parameters under the null is the same as the conditional prior for those parameters under the alternative. \n\nFollowing the previous example $H_0: \\pi = 0.5$. Under $H_1$ we use a vague prior by setting $\\pi \\sim Beta(1, 1)$.\n\nSay we flipped the coin 20 times and we found that $\\hat \\pi = 0.75$.\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](chapter4_files/figure-html/unnamed-chunk-38-1.png){width=672}\n:::\n:::\n\n\n\n\nThe ratio between the two black dots is the Bayes Factor.\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](chapter4_files/figure-html/unnamed-chunk-39-1.png){width=672}\n:::\n:::\n\n\n\n\n### @Verhagen2014-tx model^[see also @Ly2019-ow for an improvement]\n\nThe idea is using the posterior distribution of the original study as prior for a Bayesian hypothesis testing where:\n\n- $H_0: \\theta_{r} = 0$ meaning that there is no effect in the replication study\n- $H_1: \\theta_{r} \\neq 0$ and in particular is distributed as $\\delta \\sim \\mathcal{N}(\\theta_{0}, \\sigma^2_{0})$ where $\\theta_{0}$ and $\\sigma^2_{0}$ are the mean and standard error of the original study\n\nIf $H_0$ is more likely after seeing the data, there is evidence against the replication (i.e., $BF_{r0} > 1$) otherwise there is evidence for a successful replication ($BF_{r1} > 1$).\n\n::: {.callout-warning}\n**Disclaimer:** The actual implementation of @Verhagen2014-tx is different (they use the $t$ statistics). We implemented a similar model using a Bayesian linear model with `rstanarm`.\n:::\n\nLet's assume that the original study ($n = 30$) estimates a $\\theta_{0} = 0.4$ and a standard error $\\sigma^2/n$.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\n# original study\nn <- 30\nyorig <- 0.4\nse <- sqrt(1/n)\n```\n:::\n\n\n\n\n::: {.callout-note}\nThe assumption of @Verhagen2014-tx is that the original study performed a Bayesian analysis with a flat (unform) prior. Thus, the confidence interval is approximately the same as the Bayesian credible interval.\n:::\n\nFor this reason, the posterior distribution of the original study can be approximated as:\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](chapter4_files/figure-html/unnamed-chunk-41-1.png){width=672}\n:::\n:::\n\n\n\n\nLet's imagine that a new study tried to replicate the original one. They collected $n = 100$ participants with the same protocol and found an effect of $y_{rep} = 0.1$.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnrep <- 100\nyrep <- MASS::mvrnorm(nrep, mu = 0.1, Sigma = 1, empirical = TRUE)[, 1]\ndat <- data.frame(y = yrep)\nhist(yrep, main = \"Replication Study (n1 = 100)\", xlab = latex2exp::TeX(\"$\\\\theta_{r}$\"))\nabline(v = mean(yrep), lwd = 2, col = \"firebrick\")\n```\n\n::: {.cell-output-display}\n![](chapter4_files/figure-html/unnamed-chunk-42-1.png){width=672}\n:::\n:::\n\n\n\n\nWe can analyze these data with an *intercept-only regression model* setting as prior the posterior distribution of the original study:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# setting the prior on the intercept parameter\nprior <- rstanarm::normal(location = yorig,\n                          scale = se)\n\n# fitting the bayesian linear regression\nfit <- stan_glm(y ~ 1, \n                data = dat, \n                prior_intercept = prior,\n                refresh = FALSE)\n\nsummary(fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nModel Info:\n function:     stan_glm\n family:       gaussian [identity]\n formula:      y ~ 1\n algorithm:    sampling\n sample:       4000 (posterior sample size)\n priors:       see help('prior_summary')\n observations: 100\n predictors:   1\n\nEstimates:\n              mean   sd   10%   50%   90%\n(Intercept) 0.2    0.1  0.1   0.2   0.3  \nsigma       1.0    0.1  0.9   1.0   1.1  \n\nFit Diagnostics:\n           mean   sd   10%   50%   90%\nmean_PPD 0.2    0.1  0.0   0.2   0.3  \n\nThe mean_ppd is the sample average posterior predictive distribution of the outcome variable (for details see help('summary.stanreg')).\n\nMCMC diagnostics\n              mcse Rhat n_eff\n(Intercept)   0.0  1.0  2609 \nsigma         0.0  1.0  2608 \nmean_PPD      0.0  1.0  3185 \nlog-posterior 0.0  1.0  1693 \n\nFor each parameter, mcse is Monte Carlo standard error, n_eff is a crude measure of effective sample size, and Rhat is the potential scale reduction factor on split chains (at convergence Rhat=1).\n```\n\n\n:::\n:::\n\n\n\n\nWe can use the `bayestestR::bayesfactor_pointnull()` to calculate the BF using the Savage-Dickey density ratio.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbf <- bayestestR::bayesfactor_pointnull(fit, null = 0)\nprint(bf)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nBayes Factor (Savage-Dickey density ratio)\n\nParameter   |    BF\n-------------------\n(Intercept) | 0.274\n\n* Evidence Against The Null: 0\n```\n\n\n:::\n\n```{.r .cell-code}\nplot(bf)\n```\n\n::: {.cell-output-display}\n![](chapter4_files/figure-html/unnamed-chunk-44-1.png){width=672}\n:::\n:::\n\n\n\n\nYou can also use the `bf_replication()` function:\n\n\n\n\n```r\nbf_replication <- function(mu_original,\n                           se_original,\n                           replication){\n\n  # prior based on the original study\n  prior <- rstanarm::normal(location = mu_original, scale = se_original)\n\n  # to dataframe\n  replication <- data.frame(y = replication)\n\n  fit <- rstanarm::stan_glm(y ~ 1,\n                            data = replication,\n                            prior_intercept = prior,\n                            refresh = 0) # avoid printing\n\n  bf <- bayestestR::bayesfactor_pointnull(fit, null = 0, verbose = FALSE)\n\n  title <- \"Bayes Factor Replication Rate\"\n  posterior <- \"Posterior Distribution ~ Mean: %.3f, SE: %.3f\"\n  replication <- \"Evidence for replication: %3f (log %.3f)\"\n  non_replication <- \"Evidence for non replication: %3f (log %.3f)\"\n\n  if(bf$log_BF > 0){\n    replication <- cli::col_green(sprintf(replication, exp(bf$log_BF), bf$log_BF))\n    non_replication <- sprintf(non_replication, 1/exp(bf$log_BF), -bf$log_BF)\n  }else{\n    replication <- sprintf(replication, exp(bf$log_BF), bf$log_BF)\n    non_replication <- cli::col_red(sprintf(non_replication, 1/exp(bf$log_BF), -bf$log_BF))\n  }\n\n  outlist <- list(\n    fit = fit,\n    bf = bf\n  )\n\n  cat(\n    cli::col_blue(title),\n    cli::rule(),\n    sprintf(posterior, fit$coefficients, fit$ses),\n    \"\\n\",\n    replication,\n    non_replication,\n    sep = \"\\n\"\n  )\n\n  invisible(outlist)\n\n}\n```\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nbf_replication(mu_original = yorig, se_original = se, replication = yrep)\n```\n:::\n\n\n\n\nA better custom plot:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbfplot <- data.frame(\n  prior = rnorm(1e5, yorig, se),\n  posterior = rnorm(1e5, fit$coefficients, fit$ses)\n)\n \nggplot() +\n  stat_function(geom = \"line\", \n                aes(color = \"Original Study (Prior)\"),\n                linewidth = 1,\n                alpha = 0.3,\n                fun = dnorm, args = list(mean = yorig, sd = se)) +\n  stat_function(geom = \"line\",\n                linewidth = 1,\n                aes(color = \"Replication Study (Posterior)\"),\n                fun = dnorm, args = list(mean = fit$coefficients, sd = fit$ses)) +\n  xlim(c(-0.5, 1.2)) +\n  geom_point(aes(x = c(0, 0), y = c(dnorm(0, yorig, sd = se),\n                                    dnorm(0, fit$coefficients, sd = fit$ses))),\n             size = 3) +\n  xlab(latex2exp::TeX(\"\\\\delta\")) +\n  ylab(\"Density\") +\n  theme(legend.position = \"bottom\",\n        legend.title = element_blank())\n```\n\n::: {.cell-output-display}\n![](chapter4_files/figure-html/unnamed-chunk-47-1.png){width=672}\n:::\n:::\n\n\n\n\n## Conclusions\n\nWe hope to have provided a useful overview of statistical approaches to different definitions of replication. The meta-analytic and $Q$ methods are more focused on accumulating evidence combining original and replication studies. The CI and PI methods are more interested in comparing the original and replication(s) study. The BF method constitutes a kind of compromise between these perspectives, by evaluating the weight of evidence in favor of the null --- from the point of view of the posterior distribution of the replication study. \n\n## Key Questions\n\n- How might combining evidence from original and replication studies provide a richer understanding than comparing them separately?\n- Why might different definitions of replication lead to different conclusions about the success of a replication attempt?\n\n## References {.unnumbered}\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}