{
  "hash": "b857c91f361bfc378240ca4a5a2b1512",
  "result": {
    "engine": "knitr",
    "markdown": "<!-- TODO:  table with method (rows), (colums)properties, id exact or extension, bayesian or not, open source code, citation -->\n\n# Statistical methods for replication assessment\n\n\n\n\n\n::: {.cell}\n\n:::\n\n\n\n\n\n@Anderson2024-cx suggests that also the perspective of supporting the null hypothesis should be taken into account especially when we are really skeptical about the initial results. We want to replicate an effect to see the reliability, maybe better estimating the true effect with a larger sample (taking into account the inflation of the original effect) but we could also think that the original effect is a false positive and we want to do the replication to see if the null hypothesis is supported. The bayes factor approach by @Ly2019-ow @Verhagen2014-tx is exactly about this. also equivalence testing is useful\n\n<!-- TODO check the final table and the overall structure of the paper because it's great also the references are amazing -->\n\n\n\n\n\n```{mermaid}\ngraph TD\n    A[Statistical Methods]\n    A --> B[Sign/Direction]\n    B --> C[Vote Counting]\n    A --> D[Confidence/Prediction Interval]\n    D --> E[CI/PI original/replication]\n    D --> F[Small Telescope]\n    A --> G[Meta-analysis]\n    G --> H[Equal and Random-effects]\n    G --> h[cumulative meta analysis]\n    G --> I[Q Statistics]\n    A --> J[Bayesian Methods]\n    J --> K[Replication Bayes Factor]\n    J --> z[Pawel model]\n    J --> a[Skeptical p value]\n    J --> b[Skeptical prior etz]\n```\n\n\n\n\n\n<!-- TODO the idea here is also to keep track of the methods providing a clear overview -->\n\n## Introduction\n\nIn the previous chapters we introduced replication from a statistical and theoretical point of view. We purposely omitted defining the outcome of a replication study to dedicate an entire chapter about this part. Similarly to the multiple definitions problem there are several statistical measures for the replication assessment. There are also multiple ways to propose a classification but we could indentify:\n\n<!-- \npropose some kind of classification/taxonomy here. for example:\n\n- linked or not to a definition\n- aim of the metric (e.g., testing or estimating)\n- statistical approach (bayesian, frequentists, etc.)\n- practical limitations (only for one-to-one/many design, only with raw data, etc.)\n-->\n\nIn addition to the multitude of methods, the field of replication measures is relatively new and keep growing proposing new metrics, simulation studies, and comparisons between methods. \n\nA review work could be in principle the best option but this is not feasible and also useful for a very active research area. There is an amazing project proposed by @Heyard2024-hv where an extensive systematic review is supported by an online and keep up-to-date database with all replication measures organized and classified according to a common methodology. The authors reviewed roughly 50 different measures. The database is avaliable at the following link [http://rachelheyard.com/reproducibility_metrics/](http://rachelheyard.com/reproducibility_metrics/).\n\nWe decided to make a selection of methods from the database according to the following criteria:\n\n- for similar methods we keep the most recent and general version. For example, if a method $x$ originally developed for *one-to-one* replication designs has been extended to cover also *one-to-many* designs we will present the latter.\n- we select both bayesian and frequentist methods\n- we selected methods more related to inference and also methods more focused on estimation of the effect size\n- we selected methods that are not linked to a specific research area or topic. For example in the database there are methods for *voxels* in fMRI research or methods for metrology.\n\n## Methods\n\n- [x] Prediction interval: replication effect in original 95% prediction interval\n- [x] Proportion of population effects agreeing in direction with the original\n- [x] Bayes Factor: Independent Jeffreys-Zellner-Siow BF test (default BF)\n- Bayes Factor: Equality-of-effect-size BF test\n- Bayes Factor: Fixed-effect meta-analysis BF Test (Meta-analytic BF) [this probably just a bayesian meta-analysis]\n- Bayesian Evidence Synthesis (variant: Meta-Analysis Model-based Assessment of replicability (MAMBA))\n- Bayesian mixture model for reproducibility rate\n- Confidence interval: original effect in replication 95% CI (Coverage)\n- Confidence interval: replication effect in original 95%CI (Capture probability)\n- Continuously cumulating meta-analytic approach\n- Correspondence test\n- Credibility analysis (Reverse-Bayes, probability of credibility, probability of replicating an effect)\n- Design analysis [type-m/s]\n- Equivalence testing (TOST (two one-sided tests))\n- Likelihood-based approach for reproducibility (Likelihood-ratio) [similar to bf?]\n- Minimum effect testing [similar to small telescope?]\n- P interval\n- Prediction interval: replication effect in original 95% prediction interval\n- Replication Bayes factor [already included]\n- Sceptical $p$-value (versions: nominal sceptical $p$-value, golden sceptical $p$-value, controlled sceptical $p$-value)\n- Sceptical Bayes Factor (Reverse-Bayes)\n- Small Telescopes\n- Snapshot hybrid (Bayesian meta-analysis)\n- Z-curve (Exact replication rate, p-curves)\n- Consistency of original with replications, $P_{\\mbox{orig}}$\n- I squared - $I^2$ (Estimation of effect variance) [this can be togheter with Q and meta-analysis]\n- Proportion of population effects agreeing in direction with the original, $\\hat{P}_{>0}$\n- Bland-Altman Plot (Agreement measures)\n- Correlation between effects\n- Difference in effect size (Q-statistic, (meta-analytic) Q-test, difference test, Tukeyâ€™s post-hoc honest significant difference test)\n- Externally standardized residuals [idea di calcolare tipo m su effetto stimato]\n- Meta-analysis\n- Significance criterion (vote counting, two-trials rule, regulatory agreement)\n\n## Replication studies as meta-analysis\n\nAs introduced by @Hedges2019-ry replication studies can be seen from a statistical point of view as meta-analyses. There is a single initial study and one or more attempts to replicate this initial finding. For the simple one-to-one replication design we have a meta-analysis with two studies while in the one-to-many design we have a meta-analysis with $k$ studies. When comes to evaluate the results of the replication studies we can choose between several methods [see @Heyard2024-hv] and some of them are specifically meta-analysis based trying to pool togheter evidence from original and replication studies. We consider the meta-analytic thinking crucial to understand the replication studies and methods but not all methods are strictly meta-analyes in the usual sense.\n\nIn fact, while the methodology of the initial or replication studies is important when comes to evaluate the single result, at the aggregated level we evaluate how a certain focal parameter (e.g., the difference between two groups or conditions) vary across replications and id there is evidence, whatever the criteria, for a successful replication. Whatever is the model used in the original studies we can essentially think at the aggregated level without loss of generality loss.\n\n## Simulating data\n\nWhile real-world examples are important, to understand the replication methods from a statistical point of view, simulating simplified examples is a good strategies. Furthermore, simulating data is nowadays considered an important tool to teach and understand statistical methods <!-- TODO add reference about simulating for learning -->. In additions, Monte Carlo simulations are necessary to estimate statistical properties (e.g., statistical power or type-1 error rate) of complex models.\n\nFor the simulated examples we can define the following simulation approach:\n\n- primary studies always compare two independent groups on a certain response variable\n- within a study, the two groups are assumed to comes from two normal distributions with unit variance. For one group the mean is centered on zero and for the other group the mean is centered on the value representing the effect size for that specific study.\n- the sample size can vary between the two groups\n\nUsing the same notation as @sec-prob-replication we can define:\n\n- $y_0$: as the reference group\n- $y_1$: as the treated group\n\nFor example, to simulate a single study we can define the true effect size parameter $\\theta_r$ and then generate two independent groups sampled from two normal distributions with variance equal to one. One of the distribution is centered on zero while the other distribution is centered on $\\theta_r$.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntheta <- 0.3\nn0 <- 30\nn1 <- 30\n\ny0 <- rnorm(n0, 0, 1)\ny1 <- rnorm(n1, theta, 1)\nmean(y1) - mean(y0) # this is the effect size\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.7219476\n```\n\n\n:::\n\n```{.r .cell-code}\nvar(y1)/n1 + var(y0)/n0 # this is the sampling variability\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.04570848\n```\n\n\n:::\n:::\n\n\n\n\n\nNow we can simply iterate the process for $R$ studies to create a series of replications with a common true parameter $\\theta$. We can also generate a more realistic set of sample sizes sampling from a probability distribution (e.g., Poisson)\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nR <- 10\nyi <- vi <- rep(NA, R)\nn0 <- n1 <- 10 + rpois(R, 40 - 10)\n\nfor(i in 1:R){\n    y0 <- rnorm(n0[i], 0, 1)\n    y1 <- rnorm(n1[i], theta, 1)\n    yi[i] <- mean(y1) - mean(y0)\n    vi[i] <- var(y1)/n1[i] + var(y0)/n0[i]\n}\n\nsim <- data.frame(id = 0:(R - 1), yi, vi, n0, n1)\nsim\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   id          yi         vi n0 n1\n1   0  0.45319870 0.05896224 30 30\n2   1  0.75219123 0.04300375 39 39\n3   2  0.48505037 0.05021670 34 34\n4   3 -0.02538025 0.07475703 28 28\n5   4  0.03408793 0.04360755 36 36\n6   5  0.30382729 0.04678444 44 44\n7   6  0.32837823 0.07442270 35 35\n8   7  0.52876249 0.07427798 35 35\n9   8  0.16347147 0.04452318 42 42\n10  9  0.26753467 0.05095639 35 35\n```\n\n\n:::\n:::\n\n\n\n\n\nThen, if we want to include variability in the true effects as in the extension framework we can simply sample $R$ $\\theta$'s from a normal distribution with variability $\\tau^2$.\n\n\n\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n   id          yi         vi       theta n0 n1\n1   0  0.23025582 0.02991196 -0.12701687 49 49\n2   1  0.03450965 0.04275686 -0.15888936 43 43\n3   2 -0.15974907 0.05264337 -0.08834685 36 36\n4   3  0.84069792 0.03653456  0.34532322 51 51\n5   4  0.54427307 0.03850827  0.48883364 39 39\n6   5  0.85082587 0.05670523  0.78024036 43 43\n7   6 -0.29223114 0.05542844  0.18416183 38 38\n8   7 -0.31715118 0.05055857 -0.26577465 31 31\n9   8  0.49313340 0.04785534  0.46160076 55 55\n10  9  0.19670947 0.03042870  0.31707221 45 45\n```\n\n\n:::\n:::\n\n\n\n\n\nThen we can put everything in a function that can be used to simulate different scenarios.\n\n\n\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n   id        yi         vi n0 n1\n1   1 0.5031167 0.07332769 30 30\n2   2 0.6784039 0.04764774 30 30\n3   3 0.7020578 0.08696338 30 30\n4   4 0.2297185 0.05573452 30 30\n5   5 0.3727306 0.05226134 30 30\n6   6 0.5249823 0.06689612 30 30\n7   7 0.4601728 0.05637307 30 30\n8   8 0.3384389 0.07316258 30 30\n9   9 0.6214128 0.06649973 30 30\n10 10 0.5610664 0.10315956 30 30\n```\n\n\n:::\n:::\n\n\n\n\n\n\n## References\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}