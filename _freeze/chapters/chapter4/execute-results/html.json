{
  "hash": "3138e3333cd9062028af5ff5b92fd3da",
  "result": {
    "engine": "knitr",
    "markdown": "<!-- TODO:  table with method (rows), (colums)properties, id exact or extension, bayesian or not, open source code, citation -->\n\n# Statistical methods for replication assessment\n\n\n\n\n::: {.cell}\n\n:::\n\n\n\n\n@Anderson2024-cx suggests that also the perspective of supporting the null hypothesis should be taken into account especially when we are really skeptical about the initial results. We want to replicate an effect to see the reliability, maybe better estimating the true effect with a larger sample (taking into account the inflation of the original effect) but we could also think that the original effect is a false positive and we want to do the replication to see if the null hypothesis is supported. The bayes factor approach by @Ly2019-ow @Verhagen2014-tx is exactly about this. also equivalence testing is useful\n\n<!-- TODO check the final table and the overall structure of the paper because it's great also the references are amazing -->\n\n\n\n\n```{mermaid}\ngraph TD\n    A[Statistical Methods]\n    A --> B[Sign/Direction]\n    B --> C[Vote Counting]\n    A --> D[Confidence/Prediction Interval]\n    D --> E[CI/PI original/replication]\n    D --> F[Small Telescope]\n    A --> G[Meta-analysis]\n    G --> H[Equal and Random-effects]\n    G --> h[cumulative meta analysis]\n    G --> I[Q Statistics]\n    A --> J[Bayesian Methods]\n    J --> K[Replication Bayes Factor]\n    J --> z[Pawel model]\n    J --> a[Skeptical p value]\n    J --> b[Skeptical prior etz]\n```\n\n\n\n\n<!-- TODO the idea here is also to keep track of the methods providing a clear overview -->\n\n## Introduction\n\nIn the previous chapters we introduced replication from a statistical and theoretical point of view. We purposely omitted defining the outcome of a replication study to dedicate an entire chapter about this part. Similarly to the multiple definitions problem there are several statistical measures for the replication assessment. There are also multiple ways to propose a classification but we could indentify:\n\n<!-- \npropose some kind of classification/taxonomy here. for example:\n\n- linked or not to a definition\n- aim of the metric (e.g., testing or estimating)\n- statistical approach (bayesian, frequentists, etc.)\n- practical limitations (only for one-to-one/many design, only with raw data, etc.)\n-->\n\nIn addition to the multitude of methods, the field of replication measures is relatively new and keep growing proposing new metrics, simulation studies, and comparisons between methods. A review work could be in principle the best option but this is not feasible and also useful for a very active research area. There is an amazing project proposed by @Heyard2024-hv where an extensive systematic review is supported by an online and keep up-to-date database with all replication measures organized and classified according to a common methodology. The authors reviewed roughly 50 different measures. The database is avaliable at the following link [http://rachelheyard.com/reproducibility_metrics/](http://rachelheyard.com/reproducibility_metrics/).\n\nWe decided to make a selection of methods from the database according to the following criteria:\n\n- for similar methods we keep the most recent and general version. For example, if a method $x$ riginally developed for *one-to-one* replication designs has been extended to cover also *one-to-many* designs we will present the latter.\n- we select both bayesian and frequentist methods\n- we selected methods more related to inference and also methods more focused on estimation of the effect size\n- we selected methods that are not linked to a specific research area or topic. For example in the database there are methods for *voxels* in fMRI research or methods for metrology.\n\n## Methods\n\n- Prediction interval: replication effect in original 95% prediction interval\n- Proportion of population effects agreeing in direction with the original\n- Bayes Factor: Independent Jeffreys-Zellner-Siow BF test (default BF)\n- Bayes Factor: Equality-of-effect- size BF test\n- Bayes Factor: Fixed-effect meta-analysis BF Test (Meta-analytic BF) [this probably just a bayesian meta-analysis]\n- Bayesian Evidence Synthesis (variant: Meta-Analysis Model-based Assessment of replicability (MAMBA))\n- Bayesian mixture model for reproducibility rate\n- Confidence interval: original effect in replication 95% CI (Coverage)\n- Confidence interval: replication effect in original 95%CI (Capture probability)\n- Continuously cumulating meta-analytic approach\n- Correspondence test\n- Credibility analysis (Reverse-Bayes, probability of credibility, probability of replicating an effect)\n- Design analysis [type-m/s]\n- Equivalence testing (TOST (two one-sided tests))\n- Likelihood-based approach for reproducibility (Likelihood-ratio) [similar to bf?]\n- Minimum effect testing [similar to small telescope?]\n- P interval\n- Prediction interval: replication effect in original 95% prediction interval\n- Replication Bayes factor [already included]\n- Sceptical $p$-value (versions: nominal sceptical $p$-value, golden sceptical $p$-value, controlled sceptical $p$-value)\n- Sceptical Bayes Factor (Reverse-Bayes)\n- Small Telescopes\n- Snapshot hybrid (Bayesian meta-analysis)\n- Z-curve (Exact replication rate, p-curves)\n- Consistency of original with replications, $P_{\\mbox{orig}}$\n- I squared - $I^2$ (Estimation of effect variance) [this can be togheter with Q and meta-analysis]\n- Proportion of population effects agreeing in direction with the original, $\\hat{P}_{>0}$\n- Bland-Altman Plot (Agreement measures)\n- Correlation between effects\n- Difference in effect size (Q-statistic, (meta-analytic) Q-test, difference test, Tukey’s post-hoc honest significant difference test)\n- Externally standardized residuals [idea di calcolare tipo m su effetto stimato]\n- Meta-analysis\n- Significance criterion (vote counting, two-trials rule, regulatory agreement)\n\n## Worth mentioning\n\n- RepliCATS [subjective elicitation]\n- Text-based machine learning model to estimate reproducibility\n\n## for me to check\n\n- Causal replication framework\n\n## Simulating data\n\nIn light of the formalization presented in Chapter XXX and XXX we can formulate a general method to simulate data from the generative model (<!-- TODO insert citation to figures lavagna e pavel -->).\n\n<!-- TODO introduce here the notation for effect size and sampling variance -->\n\nTo simulate a single study we can define the true effect size parameter $\\theta_r$ and then generate two independent groups sampled from two normal distributions with variance equal to one. One of the distribution is centered on zero while the other distribution is centered on $\\theta_r$.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntheta <- 0.3\nn0 <- 30\nn1 <- 30\n\ny0 <- rnorm(n0, 0, 1)\ny1 <- rnorm(n1, theta, 1)\nmean(y1) - mean(y0) # this is the effect size\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] -0.02011754\n```\n\n\n:::\n\n```{.r .cell-code}\nvar(y1)/n1 + var(y0)/n0 # this is the sampling variability\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.05565741\n```\n\n\n:::\n:::\n\n\n\n\nNow we can simply iterate the process for $R$ studies to create a series of replications with a common true parameter $\\theta$. We can also generate a more realistic set of sample sizes sampling from a probability distribution (e.g., Poisson)\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nR <- 10\nyi <- vi <- rep(NA, R)\nn0 <- n1 <- 10 + rpois(R, 40 - 10)\n\nfor(i in 1:R){\n    y0 <- rnorm(n0[i], 0, 1)\n    y1 <- rnorm(n1[i], theta, 1)\n    yi[i] <- mean(y1) - mean(y0)\n    vi[i] <- var(y1)/n1[i] + var(y0)/n0[i]\n}\n\nsim <- data.frame(id = 0:(R - 1), yi, vi, n0, n1)\nsim\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   id          yi         vi n0 n1\n1   0  0.01476904 0.05239931 38 38\n2   1 -0.02142140 0.04549403 51 51\n3   2  0.36376339 0.03300727 49 49\n4   3  0.39667421 0.05930267 44 44\n5   4  0.26316570 0.05476932 37 37\n6   5  0.14944415 0.04993779 45 45\n7   6  0.34185438 0.03761737 46 46\n8   7  0.45372498 0.05433267 46 46\n9   8  0.50300280 0.05410981 40 40\n10  9  0.37954551 0.06210466 40 40\n```\n\n\n:::\n:::\n\n\n\n\nThen, if we want to include variability in the true effects as in the extension framework we can simply sample $R$ $\\theta$'s from a normal distribution with variability $\\tau^2$.\n\n\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n   id           yi         vi        theta n0 n1\n1   0  0.003218650 0.03593693 -0.126228824 48 48\n2   1  0.740607983 0.04974657  0.700254299 47 47\n3   2  0.901889954 0.03934327  0.897036878 37 37\n4   3  0.008334652 0.04281873  0.222999103 38 38\n5   4  0.150974265 0.04332951  0.383630820 46 46\n6   5  0.414119571 0.05491717  0.444614032 37 37\n7   6 -0.374487466 0.06629007 -0.238958229 32 32\n8   7  0.201141441 0.05602005  0.216779790 37 37\n9   8  0.466078981 0.04478137  0.411308942 49 49\n10  9 -0.196622204 0.04875768  0.002872426 37 37\n```\n\n\n:::\n:::\n\n\n\n\nThen we can put everything in a function that can be used to simulate different scenarios.\n\n\n\n\n```r\nsim_meta <- function(R, mu, tau2, n0, n1 = NULL, conf.level = 0.95){\n    if(is.null(n1)) n1 <- n0\n    if((length(n1) != R & length(n1) != 1) | (length(n0) != R & length(n0) != 1)){\n        stop(\"n1 and n0 need to be of length 1 or R\")\n    }\n\n    if(length(n1) == 1) n1 <- rep(n1, R)\n    if(length(n0) == 1) n0 <- rep(n0, R)\n\n    theta <- rnorm(R, mu, sqrt(tau2))\n    yi <- vi <- rep(NA, R)\n\n    for(i in 1:R){\n        y0 <- rnorm(n0[i], 0, 1)\n        y1 <- rnorm(n1[i], theta[i], 1)\n        yi[i] <- mean(y1) - mean(y0)\n        vi[i] <- var(y1)/n1[i] + var(y0)/n0[i]\n    }\n    sim <- data.frame(id = 0:(R - 1), yi, vi, theta, n0, n1)\n    \n    # compute other statistics\n    sim$sei <- sqrt(sim$vi) # standard error\n    sim$zi <- sim$yi / sim$sei # test statistics\n    tc <- abs(qnorm((1 - conf.level)/2))\n    sim$ci.lb <- sim$yi - sim$sei * tc\n    sim$ci.ub <- sim$yi + sim$sei * tc\n    return(sim)\n}\n```\n\n::: {.cell}\n\n```{.r .cell-code}\nsim_meta(R = 10, mu = 0.3, tau2 = 0.1, n0 = 30)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   id          yi         vi       theta n0 n1       sei         zi       ci.lb\n1   0  0.12685160 0.06650511 -0.04926811 30 30 0.2578858  0.4918905 -0.37859538\n2   1  0.32214536 0.06410597  0.36172305 30 30 0.2531916  1.2723384 -0.17410099\n3   2  0.57035399 0.07591139  0.80030468 30 30 0.2755202  2.0700984  0.03034427\n4   3 -0.33659132 0.07222032 -0.03667565 30 30 0.2687384 -1.2524869 -0.86330888\n5   4  0.88320631 0.07125207  0.62179008 30 30 0.2669308  3.3087460  0.36003149\n6   5 -0.08357788 0.06610357  0.02818698 30 30 0.2571061 -0.3250715 -0.58749667\n7   6  0.45593954 0.05151685  0.29803240 30 30 0.2269732  2.0087811  0.01108017\n8   7  0.16901371 0.06837960  0.22246040 30 30 0.2614949  0.6463364 -0.34350695\n9   8  0.03551535 0.04562731  0.02705788 30 30 0.2136055  0.1662661 -0.38314372\n10  9  0.05060030 0.05997280  0.02772444 30 30 0.2448934  0.2066217 -0.42938202\n       ci.ub\n1  0.6322986\n2  0.8183917\n3  1.1103637\n4  0.1901262\n5  1.4063811\n6  0.4203409\n7  0.9007989\n8  0.6815344\n9  0.4541744\n10 0.5305826\n```\n\n\n:::\n:::\n\n\n\n\nThe function can be easily extended for using different effect size measures or study designs but the overall idea is the same.\n\n\n\n\n::: {.cell}\n\n:::\n\n\n\n\n\n\n# What is considered a successful or unsuccessful replication? {.question .smaller}\n\n# Some (random) concepts {.section}\n\n## Some (random) concepts\n\n> Credibility of scientific claims is established with evidence for their replicability using new data [@Nosek2020-vh]\n\n> Replication is repeating a study’s procedure and observing whether the prior finding recurs [@Jeffreys1973-bp]\n\n> Replication is a study for which any outcome would be considered diagnostic evidence about a claim from prior research [@Nosek2020-vh].\n\n## Difficulty in drawing conclusions from replications\n\nReplication is often intended as *conditioned* to the original result. The original result could be a false positive or a biased result. Also the replication attempt could be a false positive or a false negative [@Nosek2020-vh].\n\n. . .\n\n> To be a replication, two things must be true. Outcomes consistent with a prior claim would increase confidence in the claim, and outcomes inconsistent with a prior claim would decrease confidence in the claim [@Nosek2020-vh].\n\n. . .\n\nThis is somehow similar with a Bayesian reasoning where evidence about a phenomenon is updated after collecting more data.\n\n## Exact and Conceptual replications\n\nExact replications are commonly considered as the *gold-standard* but in practice (especially in Social Sciences, Psychology, etc.) are rare.\n\nLet's imagine, an original study $y_{or}$ finding a result.\n\n- Replication $y_{rep}$ with the *exact* same method find the same result. **Replication or not?**\n- Replication $y_{rep}$ with a *similar* method find the same result. **Replication or not?**\n- Replication $y_{rep}$ with *similar* method did not find the same result. **Replication or not?**\n\n## Direct and Conceptual replications [@Schmidt2009-mq]\n\nA **direct replication** is defined as the repetition of an experimental procedure.\n\nA **conceptual replication** is defined as testing the same hypothesis with different methods.\n\n## Exact replications are (often) impossible [@Schmidt2009-mq]\n\nLet's imagine an extreme example: **testing the physiological reaction to arousing situation**:\n\n- The original study: Experiment with prehistoric reacting to an arousing stimulus\n- The actual replication: It is possible to create the exact situation? Some phenomenon changes overtime, especially people-related phenomenon\n\nExact replication is often not feasible. Even using the same experimental setup (*direct replication*) does not assure that we are studying the same phenomenon.\n\n## As Exact as possible...\n\nEven when an experiment use almost the same setup of the original study there is a source of unknown uncertainty. Which is the impact of a slightly change in the experimental setup on the actual result?\n\n- A study on the human visual system: presenting stimuli on different monitors --> small change with a huge impact\n- A study on consumer behavior: participant answering question using a smartphone or a computer --> small but (maybe) irrelevant change\n\nHow to evaluate the actual impact?\n\n# What we are going to do? {.section}\n\n## What we are (not) going to do?\n\n. . .\n\n- I will not present a strictly theoretical and philosophical approach to replication (*what is a replication?*, *what is the most appropriate definition?*, etc.). But we can discuss it together :smile:!\n\n. . .\n\n- According to the replication definitions and problems, we will explore some **statistical** methods to evaluate a replication success\n\n# Overall model and notation {.section}\n\n## Overall model and notation\n\nFor the purpose of notation and simplicity we can define a meta-analytical-based replication model [@Hedges2019-ry; @Schauer2021-ja; @Schauer2022-mj]\n\n$$\ny_i = \\mu_{\\theta} + \\delta_i + \\epsilon_i\n$$\n\n$$\n\\delta_i \\sim \\mathcal{N}(0, \\tau^2)\n$$\n\n$$\n\\epsilon_i \\sim \\mathcal{N}(0, \\sigma^2_i)\n$$\n\n## Overall model and notation\n\n- Thus each study $i$ out of the number of studies $k$.\n- $\\mu_{\\theta}$ is the real average effect and $\\theta = \\mu_{\\theta} + \\delta_i$ is the real effect of each study\n- $\\tau^2$ is the real variance among different studies. When $\\tau^2 = 0$ there is no variability among studies\n- $\\epsilon_i$ are the sampling errors that depends on $\\sigma^2_i$, the sampling variability of each study\n- We define $\\theta_{orig}$ (or $\\theta_1$) as the original study and $\\theta_{rep_i}$ (with $i$ from 2 to $k$) as the replication studies \n\n## Simulating for learning\n\nFor the examples we are going to simulate studies. Each study comes from a two-groups comparison on a continous outcome:\n\n$$\n\\Delta = \\overline{X_1} - \\overline{X_2}\n$$\n\n$$\nSE_{\\Delta} = \\sqrt{\\frac{s^2_1}{n_1} + \\frac{s^2_2}{n_2}}\n$$\n\nWith $X_{1_j} \\sim \\mathcal{N}(0, 1)$ and $X_{2_j} \\sim \\mathcal{N}(\\Delta, 1)$\n\ncontinue...\n\n## Simulating for learning\n\nThus our observed effect sizes $y_i$ is sampled from:\n$$\ny_i \\sim \\mathcal{N}(\\mu_\\theta, \\tau^2 + \\frac{1}{n_1} + \\frac{1}{n_2})\n$$\n\nWhere $\\frac{1}{n_1} + \\frac{1}{n_2}$ is the sampling variability ($\\sigma^2_i$).\n\nThe sampling variances are sampled from:\n\n$$\n\\sigma_i^2 \\sim \\frac{\\chi^2_{n_1 + n_2 - 2}}{n_1 + n_2 - 2} (\\frac{1}{n_1} + \\frac{1}{n_2})\n$$\n\n## Simulating for learning\n\nEverything is implemented into the `sim_studies()` function:\n\n\n\n\n::: {.cell layout-align=\"center\"}\n```r\nsim_studies <- function(k, theta, tau2, n0, n1, summary = FALSE){\n  yi <- rnorm(k, theta, sqrt(tau2 + 1/n0 + 1/n1))\n  vi <- (rchisq(k, n0 + n1 - 2) / (n0 + n1 - 2)) * (1/n0 + 1/n1)\n  out <- data.frame(yi, vi, sei = sqrt(vi))\n  if(summary){\n    out <- summary_es(out)\n  }\n  return(out)\n}\n```\n:::\n\n\n\n\n## Simulating for learning, an example\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"false\"}\nsim_studies(k = 10, theta = 0.5, tau2 = 0.1, n0 = 30, n1 = 30)\n```\n\n<pre class=\"r-output\"><code>          yi         vi       sei\n1  0.6306046 0.07040783 0.2653447\n2  0.3117390 0.06975489 0.2641115\n3  1.3005623 0.06786796 0.2605148\n4  0.6860657 0.05444358 0.2333315\n5  0.5698951 0.07307152 0.2703174\n6  1.1003673 0.08245891 0.2871566\n7  1.4643882 0.05544487 0.2354673\n8  0.1718634 0.07772710 0.2787958\n9  0.3836479 0.06972331 0.2640517\n10 0.3877660 0.07604360 0.2757600\n</code></pre>\n:::\n\n\n\n\n## Exact vs Approximate replication\n\nThis distinction [see @Brandt2014-da for a different terminology] refers to parameters $\\theta_i$. With *exact* are considering a case where:\n\n$$\n\\theta_1 = \\theta_2 = \\theta_3, \\dots, \\theta_k\n$$\n\nThus the true parameters of $k$ replication studies are the same. Thus the variability among true effects $\\tau^2 = 0$.\n\nSimilarly, due to (often not controllable) differences among experiments (i.e., lab, location, sample, etc.) we could expect a certain degree of variability $\\tau^2$. In other terms $\\tau^2 < \\tau^2_0$ where $\\tau^2_0$ is the maximum variability (that need to be defined). In this way studies are replicating:\n\n$$\n\\theta_i \\sim \\mathcal{N}(\\mu_\\theta, \\tau^2_0)\n$$\n\n## Types of agreement\n\nCoarsely, we can define a replication success when two or more studies obtain the \"same\" result. The definion of *sameness* it is crucial:\n\n- same *sign* or direction: two studies (original and replication) evaluating the efficacy of a treatment have a positive effect $sign(\\theta_1) = sign(\\theta_2)$ where $sign$ is the sign function.\n- same *magnitude*: two studies (original and replication) evaluating the efficacy of a treatment have the same effect in terms $|\\theta_1 - \\theta_2| = 0$ or similar up to a tolerance factor $|\\theta_1 - \\theta_2| < \\gamma$ where $\\gamma$ is the maximum difference considered as null.\n\nThe different methods that we are going to see are focused on a specific type of aggreement. For example, we could consider $\\theta_1 = 3x$ and $\\theta_2 = x$ to have the same sign but the replication study is on a completely different scale. Is this considered a successful replication?\n\n## Falsification vs Consistency\n\n. . .\n\nThis refers to how the replication setup is formulated. With $k = 2$ studies where $k_1$ is the original study and $k_2$ is the replication we have a *one-to-one* setup. In this setup we compare the replication with the original and according to the chosen method and expectation we conclude if $k_1$ has been replicated or not.\n\n. . .\n\nWhen $k > 2$ we could collapse the replication studies into a single value (e.g., using a meta-analysis method) and compare the results using a *one-to-one* or we can use a method for *one-to-many* designs.\n\n. . .\n\nRegardless the method, *falsification* approaches compared the original with the replicate(s) obtaining a yes-no answer or a continuous result. On the other side *consistency* methods are focused on evaluating the degree of similarity (i.e., consistency) among all studies.\n\n## The big picture\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n\n\n\n# Statistical Methods {.section}\n\n## Statistical Methods, disclaimer [@Schauer2021-ja]\n\n- There are no unique methods to assess replication from a statistical point of view\n- For available statistical methods, statistical properties (e.g., type-1 error rate, power, bias, etc.) are not always known or extensively examined\n- Different methods answers to the same question or to different replication definitions\n\n# Frequentists Methods {.section}\n\n## Vote Counting based on significance or direction\n\nThe simplest method is called **vote counting** [@Valentine2011-yq; @Hedges1980-gd]. A replication attempt $\\theta_{rep}$ is considered successful if the result has the same direction of the original study $\\theta_{orig}$ and it is statistically significant i.e., $p_{\\theta_{rep}} \\leq \\alpha$. Similarly we can count the number of replication with the *same sign* as the original study.\n\n:::{.pros}\n- Easy to understand, communicate and compute\n:::\n\n:::{.cons}\n- Did not consider the size of the effect\n- Depends on the power of $\\theta_{rep}$\n:::\n\n## Example with simulated data\n\nLet's simulate an exact replication:\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n## original study\nn_orig <- 30\ntheta_orig <- theta_from_z(2, n_orig)\n\norig <- data.frame(\n  yi = theta_orig,\n  vi = 4/(n_orig*2)\n)\n\norig$sei <- sqrt(orig$vi)\norig <- summary_es(orig)\n\norig\n```\n\n<pre class=\"r-output\"><code>         yi         vi       sei zi       pval      ci.lb    ci.ub\n1 0.5163978 0.06666667 0.2581989  2 0.04550026 0.01033725 1.022458\n</code></pre>\n\n```{.r .cell-code}\n## replications\n\nk <- 10\nreps <- sim_studies(k = k, theta = theta_orig, tau2 = 0, n_orig, n_orig, summary = TRUE)\n\nhead(reps)\n```\n\n<pre class=\"r-output\"><code>         yi         vi       sei       zi         pval       ci.lb     ci.ub\n1 0.8797180 0.06597473 0.2568555 3.424953 0.0006149050  0.37629053 1.3831455\n2 0.7245546 0.04843434 0.2200780 3.292262 0.0009938480  0.29320963 1.1558996\n3 0.4111667 0.04870422 0.2206903 1.863094 0.0624491032 -0.02137836 0.8437118\n4 0.7085433 0.06511996 0.2551861 2.776574 0.0054935065  0.20838765 1.2086989\n5 0.8810186 0.05914530 0.2431981 3.622638 0.0002916136  0.40435914 1.3576780\n6 0.3968792 0.06680153 0.2584599 1.535554 0.1246477547 -0.10969292 0.9034514\n</code></pre>\n:::\n\n\n\n\n## Example with simulated data\n\nLet's compute the proportions of replication studies are statistically significant:\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"false\"}\nmean(reps$pval <= 0.05)\n```\n\n<pre class=\"r-output\"><code>[1] 0.6\n</code></pre>\n:::\n\n\n\n\nLet's compute the proportions of replication studies with the same sign as the original:\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"false\"}\nmean(sign(orig$yi) == sign(reps$yi))\n```\n\n<pre class=\"r-output\"><code>[1] 1\n</code></pre>\n:::\n\n\n\n\nWe could also perform some statistical tests. See @Bushman2009-zv and @Hedges1980-gd for vote-counting methods in meta-analysis.\n\n## Vote Counting, extreme example\n\n\n\n\n\n\n\n\n\nLet's imagine an original experiment with $n_{orig} = 30$ and $\\hat \\theta_{orig} = 0.5$ that is statistically significant $p \\approx 0.045$. Now a direct replication (thus assuming $\\tau^2 = 0$) study with $n_{rep} = 350$ found $\\hat \\theta_{rep_1} = 0.15$, that is statistically significant $p\\approx 0.047$.\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](chapter4_files/figure-html/unnamed-chunk-17-1.svg){fig-align='center' width=672}\n:::\n:::\n\n\n\n\n## Confidence Interval, replication within original\n\n::: {.panel-tabset}\n\n### Theory\n\nAnother approach check if the replication attempt $\\theta_{rep}$ is contained in the % confidence interval of the original study $\\theta_{orig}$. Formally:\n\n$$\n\\theta_{orig} - \\Phi(\\alpha/2) \\sqrt{\\sigma^2_{orig}} < \\theta_{rep} < \\theta_{orig} + \\Phi(\\alpha/2) \\sqrt{\\sigma^2_{orig}}\n$$\n\nWhere $\\Phi$ is the cumulative standard normal distribution, $\\alpha$ is the type-1 error rate.\n\n:::{.pros}\n- Take into account the size of the effect and the precision of $\\theta_{orig}$\n:::\n:::{.cons}\n- The original study is assumed to be a reliable estimation\n- No extension for *many-to-one* designs\n- Low precise original studies lead to higher success rate\n:::\n\n### Plot\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nse_orig <- sqrt(4 / (2 * n_orig))\nci_orig <- theta_orig + qnorm(c(0.025, 0.975)) * se_orig\ncurve(dnorm(x, theta_orig, se_orig), \n      -1, 2, \n      ylab = \"Density\", \n      xlab = latex2exp::TeX(\"$\\\\theta$\"))\nabline(v = ci_orig, lty = \"dashed\")\npoints(theta_orig, 0, pch = 19, cex = 2)\npoints(theta_rep, 0, pch = 19, cex = 2, col = \"firebrick\")\nlegend(\"topleft\", \n       legend = c(\"Original\", \"Replication\"), \n       fill = c(\"black\", \"firebrick\"))\n```\n\n::: {.cell-output-display}\n![](chapter4_files/figure-html/unnamed-chunk-18-1.svg){fig-align='center' width=672}\n:::\n:::\n\n\n\n\n:::\n\n## Confidence Interval, replication within original\n\nOne potential problem of this method regards that low precise original studies are \"easier\" to replicate due to larger confidence intervals. \n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](chapter4_files/figure-html/unnamed-chunk-19-1.svg){fig-align='center' width=672}\n:::\n:::\n\n\n\n\n## Confidence Interval, original within replication\n\n::: {.panel-tabset}\n\n### Theory\n\nThe same approach can be applied checking if the original effect size is contained within the replication confidence interval. Clearly these methods depends on the precision of studies. Formally:\n\n$$\n\\theta_{rep} - \\Phi(\\alpha/2) \\sqrt{\\sigma_{rep}^2} < \\theta_{orig} < \\theta_{rep} + \\Phi(\\alpha/2) \\sqrt{\\sigma_{rep}^2}\n$$\n\nThe method has the same pros and cons of the previous approach. One advantage is that usually replication studies are more precise (higher sample size) thus the parameter and the % CI is more reliable.\n\n### Plot\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](chapter4_files/figure-html/unnamed-chunk-20-1.svg){fig-align='center' width=672}\n:::\n:::\n\n\n\n\n:::\n\n## Prediction interval (PI), what to expect from a replication\n\nOne problem of the previous approaches is taking into account only the uncertainty of the original or the replication study. @Patil2016-vc and @Spence2016-tz proposed a method to take into account both sources of uncertainty.\n\nIf the original and replication studies comes from the same population, the sampling distribution of the difference is centered on 0 with a certain standard error $\\theta_{orig} - \\theta_{rep_0} \\sim \\mathcal{N}\\left( 0, \\sqrt{\\sigma^2_{\\hat \\theta_{orig} - \\hat \\theta_{rep}}} \\right)$ (subscript $0$ to indicate that is expected to be sampled from the same population as $\\theta_{orig}$)\n\n$$\n\\hat \\theta_{orig} \\pm z_{95\\%} \\sqrt{\\sigma^2_{\\theta_{orig} - \\theta_{rep}}}\n$$\n\nIf factors other than standard error influence the replication result, $\\theta_{rep_0}$  is not expected to be contained within the 95% prediction interval.\n\n## Prediction interval (PI), what to expect from a replication\n\nIn the case of a (un)standardized mean difference we can compute the prediction interval as:\n\n$$\n\\sqrt{\\sigma^2_{\\epsilon_{\\hat \\theta_{orig} - \\hat \\theta_{rep_0}}}} = \\sqrt{\\left( \\frac{\\hat \\sigma^2_{o1}}{n_{o1}} +\\frac{\\hat \\sigma^2_{o2}}{n_{o2}}\\right) + \\left(\\frac{\\hat \\sigma^2_{o1}}{n_{r1}} + \\frac{\\hat\\sigma^2_{o2}}{n_{r2}}\\right)}\n$$\n\nThe first term is just the standard error of the difference between the two groups in the original study and the second term is the standard error of the **hypothetical** replication study assuming the same standard deviation of the original but a different $n$.\n\nIn this way we estimate an interval where, combining sampling variance from both studies and assuming that they comes from the same population, the replication should fall.\n\n## Prediction interval (PI), what to expect from a replication\n\n::: {.panel-tabset}\n\n### R Code\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"false\"}\nset.seed(2025)\n\no1 <- rnorm(50, 0.5, 1) # group 1\no2 <- rnorm(50, 0, 1) # group 2\nod <- mean(o1) - mean(o2) # effect size\nse_o <- sqrt(var(o1)/50 + var(o2)/50) # standard error of the difference\n\nn_r <- 100 # sample size replication\n\nse_o_r <- sqrt(se_o^2 + (var(o1)/100 + var(o2)/100))\n\nod + qnorm(c(0.025, 0.975)) * se_o_r\n```\n\n<pre class=\"r-output\"><code>[1] 0.325520 1.298378\n</code></pre>\n:::\n\n\n\n\n### Plot\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\npar(mar = c(4, 4, 0.1, 0.1))  \ncurve(dnorm(x, od, se_o_r), od - se_o_r*4, od + se_o_r*4, lwd = 2, xlab = latex2exp::TeX(\"$\\\\theta$\"), ylab = \"Density\")\nabline(v = od + qnorm(c(0.025, 0.975)) * se_o_r, lty = \"dashed\")\n```\n\n::: {.cell-output-display}\n![](chapter4_files/figure-html/unnamed-chunk-22-1.svg){fig-align='center' width=70%}\n:::\n:::\n\n\n\n\n### Pros/Cons\n\n:::{.pros}\n- Take into account uncertainty of both studies\n- We can plan a replication using the standard deviation of the original study and the expected sample size\n:::\n\n:::{.cons}\n- Low precise original studies lead to wide PI. For a replication study is difficult to fall outside the PI\n- Mainly for *one-to-one* replications design\n:::\n\n:::\n\n## Mathur & VanderWeele [-@Mathur2020-nw] $p_{orig}$\n\n::: {.panel-tabset}\n\n### Theory\n\nMathur & VanderWeele [-@Mathur2020-nw] proposed a new method based on the prediction interval to calculate a p value $p_{orig}$ representing the probability that $\\theta_{orig}$ is consistent with the replications. This method is suited for *many-to-one* replication designs. Formally:\n\n$$\nP_{orig} = 2 \\left[ 1 - \\Phi \\left( \\frac{|\\hat \\theta_{orig} - \\hat \\mu_{\\theta_{rep}}|}{\\sqrt{\\hat \\tau^2 + \\sigma^2_{orig} + \\hat{SE}^2_{\\hat \\mu_{\\theta_{rep}}}}} \\right) \\right]\n$$\n\n- $\\mu_{\\theta_{rep}}$ is the pooled (i.e., meta-analytic) estimation of the $k$ replications\n- $\\tau^2$ is the variance among replications\n\n### Pros-cons\n\nIt is interpreted as the probability that $\\theta_{orig}$ is equal or more extreme that what observed. A very low $p_{orig}$ suggest that the original study is inconsistent with replications.\n\n:::{.pros}\n- Suited for *many-to-one* designs\n- We take into account all sources of uncertainty\n- We have a p-value\n:::\n\n### R Code\n\nThe code is implemented in the `Replicate` and `MetaUtility` R packages:\n\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"false\"}\ntau2 <- 0.05\ntheta_rep <- 0.2\ntheta_orig <- 0.7\n\nn_orig <- 30\nn_rep <- 100\nk <- 20\n\nreplications <- sim_studies(k, theta_rep, tau2, n_rep, n_rep)\noriginal <- sim_studies(1, theta_orig, 0, n_orig, n_orig)\n\nfit_rep <- metafor::rma(yi, vi, data = replications) # random-effects meta-analysis\n\nReplicate::p_orig(original$yi, original$vi, fit_rep$b[[1]], fit_rep$tau2, fit_rep$se^2)\n```\n\n<pre class=\"r-output\"><code>[1] 0.5563241\n</code></pre>\n:::\n\n\n\n\n### Simulation\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\n# standard errors assuming same n and variance 1\nse_orig <- sqrt(4/(n_orig * 2))\nse_rep <- sqrt(4/(n_rep * 2))\nse_theta_rep <- sqrt(1/((1/(se_rep^2 + tau2)) * k)) # standard error of the random-effects estimate\n\nsep <- sqrt(tau2 + se_orig^2 + se_theta_rep^2) # z of p-orig denominator\n\ncurve(dnorm(x, theta_rep, sep), theta_rep - 4*sep, theta_rep + 4*sep, ylab = \"Density\", xlab = latex2exp::TeX(\"\\\\theta\"))\npoints(theta_orig, 0.02, pch = 19, cex = 2)\nabline(v = qnorm(c(0.025, 0.975), theta_rep, sep), lty = \"dashed\", col = \"firebrick\")\n```\n\n::: {.cell-output-display}\n![](chapter4_files/figure-html/unnamed-chunk-25-1.svg){fig-align='center' width=672}\n:::\n:::\n\n\n\n\n\n\n:::\n\n## Mathur & VanderWeele [-@Mathur2020-nw] $\\hat P_{> 0}$\n\n::: {.panel-tabset}\n\n### Theory\n\nAnother related metric is the $\\hat P_{> 0}$, representing the proportion of replications following the same direction as the original effect. Before simply computing the proportions we need to adjust the estimated $\\theta_{rep_i}$ with a shrinkage factor:\n\n$$\n\\tilde{\\theta}_{rep_i} = (\\theta_{rep_i} - \\mu_{\\theta_{rep_i}}) \\sqrt{\\frac{\\hat \\tau^2}{\\hat \\tau^2 + v_{rep_i}}}\n$$\n\nThis method is somehow similar to the vote counting but we are adjusting the effects taking into account $\\tau^2$.\n\n### R Code\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"false\"}\n# compute calibrated estimation for the replications\n# use restricted maximum likelihood to estimate tau2 under the hood\ntheta_sh <- MetaUtility::calib_ests(replications$yi, replications$sei, method = \"REML\")\nmean(theta_sh > 0)\n```\n\n<pre class=\"r-output\"><code>[1] 0.75\n</code></pre>\n:::\n\n\n\n\n### Bootstrap Code\n\nThe authors suggest a bootstrapping approach for making inference on $\\hat P_{> 0}$\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"false\"}\nnboot <- 1e4\ntheta_boot <- matrix(0, nrow = nboot, ncol = k)\n\nfor(i in 1:nboot){\n  idx <- sample(1:nrow(replications), nrow(replications), replace = TRUE)\n  replications_boot <- replications[idx, ]\n  theta_cal <- MetaUtility::calib_ests(replications_boot$yi, \n                                       replications_boot$sei, \n                                       method = \"REML\")\n  theta_boot[i, ] <- theta_cal\n}\n\n# calculate\np_greater_boot <- apply(theta_boot, 1, function(x) mean(x > 0))\n```\n:::\n\n\n\n\n### Bootstrap Results\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](chapter4_files/figure-html/unnamed-chunk-29-1.svg){fig-align='center' width=672}\n:::\n:::\n\n\n\n\n:::\n\n## Mathur & VanderWeele [-@Mathur2020-nw] $\\hat P_{\\gtrless q*}$\n\nInstead of using 0 as threshold, we can use meaningful effect size to be considered as low but different from 0. $\\hat P_{\\gtrless q*}$ is the proportion of (calibrated) replications greater or lower than the $q*$ value. This framework is similar to equivalence and minimum effect size testing [@Lakens2018-ri].\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"false\"}\nq <- 0.2 # minimum non zero effect\n\nfit <- metafor::rma(yi, vi, data = replications)\n\n# see ?MetaUtility::prop_stronger\nMetaUtility::prop_stronger(q = q,\n                           M = fit$b[[1]],\n                           t2 = fit$tau2,\n                           tail = \"above\",\n                           estimate.method = \"calibrated\",\n                           ci.method = \"calibrated\",\n                           dat = replications,\n                           yi.name = \"yi\",\n                           vi.name = \"vi\")\n```\n\n<pre class=\"r-output\"><code>   est        se  lo   hi  bt.mn shapiro.pval\n1 0.35 0.1137901 0.1 0.55 0.3573    0.5224829\n</code></pre>\n:::\n\n\n\n\n## Combining original and replications\n\n::: {.panel-tabset}\n\n### Theory\n\nAnother approach is to combine the original and replication results (both *one-to-one* and *many-to-one*) using a meta-analysis model. Then we can test if the pooled estimate is different from 0 or another meaningful value.\n\n:::{.pros}\n- Use all the available information, especially when fitting a random-effects model\n- Take into account the precision by inverse-variance weighting\n:::\n\n:::{.cons}\n- Did not consider the publication bias\n- For *one-to-one* designs only a fixed-effects model can be used\n:::\n\n### Fixed-effects Model\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# fixed-effects\nfit_fixed <- rma(yi, vi, method = \"FE\")\nsummary(fit_fixed)\n```\n\n<pre class=\"r-output\"><code>\nFixed-Effects Model (k = 20)\n\n  logLik  deviance       AIC       BIC      AICc   \n 20.7415   -0.0000  -39.4829  -38.4872  -39.2607   \n\nI^2 (total heterogeneity / total variability):   0.00%\nH^2 (total variability / sampling variability):  0.00\n\nTest for Heterogeneity:\nQ(df = 19) = 0.0000, p-val = 1.0000\n\nModel Results:\n\nestimate      se    zval    pval   ci.lb   ci.ub      \n  0.2000  0.0316  6.3246  <.0001  0.1380  0.2620  *** \n\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n</code></pre>\n:::\n\n\n\n\n### Random-Effects model\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# fixed-effects\nfit_random <- rma(yi, vi, method = \"REML\")\nsummary(fit_random)\n```\n\n<pre class=\"r-output\"><code>\nRandom-Effects Model (k = 20; tau^2 estimator: REML)\n\n  logLik  deviance       AIC       BIC      AICc   \n 19.7044  -39.4088  -35.4088  -33.5199  -34.6588   \n\ntau^2 (estimated amount of total heterogeneity): 0 (SE = 0.0065)\ntau (square root of estimated tau^2 value):      0\nI^2 (total heterogeneity / total variability):   0.00%\nH^2 (total variability / sampling variability):  1.00\n\nTest for Heterogeneity:\nQ(df = 19) = 0.0000, p-val = 1.0000\n\nModel Results:\n\nestimate      se    zval    pval   ci.lb   ci.ub      \n  0.2000  0.0316  6.3246  <.0001  0.1380  0.2620  *** \n\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n</code></pre>\n:::\n\n\n\n\n### Pooling replications\n\nThe previous approach can be also implemented combining replications into a single effect and then compare the original with the combined replication study.\n\nThis is similar to using the CI or PI approaches but the replication effect will probably by very precise due to pooling multiple studies.\n\n:::\n\n## Q Statistics \n\nAn interesting proposal is using the Q statistics [@Hedges2019-pr; @Hedges2019-ar; @Hedges2021-of; @Schauer2022-mj; @Schauer2021-ja; @Schauer2020-tw; @Hedges2019-ry], commonly used in meta-analysis to assess the presence of heterogeneity. Formally:\n\n$$\nQ = \\sum_{i = 1}^{k} \\frac{(\\theta_i - \\bar \\theta_w)^2}{\\sigma^2_i}\n$$\n\nWhere $\\bar \\theta_w$ is the inverse-variance weighted average (i.g., fixed-effect model). The Q statistics is essentially a weighted sum of squares. Under the null hypothesis where all studies are equal $\\theta_1 = \\theta_2, ... = \\theta_i$ the Q statistics has a $\\chi^2$ distribution with $k - 1$ degrees of freedom. Under the alternative hypothesis the distribution is a non-central $\\chi^2$ with non centrality parameter $\\lambda$. The expected value of the $Q$ is $E(Q) = v + \\lambda$, where $v$ are the degrees of freedom.\n\n## Q Statistics \n\nHedges & Schauer proposed to use the Q statistics to evaluate the consistency of a series of replications:\n\n- In case of *exact* replication, $\\lambda = 0$ because $\\theta_1 = \\theta_2, ... = \\theta_k$.\n- In case of *approximate* replication, $\\lambda < \\lambda_0$ where $\\lambda_0$ is the maximum value considered as equal to null (i.e., 0).\n\nThis approach is testing the consistency (i.e., homogeneity) of replications. A successful replication should minimize the heterogeneity and the presence of a significant Q statistics should bring evidence for not replicating the effect^[The approach has been debated by a series of opinion papers [see @Hedges2019-pr; @Mathur2019-vh]].\n\n## Q Statistics\n\nThe method has been expanded and formalized in several papers with different objectives:\n\n. . .\n\n- to cover different replications setup (burden of proof on replicating vs non-replicating, many-to-one and one-to-one, etc.)\n\n. . .\n\n- interpret and choose the $\\lambda$ parameter given that is the core of the approach\n\n. . .\n\n- evaluating the power and statistical properties under different replication scenarios\n\n. . .\n\n- the standard implementation put the burden of proof on non-replication. Thus $H_0$ is that studies replicates. They provided also a series of tests with the opposite formulation.\n\n## Q Statistics\n\nIn the case of evaluating an exact replication we can use the `Qrep()` function that simply calculate the p-value based on the Q sampling distribution.\n\n::: {.panel-tabset}\n\n### Function\n\n\n\n\n::: {.cell layout-align=\"center\"}\n```r\nQrep <- function(yi, vi, lambda0 = 0, alpha = 0.05){\n  fit <- metafor::rma(yi, vi)\n  k <- fit$k\n  Q <- fit$QE\n  df <- k - 1\n  Qp <- pchisq(Q, df = df, ncp = lambda0, lower.tail = FALSE)\n  pval <- ifelse(Qp < 0.001, \"p < 0.001\", sprintf(\"p = %.3f\", Qp))\n  lambda <- ifelse((Q - df) < 0, 0, (Q - df))\n  res <- list(Q = Q, lambda = lambda, pval = Qp, df = df, k = k, alpha = alpha, lambda0 = lambda0)\n  H0 <- ifelse(lambda0 != 0, paste(\"H0: lambda <\", lambda0), \"H0: lambda = 0\")\n  title <- ifelse(lambda0 != 0, \"Q test for Approximate Replication\", \"Q test for Exact Replication\")\n  cli::cli_rule()\n  cat(cli::col_blue(cli::style_bold(title)), \"\\n\\n\")\n  cat(sprintf(\"Q = %.3f (df = %s), lambda = %.3f, %s\", res$Q, res$df, lambda, pval), \"\\n\")\n  cat(H0, \"\\n\")\n  cli::cli_rule()\n  class(res) <- \"Qrep\"\n  invisible(res)\n}\n```\n:::\n\n\n\n\n### Code\n\n\n\n\n::: {.cell layout-align=\"center\"}\n<pre class=\"r-output\"><code><span style='color: #0000BB; font-weight: bold;'>Q test for Exact Replication</span> \n\nQ = 367.321 (df = 99), lambda = 268.321, p < 0.001 \nH0: lambda = 0 \n</code></pre>\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"false\"}\nQres <- Qrep(dat$yi, dat$vi)\n```\n\n<pre class=\"r-output\"><code><span style='color: #0000BB; font-weight: bold;'>Q test for Exact Replication</span> \n\nQ = 367.321 (df = 99), lambda = 268.321, p < 0.001 \nH0: lambda = 0 \n</code></pre>\n:::\n\n\n\n\n### Plot\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nplot.Qrep(Qres)\n```\n\n::: {.cell-output-display}\n![](chapter4_files/figure-html/unnamed-chunk-36-1.svg){fig-align='center' width=672}\n:::\n:::\n\n\n\n\n:::\n\n## Q Statistics for approximate replication\n\n- In case of approximate replication we need to set $\\lambda_0$ to a meaningful value but the overall test is the same. The critical $Q$ is no longer evaluated with a central $\\chi^2$ but a non-central $\\chi^2$ with $\\lambda_0$ as non-centrality parameter.\n\n- @Hedges2019-ry provide different strategies to choose $\\lambda_0$. They found that under some assumptions, $\\lambda = (k - 1) \\frac{\\tau^2}{\\tilde{v}}$\n\n- Given that we introduced the $I^2$ statistics we can derive a $\\lambda_0$ based in $I^2$. @Schmidt2014-kw proposed that when $\\tilde{v}$ is at least 75% of total variance $\\tilde{v} + \\tau^2$ thus $\\tau^2$ could be considered neglegible. This corresponds to a $I^2 = 25%$ and a ratio $\\frac{\\tau^2}{\\tilde{v}} = 1/3$ thus $\\lambda_0 = \\frac{(k - 1)}{3}$ can be considered a neglegible heterogeneity\n\n## Q Statistics for approximate replication\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"false\"}\nk <- 100\ndat <- sim_studies(k, 0.5, 0, 50, 50)\nQrep(dat$yi, dat$vi, lambda0 = (k - 1)/3)\n```\n\n<pre class=\"r-output\"><code><span style='color: #0000BB; font-weight: bold;'>Q test for Approximate Replication</span> \n\nQ = 98.121 (df = 99), lambda = 0.000, p = 0.977 \nH0: lambda < 33 \n</code></pre>\n:::\n\n\n\n\n## Small Telescopes [@Simonsohn2015-kg]\n\nSimonsohn [-@Simonsohn2015-kg] introduced 3 main questions when evaluating replicability:\n\n. . .\n\n1. When we combine data from the original and replication study, what is our best guess of the overall effect?\n\n. . .\n\n**meta-analysis**\n\n. . .\n\n2. Is the effect of the replication study different from the original study?\n\n. . .\n\n**meta-analysis** and standard tests, but problematic in terms of statistical power\n\n. . .\n\n3. Does the replication study suggest that the effect of interest is undetectable different from zero?\n\n. . .\n\n**small telescopes**\n\n## Small Telescopes [@Simonsohn2015-kg]\n\nThe idea is simple but quite powerful and insightful. Let's assume that an original study found an effect of $y_{orig} = 0.7$ on a two-sample design with $n = 20$ per group.\n\n. . .\n\n- we define a threshold as the effect size that is associated with a certain low power level e.g., $33\\%$ given the sample size i.e. $\\theta_{small} = 0.5$\n- the replication study found an effect of $y_{rep} = 0.2$ with $n = 100$ subjects\n\n. . .\n\nIf the $y_{rep}$ is lower (i.e., the upper bound of the confidence interval) than the *small effect* ($\\theta_{small} = 0.5$) we conclude that the effect is probably so tiny that could not have been detected by the original study. Thus there is no evidence for a replication.\n    \n## Small Telescopes [@Simonsohn2015-kg]\n\nWe can use the custom `small_telescope()` function on simulated data:\n\n\n\n\n::: {.cell layout-align=\"center\"}\n```r\nsmall_telescope <- function(or_d,\n                            or_se,\n                            rep_d,\n                            rep_se,\n                            small,\n                            ci = 0.95){\n  # quantile for the ci\n  qs <- c((1 - ci)/2, 1 - (1 - ci)/2)\n  \n  # original confidence interval\n  or_ci <- or_d + qnorm(qs) * or_se\n  \n  # replication confidence interval\n  rep_ci <- rep_d + qnorm(qs) * rep_se\n  \n  # small power\n  is_replicated <- rep_ci[2] > small\n  \n  msg_original <- sprintf(\"Original Study: d = %.3f %s CI = [%.3f, %.3f]\",\n                          or_d, ci, or_ci[1], or_ci[2])\n  \n  msg_replicated <- sprintf(\"Replication Study: d = %.3f %s CI = [%.3f, %.3f]\",\n                            rep_d, ci, rep_ci[1], rep_ci[2])\n  \n  \n  if(is_replicated){\n    msg_res <- sprintf(\"The replicated effect is not smaller than the small effect (%.3f), (probably) replication!\", small)\n    msg_res <- cli::col_green(msg_res)\n  }else{\n    msg_res <- sprintf(\"The replicated effect is smaller than the small effect (%.3f), no replication!\", small)\n    msg_res <- cli::col_red(msg_res)\n  }\n  \n  out <- data.frame(id = c(\"original\", \"replication\"),\n                    d = c(or_d, rep_d),\n                    lower = c(or_ci[1], rep_ci[1]),\n                    upper = c(or_ci[2], rep_ci[2]),\n                    small = small\n  )\n  \n  # nice message\n  cat(\n    msg_original,\n    msg_replicated,\n    cli::rule(),\n    msg_res,\n    sep = \"\\n\"\n  )\n  \n  invisible(out)\n  \n}\n```\n:::\n\n\n\n\n## Small Telescopes [@Simonsohn2015-kg]\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"false\"}\nset.seed(2025)\n\nd <- 0.2 # real effect\n\n# original study\nor_n <- 20\nor_d <- 0.7\nor_se <- sqrt(1/20 + 1/20)\nd_small <- pwr::pwr.t.test(or_n, power = 0.33)$d\n\n# replication\nrep_n <- 100 # sample size of replication study\ng0 <- rnorm(rep_n, 0, 1)\ng1 <- rnorm(rep_n, d, 1)\n\nrep_d <- mean(g1) - mean(g0)\nrep_se <- sqrt(var(g1)/rep_n + var(g0)/rep_n)\n```\n:::\n\n\n\n\nHere we are using the `pwr::pwr.t.test()` to compute the effect size $\\theta_{small}$ (in code `d`) associated with 33% power.\n\n## Small Telescopes [@Simonsohn2015-kg]\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nsmall_telescope(or_d, or_se, rep_d, rep_se, d_small, ci = 0.95)\n```\n\n<pre class=\"r-output\"><code>Original Study: d = 0.700 0.95 CI = [0.080, 1.320]\nReplication Study: d = 0.214 0.95 CI = [-0.061, 0.490]\n────────────────────────────────────────────────────────────────────────────────\n<span style='color: #BB0000;'>The replicated effect is smaller than the small effect (0.493), no replication!</span>\n</code></pre>\n:::\n\n\n\n\nAnd a (quite over-killed) plot:\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](chapter4_files/figure-html/unnamed-chunk-41-1.svg){fig-align='center' width=672}\n:::\n:::\n\n\n\n\n# Bayesian Methods {.section}\n\n## Bayes Factor\n\n@Verhagen2014-tx proposed a method to estimate the evidence of a *replication* study. The core topics to understand the method are:\n\n- Bayesian hypothesis testing using the Bayes Factor [see, @Rouder2009-jh]\n- Bayes Factor using the Savage-Dickey density ratio [SDR, @Wagenmakers2010-fj]\n\n## Bayesian inference\n\nBayesian inference is the statistical procedure where **prior beliefs** about a phenomenon are combined, using the Bayes theorem, with **evidence from data** to obtain the **posterior beliefs**. \n\n. . .\n\nThe interesting part is that the researcher express the prior beliefs in probabilistic terms. Then after collecting data, evidence from the experiment is combined increasing or decreasing the plausibility of prior beliefs.\n\n. . .\n\nLet's make an (not a very innovative :smile:) example. We need to evaluate the fairness of a coin. The crucial parameter is $\\theta$ that is the probability of success (e.g., head). We have our prior belief about the coin (e.g., fair but with some uncertainty). We toss the coin $k$ times and we observe $x$ heads. What are my conclusions?\n\n## Bayesian inference\n\n$$\np(\\theta|D) = \\frac{p(D|\\theta) \\; p(\\theta)}{p(D)}\n$$\nWhere $\\theta$ is our parameter and $D$ the data. $p(\\theta|D)$ is the posterior distribution that is the product between the likelihood $p(D|\\theta)$ and the prior $p(\\theta)$. $p(D)$ is the probability of the data (aka marginal likelihood) and is necessary only for the posterior to be a proper probability distribution.\n\nWe can \"read\" the formula as: *The probability of the parameter given the data is the product between the likelihood of the data given the parameter and the prior probability of the parameter*.\n\n## Bayesian inference\n\nLet's express our **prior** belief in probabilistic terms:\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](chapter4_files/figure-html/unnamed-chunk-42-1.svg){fig-align='center' width=672}\n:::\n:::\n\n\n\n\n## Bayesian inference\n\nNow we collect data and we observe $x = 40$ tails out of $k = 50$ trials thus $\\hat{\\theta} = 0.8$ and compute the *likelihood*:\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](chapter4_files/figure-html/unnamed-chunk-43-1.svg){fig-align='center' width=672}\n:::\n:::\n\n\n\n\n## Bayesian inference\n\nFinally we combine, using the Bayes rule, **prior** and **likelihood** to obtain the **posterior** distribution:\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](chapter4_files/figure-html/unnamed-chunk-44-1.svg){fig-align='center' width=672}\n:::\n:::\n\n\n\n\n## Bayes Factor\n\nThe idea of the Bayes Factor is computing the evidence of the data under two competing hypotheses, $H_0$ and $H_1$ (~ $\\theta$ in our previous example):\n\n$$\n\\frac{p(H_0|D)}{p(H_1|D)} = \\frac{f(D|H_0)}{f(D|H_1)} \\times \\frac{p(H_0)}{p(H_1)}\n$$\n\nWhere $f$ is the likelihood function, $y$ are the data. The $\\frac{p(H_0)}{p(H_1)}$ is the prior odds of the two hypothesis. The Bayes Factor is the ratio between the likelihood of the data under the two hypotheses.\n\n## Bayes Factor using the SDR\n\nCalculating the BF can be problematic in some condition. The SDR is a convenient shortcut to calculate the Bayes Factor [@Wagenmakers2010-fj]. The idea is that the ratio between the prior and posterior density distribution for the $H_1$ is an estimate of the Bayes factor calculated in the standard way.\n    \n$$\nBF_{01} = \\frac{p(D|H_0)}{p(D|H_1)} = \\frac{p(\\theta = x|D, H_1)}{p(\\theta = x, H_1)}\n$$\n\nWhere $\\theta$ is the parameter of interest and $x$ is the null value under $H_0$ e.g., 0. and $D$ are the data.\n\n## Bayes Factor using the SDR, Example:\n\nFollowing the previous example $H_0: \\theta = 0.5$. Under $H_1$ we use a completely uninformative prior by setting $\\theta \\sim Beta(1, 1)$.\n\nWe flip again the coin 20 times and we found that $\\hat \\theta = 0.75$.\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](chapter4_files/figure-html/unnamed-chunk-45-1.svg){fig-align='center' width=672}\n:::\n:::\n\n\n\n\n## Bayes Factor using the SDR, Example:\n\nThe ratio between the two black dots is the Bayes Factor.\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](chapter4_files/figure-html/unnamed-chunk-46-1.svg){fig-align='center' width=672}\n:::\n:::\n\n\n\n\n::: {.notes}\nIf the probability density of the null value decrease after seeing data (from prior to posterior) this means that the Bayes factor should favor the alternative hypothesis.\nOn the left, the density of 0.5 is lower after seeing the data --> evidence for H1\nOn the right the density of 0.5 is higher after seeing the data --> evidence for H0\n:::\n\n## @Verhagen2014-tx model^[see also @Ly2019-ow for an improvement]\n\nThe idea is using the posterior distribution of the original study as prior for a Bayesian hypothesis testing where:\n\n- $H_0: \\theta_{rep} = 0$ thus there is no effect in the replication study\n- $H_1: \\theta_{rep} \\neq 0$ and in particular is distributed as $\\delta \\sim \\mathcal{N}(\\theta_{orig}, \\sigma^2_{orig})$ where $\\theta_{orig}$ and $\\sigma^2_{orig}$ are the mean and standard error of the original study\n\nIf $H_0$ is more likely after seeing the data, there is evidence against the replication (i.e., $BF_{r0} > 1$) otherwise there is evidence for a successful replication ($BF_{r1} > 1$).\n\n## @Verhagen2014-tx model\n\n::: {.callout-warning}\n**Disclaimer:** The actual implementation of @Verhagen2014-tx is different (they use the $t$ statistics). The proposed implementation for the current workshop use a standard linear model.\n:::\n\n## Example\n\nLet's assume that the original study ($n = 30$) estimate a $y_{orig} = 0.4$ and a standard error of $\\sigma^2/n$.\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"false\"}\n# original study\nn <- 30\nyorig <- 0.4\nse <- sqrt(1/30)\n```\n:::\n\n\n\n\n::: {.callout-note}\nThe assumption of Verhagen & Wagenmakers (2014) is that the original study performed a Bayesian analysis with a completely flat prior. Thus the confidence interval is the same as the Bayesian credible interval.\n:::\n\n## Example\n\nFor this reason, the posterior distribution of the original study can be approximated as:\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](chapter4_files/figure-html/unnamed-chunk-48-1.svg){fig-align='center' width=672}\n:::\n:::\n\n\n\n\n::: {.notes}\nWith an uninformative prior the credible interval is the same as the confidence interval\n:::\n\n## Example\n\nLet's imagine that a new study tried to replicate the original one. They collected $n = 100$ participants with the same protocol and found and effect of $y_{rep} = 0.1$.\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nnrep <- 100\nyrep <- MASS::mvrnorm(nrep, mu = 0.1, Sigma = 1, empirical = TRUE)[, 1]\ndat <- data.frame(y = yrep)\nhist(yrep, main = \"Replication Study (n1 = 100)\", xlab = latex2exp::TeX(\"$y_{rep}$\"))\nabline(v = mean(yrep), lwd = 2, col = \"firebrick\")\n```\n\n::: {.cell-output-display}\n![](chapter4_files/figure-html/unnamed-chunk-49-1.svg){fig-align='center' width=672}\n:::\n:::\n\n\n\n\n## Example\n\nWe can analyze these data with an *intercept-only regression model* setting as prior the posterior distribution of the original study:\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# setting the prior on the intercept parameter\nprior <- rstanarm::normal(location = yorig,\n                          scale = se)\n\n# fitting the bayesian linear regression\nfit <- stan_glm(y ~ 1, \n                data = dat, \n                prior_intercept = prior,\n                refresh = FALSE)\n\nsummary(fit)\n```\n\n<pre class=\"r-output\"><code>\nModel Info:\n function:     stan_glm\n family:       gaussian [identity]\n formula:      y ~ 1\n algorithm:    sampling\n sample:       4000 (posterior sample size)\n priors:       see help('prior_summary')\n observations: 100\n predictors:   1\n\nEstimates:\n              mean   sd   10%   50%   90%\n(Intercept) 0.2    0.1  0.1   0.2   0.3  \nsigma       1.0    0.1  0.9   1.0   1.1  \n\nFit Diagnostics:\n           mean   sd   10%   50%   90%\nmean_PPD 0.2    0.1  0.0   0.2   0.3  \n\nThe mean_ppd is the sample average posterior predictive distribution of the outcome variable (for details see help('summary.stanreg')).\n\nMCMC diagnostics\n              mcse Rhat n_eff\n(Intercept)   0.0  1.0  2609 \nsigma         0.0  1.0  2608 \nmean_PPD      0.0  1.0  3185 \nlog-posterior 0.0  1.0  1693 \n\nFor each parameter, mcse is Monte Carlo standard error, n_eff is a crude measure of effective sample size, and Rhat is the potential scale reduction factor on split chains (at convergence Rhat=1).\n</code></pre>\n:::\n\n\n\n\n## Example\n\n::: {.panel-tabset}\n\n### Results\n\nWe can use the `bayestestR::bayesfactor_pointnull()` to calculate the BF using the Savage-Dickey density ratio.\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nbf <- bayestestR::bayesfactor_pointnull(fit, null = 0)\nprint(bf)\n```\n:::\n\n\n\n\n### Plot\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nplot(bf)\n```\n:::\n\n\n\n\n:::\n\n## Example\n\nYou can also use the `bf_replication()` function:\n\n\n\n\n::: {.cell layout-align=\"center\"}\n```r\nbf_replication <- function(mu_original,\n                           se_original,\n                           replication){\n  \n  # prior based on the original study\n  prior <- rstanarm::normal(location = mu_original, scale = se_original)\n  \n  # to dataframe\n  replication <- data.frame(y = replication)\n  \n  fit <- rstanarm::stan_glm(y ~ 1,\n                            data = replication,\n                            prior_intercept = prior, \n                            refresh = 0) # avoid printing\n  \n  bf <- bayestestR::bayesfactor_pointnull(fit, null = 0, verbose = FALSE)\n  \n  title <- \"Bayes Factor Replication Rate\"\n  posterior <- \"Posterior Distribution ~ Mean: %.3f, SE: %.3f\"\n  replication <- \"Evidence for replication: %3f (log %.3f)\"\n  non_replication <- \"Evidence for non replication: %3f (log %.3f)\"\n  \n  if(bf$log_BF > 0){\n    replication <- cli::col_green(sprintf(replication, exp(bf$log_BF), bf$log_BF))\n    non_replication <- sprintf(non_replication, 1/exp(bf$log_BF), -bf$log_BF)\n  }else{\n    replication <- sprintf(replication, exp(bf$log_BF), bf$log_BF)\n    non_replication <- cli::col_red(sprintf(non_replication, 1/exp(bf$log_BF), -bf$log_BF))\n  }\n  \n  outlist <- list(\n    fit = fit,\n    bf = bf\n  )\n  \n  cat(\n    cli::col_blue(title),\n    cli::rule(),\n    sprintf(posterior, fit$coefficients, fit$ses),\n    \"\\n\",\n    replication,\n    non_replication,\n    sep = \"\\n\"\n  )\n  \n  invisible(outlist)\n  \n}\n```\n:::\n\n\n\n\n## Example\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"false\"}\nbf_replication(mu_original = yorig, se_original = se, replication = yrep)\n```\n:::\n\n\n\n\n## Example\n\nA better custom plot:\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nbfplot <- data.frame(\n  prior = rnorm(1e5, yorig, se),\n  posterior = rnorm(1e5, fit$coefficients, fit$ses)\n)\n \nplt <- ggplot() +\n  stat_function(geom = \"line\", \n                aes(color = \"Original Study (Prior)\"),\n                linewidth = 1,\n                alpha = 0.3,\n                fun = dnorm, args = list(mean = yorig, sd = se)) +\n  stat_function(geom = \"line\",\n                linewidth = 1,\n                aes(color = \"Replication Study (Posterior)\"),\n                fun = dnorm, args = list(mean = fit$coefficients, sd = fit$ses)) +\n  xlim(c(-0.5, 1.2)) +\n  geom_point(aes(x = c(0, 0), y = c(dnorm(0, yorig, sd = se),\n                                    dnorm(0, fit$coefficients, sd = fit$ses))),\n             size = 3) +\n  xlab(latex2exp::TeX(\"\\\\delta\")) +\n  ylab(\"Density\") +\n  theme(legend.position = \"bottom\",\n        legend.title = element_blank())\n```\n:::\n\n\n\n\n## Example\n\nA better custom plot:\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](chapter4_files/figure-html/unnamed-chunk-56-1.svg){fig-align='center' width=672}\n:::\n:::\n\n\n\n\n## References\n",
    "supporting": [
      "chapter4_files/figure-html"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}