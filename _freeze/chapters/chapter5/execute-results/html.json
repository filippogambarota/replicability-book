{
  "hash": "c677205c9df1f98c24afeb53997bd29a",
  "result": {
    "engine": "knitr",
    "markdown": "---\nexecute: \n  echo: true\nknitr:\n  opts_chunk: \n    collapse: true\n    comment: \"#>\"\n---\n\n\n\n\n\n# Meta-analysis and publication bias\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Meta-analysis {.section}\n\n## Meta-analysis\n\n- The meta-analysis is a statistical procedure to combine evidence from a group of studies.\n\n. . .\n\n- The idea is to \"switch\" the statistical unit from e.g., participants to studies\n\n. . .\n\n- The motto could be that (appropriately) combining similar studies with a similar aim is the best way to understand something about a phenomenon\n\n## Meta-analysis and Systematic Review {.smaller}\n\nUsually a meta-analysis work follows these steps:\n\n1. **Identify the research question**: is the treatment *x* effective?, Does the experimental effect *y* exist?\n2. **Define inclusion/exclusion criteria**: From the research question (1), keep only e.g., randomized controlled trials, studies with healthy participants, etc.\n3. **Systematically search for studies**: Analyze the literature to find all relevant studies\n4. **Extract relevant information**: Read, extract and organize relevant information e.g., sample size, treatment type, age, etc.\n5. **Summarize the results**: Create a narrative (flowcharts, tables, etc.) summary of included studies. This is the Systematic review part.\n6. **Choose an effect size**: Choose a way to standardize the effect across included studies\n7. **Meta-analysis model**: Choose and implement a meta-analysis model\n8. **Interpret and report results**\n\n## Before the fun part...\n\n::: {.incremental}\n- We are dealing only with the **statistical part**. The study selection, data extraction, studies evaluation etc. is another story\n- The quality of the meta-analysis is the **quality of included studies**\n:::\n\n. . .\n\n![](img/gigo.png){fig-align=\"center\" width=60%}\n\n## Unstandardized effect sizes\n\nThe basic idea of an effect size is just using the raw measure. For example studies using reaction times we can calculate the difference between two conditions as $\\overline X_1 - \\overline X_2$:\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](chapter5_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\n\n\n\n## Unstandardized effect sizes\n\nBut another study (with the same research question) could use another measure, e.g., accuracy. We can still (not the best strategy but) compute the difference between the group means.\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](chapter5_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n\n\n\n\n\n## Unstandardized effect sizes\n\nClearly we cannot directly compare the two effects but we need to standardize the measure.\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](chapter5_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\n\n\n\n## Standardized effect sizes\n\nTo compare results from different studies, we should use a common metric. Frequently meta-analysts use *standardized* effect sizes. For example the Pearson correlation or the Cohen's $d$.\n\n$$\nr = \\frac{\\sum{(x_i - \\bar{x})(y_i - \\bar{y})}}{\\sqrt{\\sum{(x_i - \\bar{x})^2}\\sum{(y_i - \\bar{y})^2}}}\n$$ {#eq-correlation}\n\n$$\nd = \\frac{\\bar{x_1} - \\bar{x_2}}{s_p}\n$$\n\n$$\ns_p = \\sqrt{\\frac{(n_1 - 1)s_1^2 + (n_2 - 1)s_2^2}{n_1 + n_2 - 2}}\n$$\n\n## Standardized effect sizes\n\nThe advantage of standardized effect size is that regardless the original variable, the interpretation and the scale is the same. For example the pearson correlation ranges between -1 and 1 and the Cohen's $d$ between $- \\infty$ and $\\infty$ and is interpreted as how many standard deviations the two groups/conditions differs.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nS <- matrix(c(1, 0.7, 0.7, 1), nrow = 2)\nX <- MASS::mvrnorm(100, c(0, 2), S, empirical = TRUE)\n\npar(mfrow = c(1,2))\nplot(X, xlab = \"x\", ylab = \"y\", cex = 1.3, pch = 19,\n     cex.lab = 1.2, cex.axis = 1.2,\n     main = latex2exp::TeX(sprintf(\"$r = %.2f$\", cor(X[, 1], X[, 2]))))\nabline(lm(X[, 2] ~ X[, 1]), col = \"firebrick\", lwd = 2)\n\n\nplot(density(X[, 1]), xlim = c(-5, 7), ylim = c(0, 0.5), col = \"dodgerblue\", lwd = 2,\n     main = latex2exp::TeX(sprintf(\"$d = %.2f$\", lsr::cohensD(X[, 1], X[, 2]))),\n     xlab = \"\")\nlines(density(X[, 2]), col = \"firebrick\", lwd = 2)\n```\n\n::: {.cell-output-display}\n![](chapter5_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\n\n\n\n\n## Standardized vs unstandardized\n\nThe main difference is (usually) the absence of a effect-size-variance relationship for unstandardized effects. For example, the variance of the difference between two groups is:\n\n$$\nV_d = \\frac{s^2_1}{n_1} + \\frac{s^2_2}{n_2}\n$$ {#eq-var-umd}\n\nWhile the variance of a Cohen's $d$ can be calculated as:\n\n$$\nV_d = \\frac{n_1 + n_2}{n_1 n_2} + \\frac{d^2}{2(n_1 + n_2)}\n$$\n\n## Standardized vs unstandardized\n\nIn this [amazing blog post](https://www.jepusto.com/alternative-formulas-for-the-smd/) James Pustejovsky explained where the equations comes from. Basically, the $\\frac{n_1 + n_2}{n_1 n_2}$ term is the same as the $\\frac{s^2_1}{n_1} + \\frac{s^2_2}{n_2}$ while the extra $\\frac{d^2}{2(n_1 + n_2)}$ is for the non-centrality induced by the standardized difference.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nn <- c(10, 50, 100)\nd <- seq(0, 2, 0.001)\n\ndd <- expand.grid(n = n, d = d)\n\ndd$vumd <- with(dd, 1/n + 1/n)\ndd$vd <- with(dd, (n + n) / (n * n) + d^2/(2 * (n + n)))\n\ntidyr::pivot_longer(dd, 3:4) |> \n  ggplot(aes(x = d, y = value, color = name, linetype = factor(n))) +\n  geom_line() +\n  labs(linetype = \"Sample Size\",\n       color = NULL)\n```\n\n::: {.cell-output-display}\n![](chapter5_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n\n\n\n\n## Effect size sampling variability {#sec-effsize-se}\n\nCrucially, we can calculate also the **sampling variability** of each effect size. The **sampling variability** is the precision of estimated value.\n\nFor example, there are multiple methods to estimate the Cohen's $d$ sampling variability. For example:\n\n$$\nV_d = \\frac{n_1 + n_2}{n_1 n_2} + \\frac{d^2}{2(n_1 + n_2)}\n$$\n\nEach effect size has a specific formula for the sampling variability. The sample size is usually the most important information. Studies with high sample size have low sampling variability.\n\n## Effect size sampling variability\n\nAs the sample size grows and tends to infinity, the sampling variability approach zero.\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](chapter5_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\n\n\n\n\n## Unstandardized effect sizes\n\nFor the examples and plots I'm going to use simulated data. We simulate *unstandardized* effect sizes (UMD) because the computations are easier and the estimator is unbiased [e.g., @Viechtbauer2005-zt]\n\nMore specifically we simulate hypothetical studies where two independent groups are compared:\n\n$$\n\\Delta = \\overline{X_1} - \\overline{X_2}\n$$ {#eq-umd}\n\n$$\nSE_{\\Delta} = \\sqrt{\\frac{s^2_1}{n_1} + \\frac{s^2_2}{n_2}}\n$$\n\nWith $X_{1_i} \\sim \\mathcal{N}(0, 1)$ and $X_{2_i} \\sim \\mathcal{N}(\\Delta, 1)$\n\nThe main advantage is that, compared to standardized effect size, the sampling variability do not depends on the effect size itself, simplifying the computations.\n\n# Simulating a single study {.section}\n\n## Simulating a single study - UMD\n\nTo simulate a single study using a UMD we need to generate data according to the appropriate model. Here we have a difference between two groups. We can assume that the two groups comes from a normal distribution where group 1 $g_1 \\sim \\mathcal{N}(0, 1)$ and group 2 $g_2 \\sim \\mathcal{N}(D, 1)$ where $D$ is the effect size. Then using Equations [-@eq-var-umd; -@eq-umd] we can estimate the effect size and the variance.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nD <- 1  # effect size\nn <- 50 # sample size\ng1 <- rnorm(n, mean = 0, sd = 1)\ng2 <- rnorm(n, mean = D, sd = 1)\n\n# effect size\nmean(g2) - mean(g1)\n#> [1] 0.8988085\n\n# variance\nvar(g1)/n + var(g2)/n\n#> [1] 0.0381312\n```\n:::\n\n\n\n\n\n## Simulating a single study - UMD\n\nFor simplicity we can wrap everything within a function:\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# default sd = 1\nsim_umd <- function(n1, n2 = NULL, D, sd = 1){\n  if(is.null(n2)) n2 <- n1 # same to n1 if null \n  g1 <- rnorm(n1, mean = 0, sd = sd)\n  g2 <- rnorm(n2, mean = D, sd = sd)\n  yi <- mean(g2) - mean(g1)\n  vi <- var(g1)/n1 + var(g2)/n2\n  data.frame(yi, vi)\n}\n\nsim_umd(100, D = 0.5)\n#>          yi         vi\n#> 1 0.4114704 0.02041681\nsim_umd(50, D = 0.1)\n#>           yi         vi\n#> 1 -0.1741297 0.04320926\n```\n:::\n\n\n\n\n\n## Simulating a single study - UMD\n\nWe can also generate a large number of studies and check the distribution of effect size and sampling variances. Note that the real $D = 1$ and the real variance $V_D = 1/50 + 1/50 = 0.04$\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstudies <- replicate(1000, sim_umd(n1 = 50, D = 1), simplify = FALSE) # simplify = FALSE return a list\nstudies <- do.call(rbind, studies) # to dataframe\nhead(studies)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#>          yi         vi\n#> 1 1.0498116 0.03577581\n#> 2 0.6682341 0.03041130\n#> 3 0.9836933 0.03880497\n#> 4 1.0205462 0.04067314\n#> 5 1.2316595 0.03710892\n#> 6 1.2021872 0.03864621\n```\n\n\n:::\n:::\n\n\n\n\n\n## Simulating a single study - UMD {#sec-umd-sampling-distribution}\n\nThen we can plot the sampling distributions:\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](chapter5_files/figure-html/unnamed-chunk-12-1.png){width=672}\n:::\n:::\n\n\n\n\n\n## Simulating a single study - SMD\n\nThe idea is the same when simulating a SDM but we need extra steps. Let's adjust the previous function:\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsim_smd <- function(n1, n2 = NULL, D){\n  if(is.null(n2)) n2 <- n1 # same to n1 if null \n  g1 <- rnorm(n1, mean = 0, sd = 1)\n  g2 <- rnorm(n2, mean = D, sd = 1)\n  \n  v1 <- var(g1)\n  v2 <- var(g2)\n  \n  # pooled standard deviation\n  sp <- sqrt((v1 * (n1 - 1) + v2 * (n2 - 1)) / (n1 + n2 - 2))\n  \n  yi <- (mean(g2) - mean(g1)) / sp\n  vi <- (n1 + n2) / (n1 * n2) + yi^2/(2*(n1 + n2))\n  data.frame(yi, vi)\n}\n```\n:::\n\n\n\n\n\n## Simulating a single study - SMD\n\nWhen working with SMD, calculating the sampling variance can be challenging. @Veroniki2016-nw identified 16 different estimators with different properties. Furthermore, it is a common practice to correct the SDM effect and variance using the Hedges's correction [@Hedges1989-ip]. \n\nYou can directly implement another equation for the sampling variance or the Hedges's correction directly in the simulation function.\n\n## Simulating a single study - Pearson $\\rho$\n\nAnother common effect size is the Pearson correlation coefficient $\\rho$ (and the estimate $r$, see @eq-correlation). The variance of the correlation is calculated as:\n\n$$\nV_{r} = \\frac{(1 - r^2)^2}{n - 1}\n$$\n\n## Simulating a single study - Pearson $\\rho$\n\nThere is a huge dependency between $r$ and it's sampling variance (similar to the Cohen's $d$):\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nn <- 50\nr <- seq(0, 1, 0.01)\nv <- (1 - r^2)^2 / (n - 1) \n\nplot(r, v, type = \"l\", main = \"N = 50\", xlab = \"r\", ylab = latex2exp::TeX(\"$V_r$\"))\n```\n\n::: {.cell-output-display}\n![](chapter5_files/figure-html/unnamed-chunk-14-1.png){width=672}\n:::\n:::\n\n\n\n\n\n## Simulating a single study - Pearson $\\rho$\n\nFor this reason the so-called Fisher's $z$ transformation is used to stabilize the relationship.\n\n$$\nz = \\frac{\\log{\\frac{1 + r}{1 - r}}}{2}\n$$\n\n$$\nV_z = \\frac{1}{n - 3}\n$$\n\nNow the variance is completely independent from the correlation value.\n\n## Simulating a single study - Pearson $\\rho$\n\nThis is the relationship between $r$ and $z$:\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nn <- 50\nr <- seq(-1, 1, 0.01)\nv <- (1 - r^2)^2 / (n - 1) \nz <- log((1 + r)/(1 - r))/2\n\nplot(z, r, type = \"l\", xlab = \"Fisher's z\", ylab = \"Correlation\", main = \"Correlation to Fisher's z\")\n```\n\n::: {.cell-output-display}\n![](chapter5_files/figure-html/unnamed-chunk-15-1.png){width=672}\n:::\n:::\n\n\n\n\n\n## Simulating a single study - Pearson $\\rho$\n\nTo simulate a study using correlations we can use the `MASS::mvrnorm()` function that can generate correlated data from a multivariate normal distribution.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsim_r <- function(n, r){\n  R <- r + diag(1 - r, nrow = 2) # 2 x 2 correlation matrix\n  X <- MASS::mvrnorm(n, mu = c(0, 0), Sigma = R) # the means are not relevant here\n  r <- cor(X)[1, 2] # extract correlation\n  vr <- (1 - r^2)^2 / (n - 1)  # variance of r\n  yi <- log((1 + r)/(1 - r))/2 # fisher z\n  vi <- 1 / (n - 3) # fisher z variance\n  data.frame(yi, vi, r, vr) # including also the pearson correlation and variance\n}\n```\n:::\n\n\n\n\n\n## Simulating a single study - Pearson $\\rho$\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsim_r(100, 0.5)\n#>          yi         vi         r          vr\n#> 1 0.5132608 0.01030928 0.4724819 0.006094519\nsim_r(50, 0.8)\n#>         yi        vi         r          vr\n#> 1 1.195983 0.0212766 0.8324254 0.001924302\n\n# also here the sampling distributions\nstudies <- replicate(1000, sim_r(50, 0.7), simplify = FALSE)\nstudies <- do.call(rbind, studies)\nsummary(studies)\n#>        yi               vi                r                vr          \n#>  Min.   :0.4563   Min.   :0.02128   Min.   :0.4270   Min.   :0.001173  \n#>  1st Qu.:0.7820   1st Qu.:0.02128   1st Qu.:0.6539   1st Qu.:0.003783  \n#>  Median :0.8770   Median :0.02128   Median :0.7049   Median :0.005166  \n#>  Mean   :0.8816   Mean   :0.02128   Mean   :0.7001   Mean   :0.005397  \n#>  3rd Qu.:0.9836   3rd Qu.:0.02128   3rd Qu.:0.7546   3rd Qu.:0.006688  \n#>  Max.   :1.3410   Max.   :0.02128   Max.   :0.8719   Max.   :0.013643\n```\n:::\n\n\n\n\n\n## More on effect sizes\n\nThe same logic can be applied to any situation. Just understand the data generation process, find the effect size equations and generate data.\n\n- @Borenstein2009-mo for all effect sizes equations. Also with equations to convert among effect sizes (useful in real-world meta-analyses)   \n- the [`metafor::escalc()`](https://wviechtb.github.io/metafor/reference/escalc.html) function implements basically any effect size. You can see also the [source code](https://github.com/wviechtb/metafor/blob/master/R/escalc.r) to see the actual R implementation.\n- [Guide to effect sizes](https://matthewbjane.quarto.pub/guide-to-effect-sizes-and-confidence-intervals): a modern and complete overview of effect sizes\n\n## Simulating from sampling distributions [#extra]\n\nThe previous simulation examples are participant-level simulations. In fact we simulated $n$ observations then we aggregated calculating the effect sizes.\n\n. . .\n\nThis is the most flexible and general data simulation strategy but is computationally not efficient.\n\n. . .\n\nAnother strategy individuate the exact effect size sampling distribution. Then we can sample directly from it. The downside is that we need to derive (or find) the equation.\n\n## Simulating from sampling distributions [#extra]\n\nFor example, when generating UMD we can simulate from the sampling distribution presented in @sec-umd-sampling-distribution.\n\n$$\ny_i \\sim \\mathcal{N}(\\theta, \\sqrt{\\sigma^2_i})\n$$\n$$\n\\sigma^2_i \\sim \\frac{\\chi^2_{n_1 + n_2 - 2}}{n_1 + n_2 - 2} (\\frac{1}{n_1} + \\frac{1}{n_2})\n$$\n\nIn this way we can sample $k$ effects and sampling variances directly from the sampling distributions. Without generating data and then aggregate.\n\n## Simulating from sampling distributions [#extra]\n\nWe can again put everything within a function:\n\n```r\nsim_k_umd <- function(k, D, n1, n2 = NULL){\n  if(is.null(n2)) n2 <- n1\n  yi <- rnorm(k, D, sqrt(1/n1 + 1/n2))\n  vi <- (rchisq(k, n1 + n2 - 2) / (n1 + n2 - 2)) * (1/n1 + 1/n2)\n  data.frame(yi, vi)\n}\n```\n\n## Simulating from sampling distributions [#extra]\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsim_k_umd(k = 10, D = 0.5, n1 = 50)\n#>           yi         vi\n#> 1  0.5542286 0.04302018\n#> 2  0.7837123 0.03991671\n#> 3  0.3708463 0.04113782\n#> 4  0.6159170 0.03526492\n#> 5  0.6729047 0.03892563\n#> 6  0.4946377 0.03992805\n#> 7  0.5045990 0.03664000\n#> 8  0.3628192 0.04725545\n#> 9  0.2318982 0.04106268\n#> 10 0.6832070 0.05237131\n```\n:::\n\n\n\n\n\n## Simulating from sampling distributions [#extra]\n\nWe can compare the two methods and see that we are sampling from the same data generation process.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nk <- 1e4\ns_umd <- sim_k_umd(k, D = 1, n1 = 50)\nip_umd <- replicate(k, sim_umd(n1 = 50, D = 1), simplify = FALSE)\nip_umd <- do.call(rbind, ip_umd)\n```\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![](chapter5_files/figure-html/unnamed-chunk-20-1.png){width=672}\n:::\n:::\n\n\n\n\n\n## Simulating from sampling distributions [#extra]\n\nThe actual advantage is in terms of computational speed. To simulate $k = 10$ studies for 1000 times (similar to a standard Monte Carlo simulation):\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbench <- microbenchmark::microbenchmark(\n  sampling = sim_k_umd(k = 10, n1 = 50, D = 1),\n  participant = replicate(10, sim_umd(n1 = 50, D = 1)),\n  times = 1000 \n)\n\n(bench <- summary(bench))\n#>          expr      min        lq      mean   median       uq      max neval cld\n#> 1    sampling  119.875  126.2125  132.4143  130.801  136.913  393.049  1000  a \n#> 2 participant 1533.363 1570.9390 1629.5997 1588.637 1605.914 6118.904  1000   b\n\nbench$mean[2] / bench$mean[1] # faster\n#> [1] 12.30682\n```\n:::\n\n\n\n\n\n# Simulation setup {.section}\n\n## Notation {.smaller}\n\nMeta-analysis notation is a little bit inconsistent in textbooks and papers. We define here some rules to simplify the work.\n\n- $k$ is the number of studies\n- $n_j$ is the sample size of the group $j$ within a study\n- $y_i$ are the observed effect size included in the meta-analysis\n- $\\sigma_i^2$ are the observed sampling variance of studies and $\\epsilon_i$ are the sampling errors\n- $\\theta$ is the equal-effects parameter (see @eq-ee1)\n- $\\delta_i$ is the random-effect (see @eq-re-mod2)\n- $\\mu_\\theta$ is the average effect of a random-effects model (see @eq-re-mod1)\n- $w_i$ are the meta-analysis weights\n- $\\tau^2$ is the heterogeneity (see @eq-re-mod2)\n- $\\Delta$ is the (generic) population effect size\n- $s_j^2$ is the variance of the group $j$ within a study\n\n## Simulation setup\n\nGiven the introduction to effect sizes, from now we will simulate data using UMD and the individual-level data. \n\nBasically we are simulating an effect size $D$ coming from the comparison of two independent groups $G_1$ and $G_2$.\n\nEach group is composed by $n$ participants measured on a numerical outcome (e.g., reaction times)\n\n## Simulation setup\n\nA more general, clear and realistic approach to simulate data is by generating $k$ studies with same/different sample sizes and (later) true effect sizes.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nk <- 10 # number of studies\nn1 <- n2 <- 10 + rpois(k, 30 - 10) # sample size from poisson distribution with lambda 40 and minimum 10\nD <- 0.5 # effect size\n\nyi <- rep(NA, k)\nvi <- rep(NA, k)\n  \nfor(i in 1:k){\n  g1 <- rnorm(n1[i], 0, 1)\n  g2 <- rnorm(n2[i], D, 1)\n  yi[i] <- mean(g2) - mean(g1)\n  vi[i] <- var(g1)/n1[i] + var(g2)/n2[i]\n}\n  \nsim <- data.frame(id = 1:k, yi, vi)\n\nhead(sim)\n#>   id        yi         vi\n#> 1  1 0.6161948 0.05161202\n#> 2  2 0.3942969 0.07989387\n#> 3  3 0.6223084 0.07811994\n#> 4  4 0.2317775 0.09670337\n#> 5  5 0.2699480 0.04687939\n#> 6  6 0.6029536 0.08060074\n```\n:::\n\n\n\n\n\n## Simulation setup\n\nWe can again put everything within a function:\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsim_studies <- function(k, es, n1, n2 = NULL){\n  if(length(n1) == 1) n1 <- rep(n1, k)\n  if(is.null(n2)) n2 <- n1\n  if(length(es) == 1) es <- rep(es, k)\n  \n  yi <- rep(NA, k)\n  vi <- rep(NA, k)\n  \n  for(i in 1:k){\n    g1 <- rnorm(n1[i], 0, 1)\n    g2 <- rnorm(n2[i], es[i], 1)\n    yi[i] <- mean(g2) - mean(g1)\n    vi[i] <- var(g1)/n1[i] + var(g2)/n2[i]\n  }\n  \n  sim <- data.frame(id = 1:k, yi, vi, n1 = n1, n2 = n2)\n  \n  # convert to escalc for using metafor methods\n  sim <- metafor::escalc(yi = yi, vi = vi, data = sim)\n  \n  return(sim)\n}\n```\n:::\n\n\n\n\n\n## Simulation setup - Disclaimer\n\nThe proposed simulation approach using a `for` loop and separated vectors. For the purpose of the workshop this is the best option. In real-world meta-analysis simulations you can choose a more functional approach starting from a simulation grid as `data.frame` and mapping the simulation functions.\n\nFor some examples see:\n\n- @Gambarota2023-on\n- [www.jepusto.com/simulating-correlated-smds](https://www.jepusto.com/simulating-correlated-smds)\n\n## Simulation setup - Disclaimer\n\nFor a more extended overview of the simulation setup we have an entire paper. Supplementary materials ([github.com/shared-research/simulating-meta-analysis](https://github.com/shared-research/simulating-meta-analysis)) contains also more examples for complex (multilevel and multivariate models.)\n\n![](img/gambarota2023.png){fig-align=\"center\"}\n\n# Combining studies {.section}\n\n## Combining studies\n\nLet's imagine to have $k = 10$ studies, a $D = 0.5$ and heterogeneous sample sizes in each study.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nk <- 10\nD <- 0.5\nn <- 10 + rpois(k, lambda = 20) \ndat <- sim_studies(k = k, es = D, n1 = n)\nhead(dat)\n#> \n#>   id      yi     vi n1 n2 \n#> 1  1 -0.0200 0.0826 32 32 \n#> 2  2 -0.0533 0.1244 21 21 \n#> 3  3  0.8737 0.0524 31 31 \n#> 4  4  0.6142 0.0664 29 29 \n#> 5  5  0.2540 0.0510 37 37 \n#> 6  6  0.5867 0.0511 29 29\n```\n:::\n\n\n\n\n\n. . .\n\nWhat is the best way to combine the studies?\n\n## Combining studies\n\nWe can take the average effect size and considering it as a huge study. This can be considered the best way to combine the effects.\n\n$$\n\\hat{D} = \\frac{\\sum^{k}_{i = 1} D_i}{k}\n$$\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmean(dat$yi)\n#> [1] 0.3874658\n```\n:::\n\n\n\n\n\n. . .\n\nIt is appropriate? What do you think? Are we missing something?\n\n## Weighting studies\n\nWe are not considering that some studies, despite providing a similar effect size could give more information. An higher sample size (or lower sampling variance) produce a more reliable estimation.\n\n. . .\n\nWould you trust more a study with $n = 100$ and $D = 0.5$ or a study with $n = 10$ and $D = 0.5$? The \"meta-analysis\" that we did before is completely ignoring this information.\n\n## Weighting studies\n\nWe need to find a value (called weight $w_i$) that allows assigning more trust to a study because it provide more information. \n\n. . .\n\nThe simplest weights are just the sample size, but in practice we use the so-called **inverse-variance weighting**. We use the (inverse) of the sampling variance of the effect size to weight each study. \n\n. . .\n\nThe basic version of a meta-analysis is just a **weighted average**:\n\n$$\n\\overline D_w = \\frac{\\sum^k_{i = 1}{w_iD_i}}{\\sum^k_{i = 1}{w_i}}\n$$\n\n. . .\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwi <- 1/dat$vi\nsum(dat$yi * wi) / sum(wi)\n#> [1] 0.41158\n# weighted.mean(dat$yi, wi)\n```\n:::\n\n\n\n\n\n## Weighting studies\n\nGraphically, the two models can be represented in this way:\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\ndw <- weighted.mean(dat$yi, 1/dat$vi)\ndunw <- mean(dat$yi)\n\nunw_plot <- ggplot(dat, aes(x = yi, y = factor(id))) +\n  geom_point(size = 3) +\n  xlim(c(-0.5, 1.5)) +\n  geom_vline(xintercept = dunw) +\n  xlab(latex2exp::TeX(\"$y_i$\")) +\n  ylab(\"Study\") +\n  theme_minimal(15) +\n  annotate(\"label\", x = 0.5, y = k + 0.4, label = latex2exp::TeX(sprintf(\"$\\\\bar{D} = %.2f$\", dunw))) +\n  geom_label(aes(x = 1, y = id, label = paste0(\"n = \", n1)))\n\nw_plot <- ggplot(dat, aes(x = yi, y = factor(id))) +\n  geom_point(aes(size = 1/vi),\n             show.legend = FALSE) +\n  xlim(c(-0.5, 1.5)) +\n  geom_vline(xintercept = dw) +\n  xlab(latex2exp::TeX(\"$y_i$\")) +\n  ylab(\"Study\") +\n  theme_minimal(15) +\n  annotate(\"label\", x = 0.5, y = k + 0.4, label = latex2exp::TeX(sprintf(\"$\\\\bar{D}_w = %.2f$\", dw))) +\n  geom_label(aes(x = 1, y = id, label = paste0(\"n = \", n1)))\n\nunw_plot + w_plot\n```\n\n::: {.cell-output-display}\n![](chapter5_files/figure-html/unnamed-chunk-27-1.png){width=672}\n:::\n:::\n\n\n\n\n\n# Equal-effects (EE) meta-analysis {.section}\n\n## EE meta-analysis\n\nWhat we did in the last example (the weighted mean) is the exactly a meta-analysis model called **equal-effects** (or less precisely fixed-effect). The assumptions are very simple:\n\n- there is a unique, true effect size to estimate $\\theta$\n- each study is a more or less precise estimate of $\\theta$\n- there is no TRUE variability among studies. The observed variability is due to studies that are imprecise (i.e., sampling error)\n- assuming that each study has a very large sample size, the observed variability is close to zero.\n\n## EE meta-analysis, formally\n\n$$\ny_i = \\theta + \\epsilon_i\n$$ {#eq-ee1}\n\n$$\n\\epsilon_i \\sim \\mathcal{N}(0, \\sigma^2_i)\n$$ {#eq-ee2}\n\nWhere $\\sigma^2_i$ is the vector of sampling variabilities of $k$ studies. This is a standard linear model but with heterogeneous sampling variances.\n\n## EE meta-analysis\n\n\n\n\n\n::: {.cell}\n\n:::\n\n\n\n\n\n<!-- TODO fix the image, ggplot version -->\n\n## Simulating an EE model\n\nWhat we were doing with the `sim_studies()` function so far was simulating an EE model. In fact, there were a single $\\theta$ parameter and the observed variability was a function of the `rnorm()` randomness.\n\nBased on previous assumptions and thinking a little bit, what could be the result of simulating studies with a very large $n$?\n\n. . .\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nns <- c(10, 50, 100, 1000, 1e4)\nD <- 0.5\ndats <- lapply(ns, function(n) sim_studies(10, es = D, n1 = n))\n```\n:::\n\n\n\n\n\n## Simulating an EE modelm {#sec-ee-impact-n}\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](chapter5_files/figure-html/unnamed-chunk-30-1.png){width=1440}\n:::\n:::\n\n\n\n\n\n## Simulating an EE model\n\nFormulating the model as a intercept-only regression (see Equations [@eq-ee1] and [@eq-ee2]) we can generate data directly:\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nD <- 0.5\nn <- 30\nk <- 10\n\nyi <- D + rnorm(k, 0, sqrt(1/n + 1/n))\n# or equivalently\n# yi <- rnorm(k, D, sqrt(1/n + 1/n))\n```\n:::\n\n\n\n\n\nAs we did for the aggregated data approach. Clearly we need to simulate also the `vi` vector from the appropriate distribution. Given that we simulated data starting from the participant-level the uncertainty of `yi` and `vi` is already included.\n\n## Fitting an EE model\n\nThe model can be fitted using the `metafor::rma()` function, with `method = \"EE\"`^[There is a confusion about the *fixed-effects* vs *fixed-effect* (no *s*) and *equal-effects* models. See [https://wviechtb.github.io/metafor/reference/misc-models.html](https://wviechtb.github.io/metafor/reference/misc-models.html)].\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntheta <- 0.5\nk <- 15\nn <- 10 + rpois(k, 30 - 10)\ndat <- sim_studies(k = k, es = theta, n1 = n)\nfit <- rma(yi, vi, data = dat, method = \"EE\")\nsummary(fit)\n#> \n#> Equal-Effects Model (k = 15)\n#> \n#>   logLik  deviance       AIC       BIC      AICc   \n#>   2.4968    8.3471   -2.9936   -2.2855   -2.6859   \n#> \n#> I^2 (total heterogeneity / total variability):   0.00%\n#> H^2 (total variability / sampling variability):  0.60\n#> \n#> Test for Heterogeneity:\n#> Q(df = 14) = 8.3471, p-val = 0.8705\n#> \n#> Model Results:\n#> \n#> estimate      se    zval    pval   ci.lb   ci.ub      \n#>   0.5999  0.0647  9.2691  <.0001  0.4731  0.7268  *** \n#> \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n\n\n\n\n\n## Interpreting an EE model\n\n- The first section (`logLik`, `deviance`, etc.) presents some general model statistics and information criteria\n- The $I^2$ and $H^2$ are statistics evaluating the observed heterogeneity (see next slides)\n- The `Test of Heterogeneity` section presents the test of the $Q$ statistics for the observed heterogeneity (see next slides)\n- The `Model Results` section presents the estimation of the $\\theta$ parameter along with the standard error and the Wald $z$ test ($H_0: \\theta = 0$)\n\nThe `metafor` package has a several well documented functions to calculate and plot model results, residuals analysis etc.\n\n## Interpreting an EE model\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(fit) # general plots\n```\n\n::: {.cell-output-display}\n![](chapter5_files/figure-html/unnamed-chunk-33-1.png){width=672}\n:::\n:::\n\n\n\n\n\n## Interpreting an EE Model\n\nThe main function for plotting model results is the `forest()` function that produce the forest plot.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nforest(fit)\n```\n\n::: {.cell-output-display}\n![](chapter5_files/figure-html/unnamed-chunk-34-1.png){width=672}\n:::\n:::\n\n\n\n\n\n## Interpreting an EE Model\n\nWe did not introduced the concept of heterogeneity, but the $I^2$, $H^2$ and $Q$ statistics basically evaluate if the observed heterogeneity should be attributed to **sampling variability** (uncertainty in estimating $\\theta$ because we have a limited $k$ and $n$) or **sampling variability** plus other sources of heterogeneity.\n\n## EE model as a weighted Average\n\nFormally $\\theta$ is estimated as [see @Borenstein2009-mo, p. 66]\n\n$$\n\\hat{\\theta} = \\frac{\\sum^k_{i = 1}{w_iy_i}}{\\sum^k_{i = 1}{w_i}}; \\;\\;\\; w_i = \\frac{1}{\\sigma^2_i}\n$$\n\n$$\nSE_{\\theta} = \\frac{1}{\\sum^k_{i = 1}{w_i}}\n$$\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwi <- 1/dat$vi\ntheta_hat <- with(dat, sum(yi * wi)/sum(wi))\nse_theta_hat <- sqrt(1/sum(wi))\nc(theta = theta_hat, se = se_theta_hat, z = theta_hat / se_theta_hat)\n#>     theta        se         z \n#> 0.5999291 0.0647239 9.2690513\n```\n:::\n\n\n\n\n\n# Random-effects (RE) meta-analysis {.section}\n\n## Are the EE assumptions realistic?\n\nThe EE model is appropriate if our studies are somehow **exact replications** of the exact same effect. We are assuming that there is **no real variability**.\n\n. . .\n\nHowever, meta-analysis rarely report the results of $k$ exact replicates. It is more common to include **studies answering the same research question** but with different methods, participants, etc.\n\n. . .\n\n- people with different ages or other participant-level differences\n- different methodology\n- ...\n\n## Are the EE assumptions realistic?\n\n. . .\n\nIf we relax the previous assumption we are able to combine studies that are not exact replications. \n\n. . .\n\nThus the real effect $\\theta$ is no longer a single **true** value but can be larger or smaller in some conditions.\n\n. . .\n\nIn other terms we are assuming that there could be some variability (i.e., **heterogeneity**) among studies that is independent from the sample size. Even with studies with $\\lim_{n\\to\\infty}$ the observed variability is not zero.\n\n## Random-effects model (RE)\n\nWe can extend the EE model including another source of variability, $\\tau^2$. $\\tau^2$ is the true heterogeneity among studies caused by methdological differences or intrisic variability in the phenomenon.\n\nFormally we can extend @eq-ee1 as:\n$$\ny_i = \\mu_{\\theta} + \\delta_i + \\epsilon_i\n$$ {#eq-re-mod1}\n\n$$\n\\delta_i \\sim \\mathcal{N}(0, \\tau^2)\n$$ {#eq-re-mod2}\n\n$$\n\\epsilon_i \\sim \\mathcal{N}(0, \\sigma^2_i)\n$$\n\nWhere $\\mu_{\\theta}$ is the average effect size and $\\delta_i$ is the study-specific deviation from the average effect (regulated by $\\tau^2$). Clearly each study specific effect is $\\theta_i = \\mu_{\\theta} + \\delta_i$.\n\n## RE model\n\n\n\n\n\n::: {.cell}\n\n:::\n\n\n\n\n\n<!-- TODO fix the image, ggplot version -->\n\n## RE model estimation\n\nGiven that we extended the EE model equation. Also the estimation of the average effect need to be extended. Basically the RE is still a weighted average but weights need to include also $\\tau^2$.\n\n$$\n\\overline y = \\frac{\\sum_{i = 1}^k y_iw^*_i}{\\sum_{i = 1}^k w^*_i}\n$$ {#eq-re1}\n\n$$\nw^*_i = \\frac{1}{\\sigma^2_i + \\tau^2}\n$$ {#eq-re2}\n\nThe weights are different compared to the EE model. Extremely precise/imprecise studies will have less impact in the RE model.\n\n## RE vs EE model\n\nThe crucial difference with the EE model is that even with large $n$, only the $\\mu_{\\theta} + \\delta_i$ are estimated (almost) without error. As long $\\tau^2 \\neq 0$ there will be variability in the effect sizes.\n\n\n\n\n\n::: {.cell}\n\n:::\n\n\n\n\n\n<!-- TODO fix the image, ggplot version -->\n\n## Simulating a RE Model\n\nTo simulate the RE model we simply need to include $\\tau^2$ in the EE model simulation.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nk <- 15 # number of studies\nmu <- 0.5 # average effect\ntau2 <- 0.1 # heterogeneity\nn <- 10 + rpois(k, 30 - 10) # sample size\ndeltai <- rnorm(k, 0, sqrt(tau2)) # random-effects\nthetai <- mu + deltai # true study effect\n\ndat <- sim_studies(k = k, es = thetai, n1 = n)\n\nhead(dat)\n#> \n#>   id     yi     vi n1 n2 \n#> 1  1 0.4462 0.0498 37 37 \n#> 2  2 1.0966 0.0641 31 31 \n#> 3  3 0.6987 0.0676 32 32 \n#> 4  4 0.7012 0.0517 33 33 \n#> 5  5 0.2842 0.0767 27 27 \n#> 6  6 0.2323 0.0703 31 31\n```\n:::\n\n\n\n\n\n## Simulating a RE model\n\nAgain, we can put everything within a function expanding the previous `sim_studies()` by including $\\tau^2$:\n\n\n\n\n\n\n\n\n\n\n\n```r\nsim_studies <- function(k, es, tau2 = 0, n1, n2 = NULL, add = NULL){\n  if(length(n1) == 1) n1 <- rep(n1, k)\n  if(is.null(n2)) n2 <- n1\n  if(length(es) == 1) es <- rep(es, k)\n  \n  yi <- rep(NA, k)\n  vi <- rep(NA, k)\n  \n  # random effects\n  deltai <- rnorm(k, 0, sqrt(tau2))\n  \n  for(i in 1:k){\n    g1 <- rnorm(n1[i], 0, 1)\n    g2 <- rnorm(n2[i], es[i] + deltai[i], 1)\n    yi[i] <- mean(g2) - mean(g1)\n    vi[i] <- var(g1)/n1[i] + var(g2)/n2[i]\n  }\n  \n  sim <- data.frame(id = 1:k, yi, vi, n1 = n1, n2 = n2)\n  \n  if(!is.null(add)){\n    sim <- cbind(sim, add)\n  }\n  \n  # convert to escalc for using metafor methods\n  sim <- metafor::escalc(yi = yi, vi = vi, data = sim)\n  \n  return(sim)\n}\n```\n\n## Simulating a RE model\n\nThe data are similar to the EE simulation but we have an extra source of heterogeneity.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\ndat |>\n  summary() |>\n  qforest()\n```\n\n::: {.cell-output-display}\n![](chapter5_files/figure-html/unnamed-chunk-40-1.png){width=672}\n:::\n:::\n\n\n\n\n\n## Simulating a RE model\n\nTo see the actual impact of $\\tau^2$ we can follow the same approach of @sec-ee-impact-n thus using a large $n$. The sampling variance `vi` of each study is basically 0.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# ... other parameters as before\nn <- 1e4\ndeltai <- rnorm(k, 0, sqrt(tau2)) # random-effects\nthetai <- mu + deltai # true study effect\ndat <- sim_studies(k = k, es = thetai, n1 = n)\n# or equivalently \n# dat <- sim_studies(k = k, es = mu, tau2 = tau2, n1 = n)\n\nhead(dat)\n#> \n#>   id     yi     vi    n1    n2 \n#> 1  1 0.2071 0.0002 10000 10000 \n#> 2  2 0.6257 0.0002 10000 10000 \n#> 3  3 0.7598 0.0002 10000 10000 \n#> 4  4 0.5984 0.0002 10000 10000 \n#> 5  5 0.4834 0.0002 10000 10000 \n#> 6  6 0.2013 0.0002 10000 10000\n```\n:::\n\n\n\n\n\n## Simulating a RE Model\n\nClearly, compared to @sec-ee-impact-n, even with large $n$ the variability is not reduced because $\\tau^2 \\neq 0$. As $\\tau^2$ approach zero the EE and RE models are similar.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\ndat |>\n  summary() |>\n  qforest()\n```\n\n::: {.cell-output-display}\n![](chapter5_files/figure-html/unnamed-chunk-42-1.png){width=672}\n:::\n:::\n\n\n\n\n\n## RE model estimation\n\nGiven that we extended the EE model equation. Also the estimation of the average effect need to be extended. Basically the RE is still a weighted average but weights need to include also $\\tau^2$.\n\n$$\n\\overline y = \\frac{\\sum_{i = 1}^k y_iw^*_i}{\\sum_{i = 1}^k w^*_i}\n$$ {#eq-re1}\n\n$$\nw^*_i = \\frac{1}{\\sigma^2_i + \\tau^2}\n$$ {#eq-re2}\n\nThe weights are different compared to the EE model. Extremely precise/imprecise studies will have less impact in the RE model.\n\n## Fitting a RE model\n\nIn R we can use the `metafor::rma()` function using the `method = \"REML\"`.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit <- rma(yi, vi, data = dat, method = \"REML\")\nsummary(fit)\n#> \n#> Random-Effects Model (k = 15; tau^2 estimator: REML)\n#> \n#>   logLik  deviance       AIC       BIC      AICc   \n#>  -3.9162    7.8325   11.8325   13.1106   12.9234   \n#> \n#> tau^2 (estimated amount of total heterogeneity): 0.1022 (SE = 0.0387)\n#> tau (square root of estimated tau^2 value):      0.3198\n#> I^2 (total heterogeneity / total variability):   99.80%\n#> H^2 (total variability / sampling variability):  512.56\n#> \n#> Test for Heterogeneity:\n#> Q(df = 14) = 7157.7215, p-val < .0001\n#> \n#> Model Results:\n#> \n#> estimate      se    zval    pval   ci.lb   ci.ub      \n#>   0.6142  0.0826  7.4326  <.0001  0.4523  0.7762  *** \n#> \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n\n\n\n\n\n## Intepreting the RE model\n\nThe model output is quite similar to the EE model and also the intepretation is similar.\n\nThe only extra section is `tau^2/tau` that is the estimation of the between-study heterogeneity.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(fit)\n#> \n#> Random-Effects Model (k = 15; tau^2 estimator: REML)\n#> \n#>   logLik  deviance       AIC       BIC      AICc   \n#>  -3.9162    7.8325   11.8325   13.1106   12.9234   \n#> \n#> tau^2 (estimated amount of total heterogeneity): 0.1022 (SE = 0.0387)\n#> tau (square root of estimated tau^2 value):      0.3198\n#> I^2 (total heterogeneity / total variability):   99.80%\n#> H^2 (total variability / sampling variability):  512.56\n#> \n#> Test for Heterogeneity:\n#> Q(df = 14) = 7157.7215, p-val < .0001\n#> \n#> Model Results:\n#> \n#> estimate      se    zval    pval   ci.lb   ci.ub      \n#>   0.6142  0.0826  7.4326  <.0001  0.4523  0.7762  *** \n#> \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n\n\n\n\n\n\n## Estimating $\\tau^2$\n\n\n![](img/langan2017.png){fig-align=\"center\" width=80%}\n\n## Estimating $\\tau^2$\n\nThe **Restricted Maximum Likelihood** (REML) estimator is considered one of the best. We can compare the results using the `all_rma()` custom function that tests all the estimators^[The `filor::compare_rma()` function is similar to the `car::compareCoefs()` function].\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfitl <- all_rma(fit)\nround(filor::compare_rma(fitlist = fitl), 3)\n#>           DL     HE     HS    HSk     SJ     ML   REML     EB     PM    PMM\n#> b      0.614  0.614  0.614  0.614  0.614  0.614  0.614  0.614  0.614  0.614\n#> se     0.083  0.083  0.080  0.083  0.083  0.080  0.083  0.083  0.083  0.085\n#> zval   7.442  7.433  7.703  7.442  7.433  7.694  7.433  7.433  7.433  7.255\n#> pval   0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000\n#> ci.lb  0.452  0.452  0.458  0.452  0.452  0.458  0.452  0.452  0.452  0.448\n#> ci.ub  0.776  0.776  0.771  0.776  0.776  0.771  0.776  0.776  0.776  0.780\n#> I2    99.804 99.805 99.790 99.804 99.805 99.791 99.805 99.805 99.805 99.814\n#> tau2   0.102  0.102  0.095  0.102  0.102  0.095  0.102  0.102  0.102  0.107\n```\n:::\n\n\n\n\n\n## Intepreting heterogeneity $\\tau^2$\n\nLooking at @eq-re-mod2, $\\tau^2$ is essentially the variance of the random-effect. This means that we can intepret it as the variability (or the standard deviation) of the true effect size distribution.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\ntau2s <- c(0.01, 0.05, 0.1, 0.2)\ntau2s_t <- latex2exp::TeX(sprintf(\"$\\\\tau^2 = %.2f$\", tau2s))\n\npar(mfrow = c(1, 3))\nhist(rnorm(1e4, 0, sqrt(tau2s[1])), xlim = c(-2, 2), xlab = latex2exp::TeX(\"$y_i$\"), main = tau2s_t[1], probability = TRUE, ylim = c(0, 4.5), col = \"dodgerblue\")\nhist(rnorm(1e4, 0, sqrt(tau2s[2])), xlim = c(-2, 2), xlab = latex2exp::TeX(\"$y_i$\"), main = tau2s_t[2], probability = TRUE, ylim = c(0, 4.5), col = \"dodgerblue\")\n#hist(rnorm(1e4, 0, sqrt(tau2s[3])), xlim = c(-2, 2), xlab = latex2exp::TeX(\"$y_i$\"), main = tau2s_t[3], probability = TRUE, ylim = c(0, 5))\nhist(rnorm(1e4, 0, sqrt(tau2s[4])), xlim = c(-2, 2), xlab = latex2exp::TeX(\"$y_i$\"), main = tau2s_t[4], probability = TRUE, ylim = c(0, 4.5), col = \"dodgerblue\")\n```\n\n::: {.cell-output-display}\n![](chapter5_files/figure-html/unnamed-chunk-46-1.png){width=672}\n:::\n:::\n\n\n\n\n\n## Intepreting $\\tau^2$\n\nAs in the previus plot we can assume $n = \\infty$ and generate true effects from @eq-re-mod2. In this way we understand the impact of assuming (or estimating) a certain $\\tau^2$.\n\nFor example, a $\\tau = 0.2$ and a $\\mu_{\\theta} = 0.5$, 50% of the true effects ranged between:\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nD <- 0.5\nyis <- D + rnorm(1e5, 0, 0.2)\nquantile(yis, c(0.75, 0.25))\n#>       75%       25% \n#> 0.6358425 0.3664574\n```\n:::\n\n\n\n\n\n## The $Q$ Statistics^[See @Harrer2021-go (Chapter 5) and @Hedges2019-ry for an overview about the Q statistics]\n\nThe Q statistics is used to make inference on the heterogeneity. Can be considered as a weighted sum of squares:\n\n$$\nQ = \\sum^k_{i = 1}w_i(y_i - \\hat \\mu)^2\n$$\n\nWhere $\\hat \\mu$ is EE estimation (regardless if $\\tau^2 \\neq 0$) and $w_i$ are the inverse-variance weights. Note that in the case of $w_1 = w_2 ... = w_i$, Q is just a standard sum of squares (or deviance).\n\n## The $Q$ Statistics\n\n- Given that we are summing up squared distances, they should be approximately $\\chi^2$ with $df = k - 1$. In case of no heterogeneity ($\\tau^2 = 0$) the observed variability is only caused by sampling error and the expectd value of the $\\chi^2$ is just the degrees of freedom ($df = k - 1$).\n- In case of $\\tau^2 \\neq 0$, the expected value is $k - 1 + \\lambda$ where $\\lambda$ is a non-centrality parameter.\n- In other terms, if the expected value of $Q$ exceed the expected value assuming no heterogeneity, we have evidence that $\\tau^2 \\neq 0$.\n\n## The $Q$ Statistics\n\nLet's try a more practical approach. We simulate a lot of meta-analysis with and without heterogeneity and we check the Q statistics.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nget_Q <- function(yi, vi){\n  wi <- 1/vi\n  theta_ee <- weighted.mean(yi, wi)\n  sum(wi*(yi - theta_ee)^2)\n}\n\nk <- 30\nn <- 30\ntau2 <- 0.1\nnsim <- 1e4\n\nQs_tau2_0 <- rep(0, nsim)\nQs_tau2 <- rep(0, nsim)\nres2_tau2_0 <- vector(\"list\", nsim)\nres2_tau2 <- vector(\"list\", nsim)\n\nfor(i in 1:nsim){\n  dat_tau2_0 <- sim_studies(k = 30, es = 0.5, tau2 = 0, n1 = n)\n  dat_tau2 <- sim_studies(k = 30, es = 0.5, tau2 = tau2, n1 = n)\n  \n  theta_ee_tau2_0 <- weighted.mean(dat_tau2_0$yi, 1/dat_tau2_0$vi)\n  theta_ee <- weighted.mean(dat_tau2$yi, 1/dat_tau2$vi)\n  \n  res2_tau2_0[[i]] <- dat_tau2_0$yi - theta_ee_tau2_0\n  res2_tau2[[i]] <- dat_tau2$yi - theta_ee\n  \n  Qs_tau2_0[i] <- get_Q(dat_tau2_0$yi, dat_tau2_0$vi)\n  Qs_tau2[i] <- get_Q(dat_tau2$yi, dat_tau2$vi)\n}\n```\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![](chapter5_files/figure-html/unnamed-chunk-49-1.png){width=672}\n:::\n:::\n\n\n\n\n\n## The $Q$ Statistics\n\nLet's try a more practical approach. We simulate a lot of meta-analysis with and without heterogeneity and we check the Q statistics.\n\n. . .\n\n- clearly, in the presence of heterogeneity, the expected value of the Q statistics is higher (due to $\\lambda \\neq 0$) and also residuals are larger (the $\\chi^2$ is just a sum of squared weighted residuals)\n\n. . .\n\n- we can calculate a p-value for deviation from the $\\tau^2 = 0$ case as evidence agaist the absence of heterogeneity\n\n## $I^2$ [@Higgins2002-fh]\n\nWe have two sources of variability in a random-effects meta-analysis, the sampling variability $\\sigma_i^2$ and true heterogeneity $\\tau^2$. We can use the $I^2$ to express the interplay between the two.\n$$\nI^2 = 100\\% \\times \\frac{\\hat{\\tau}^2}{\\hat{\\tau}^2 + \\tilde{v}}\n$${#eq-i2}\n\n$$\n\\tilde{v} = \\frac{(k-1) \\sum w_i}{(\\sum w_i)^2 - \\sum w_i^2},\n$$\n\nWhere $\\tilde{v}$ is the typical sampling variability. $I^2$ is intepreted as the proportion of total variability due to real heterogeneity (i.e., $\\tau^2$)\n\n## $I^2$ [@Higgins2002-fh]^[see [https://www.meta-analysis-workshops.com/download/common-mistakes1.pdf](https://www.meta-analysis-workshops.com/download/common-mistakes1.pdf)]\n\nNote that we can have the same $I^2$ in two completely different meta-analysis. An high $I^2$ does not represent high heterogeneity. Let's assume to have two meta-analysis with $k$ studies and small ($n = 30$) vs large ($n = 500$) sample sizes. \n\nLet's solve @eq-i2 for $\\tau^2$ (using `filor::tau2_from_I2()`) and we found that the same $I^2$ can be obtained with two completely different $\\tau^2$ values:\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nn_1 <- 30\nvi_1 <- 1/n_1 + 1/n_1\ntau2_1 <- filor::tau2_from_I2(0.8, vi_1)\ntau2_1\n#> [1] 0.2666667\n\nn_2 <- 500\nvi_2 <- 1/n_2 + 1/n_2\ntau2_2 <- filor::tau2_from_I2(0.8, vi_2)\ntau2_2\n#> [1] 0.016\n```\n:::\n\n\n\n\n\n## $I^2$ [@Higgins2002-fh]\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nn_1 <- 30\nvi_1 <- 1/n_1 + 1/n_1\ntau2_1 <- filor::tau2_from_I2(0.8, vi_1)\ntau2_1\n#> [1] 0.2666667\n\nn_2 <- 500\nvi_2 <- 1/n_2 + 1/n_2\ntau2_2 <- filor::tau2_from_I2(0.8, vi_2)\ntau2_2\n#> [1] 0.016\n```\n:::\n\n\n\n\n\n. . .\n\nIn other terms, the $I^2$ can be considered a good index of heterogeneity only when the total variance ($\\tilde{v} + \\tau^2$) is similar.\n\n## What about $\\tilde{v}$?\n\n$\\tilde{v}$ is considered the \"typical\" within-study variability (see [https://www.metafor-project.org/doku.php/tips:i2_multilevel_multivariate](https://www.metafor-project.org/doku.php/tips:i2_multilevel_multivariate)). There are different estimators but @eq-tildev [@Higgins2002-fh] is the most common.\n\n$$\n\\tilde{v} = \\frac{(k-1) \\sum w_i}{(\\sum w_i)^2 - \\sum w_i^2}\n$$ {#eq-tildev}\n\n## What about $\\tilde{v}$?\n\nIn the hypothetical case where $\\sigma^2_1 = \\dots = \\sigma^2_k$, $\\tilde{v}$ is just $\\sigma^2$. This fact is commonly used to calculate the statistical power analytically [@Borenstein2009-mo, Chapter 29].\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nvtilde <- function(wi){\n  k <- length(wi)\n  (k - 1) * sum(wi) / (sum(wi)^2 - sum(wi^2))\n}\n\nk <- 20\n\n# same vi\nvi <- rep((1/30 + 1/30), k)\nhead(vi)\n#> [1] 0.06666667 0.06666667 0.06666667 0.06666667 0.06666667 0.06666667\nvtilde(1/vi)\n#> [1] 0.06666667\n\n# heterogeneous vi\nn <- 10 + rpois(k, 30 - 10)\nvi <- sim_vi(k = k, n1 = n)\nvtilde(1/vi)\n#> [1] 0.06389075\n```\n:::\n\n\n\n\n\n## What about $\\tilde{v}$?\n\nUsing simulations we can see that $\\tilde{v}$ with heterogenenous variances (i.e., sample sizes in this case) can be approximated by the central tendency of the sample size distribution. Note that we are fixing $\\sigma^2 = 1$ thus we are not including uncertainty.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nk <- 100 # number of studies\nn <- 30 # sample size\n\nvti <- rep(NA, 1e5)\n\nfor(i in 1:1e5){\n  ni <- rpois(k, n)\n  vi <- 1/ni + 1/ni\n  vti[i] <- vtilde(1/vi)\n}\n\n# vtilde calculated from lambda\nvt <- 1/n + 1/n\n```\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![](chapter5_files/figure-html/unnamed-chunk-54-1.png){width=672}\n:::\n:::\n\n\n\n\n\n## $H^2$\n\nThe $H^2$ is an alternative index of heterogeneity. Is calculated as:\n\n$$\nH^2 = \\frac{Q}{k - 1}\n$$\n\nWe defined $Q$ as the weighted sum of squares representing the total variability. $k - 1$ is the expected value of the $\\chi^2$ statistics (i.e., sum of squares) when $\\tau^2 = 0$ (or $\\lambda = 0$). \n\nThus $H^2$ is the ratio between total heterogeneity and sampling variability. Higher $H^2$ is associated with higher heterogeneity **relative** to the sampling variability. $H^2$ is not a measure of absolute heterogeneity.\n\n## $H^2$\n\nWhen we are fitting a RE model, the $I^2$ and $H^2$ equations are slightly different [@Higgins2002-fh]. See also the `metafor` [source code](https://github.com/cran/metafor/blob/994d26a65455fac90760ad6a004ec1eaca5856b1/R/rma.uni.r#L2459C30-L2459C30).\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nk <- 100\nmu <- 0.5\ntau2 <- 0.1\nn <- 30\n\ndat <- sim_studies(k = k, es = mu, tau2 = tau2, n1 = n)\nfit_re <- rma(yi, vi, data = dat, method = \"REML\")\nfit_ee <- rma(yi, vi, data = dat, method = \"EE\")\n\n# H2 with EE model\n\ntheta_ee <- fit_ee$b[[1]] # weighted.mean(dat$yi, 1/dat$vi)\nwi <- 1/dat$vi\nQ <- with(dat, sum((1/vi)*(yi - theta_ee)^2))\nc(Q, fit_ee$QE) # same\n#> [1] 350.1654 350.1654\n\nc(H2 = fit_ee$QE / (fit_ee$k - fit_ee$p), H2_model = fit_ee$H2) # same\n#>       H2 H2_model \n#> 3.537024 3.537024\n\n# H2 with RE model\n\nvt <- vtilde(1/dat$vi)\nc(H2 = fit_re$tau2 / vt + 1, H2_model = fit_re$H2) # same\n#>       H2 H2_model \n#> 3.495119 3.495119\n```\n:::\n\n\n\n\n\n## Confidence Intervals\n\nWhat is reported in the model summary as `ci.lb` and `ci.ub` refers to the 95% confidence interval representing the uncertainty in estimating the effect (or a meta-regression parameter).\n\nWithout looking at the equations, let's try to implement this idea using simulations.\n\n- choose $k$, $\\tau^2$ and $n$\n- simulate data (several times) accordingly and fit the RE model\n- extract the estimated effect size\n- compare the simulated sampling distribution with the analytical result\n\n## Confidence Intervals\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nk <- 30\nn <- 30\ntau2 <- 0.05\nmu <- 0.5\nnsim <- 5e3\n\n# true parameters (see Borenstein, 2009; Chapter 29)\nvt <- 1/n + 1/n\nvs <- (vt + tau2)/ k\nse <- sqrt(vs)\n\nmui <- rep(NA, nsim)\n\nfor(i in 1:nsim){\n  dat <- sim_studies(k = k, es = mu, tau2 = tau2, n1 = n)\n  fit <- rma(yi, vi, data = dat)\n  mui[i] <- coef(fit)[1]\n}\n\n# standard error\nc(simulated = sd(mui), analytical = fit$se)\n#>  simulated analytical \n#> 0.06322640 0.06508356\n\n# confidence interval\nrbind(\n  \"simulated\"  = quantile(mui, c(0.05, 0.975)),\n  \"analytical\" = c(\"2.5%\" = fit$ci.lb, \"97.5%\" = fit$ci.ub)\n)\n#>                   5%     97.5%\n#> simulated  0.3958958 0.6226868\n#> analytical 0.3721610 0.6272838\n```\n:::\n\n\n\n\n\n## Confidence Intervals\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhist(mui, breaks = 50, freq = FALSE, main = \"Sampling Distribution\", xlab = latex2exp::TeX(\"$\\\\mu_{\\\\theta}$\"))\ncurve(dnorm(x, mu, se), add = TRUE, col = \"firebrick\", lwd = 1.5)\n```\n\n::: {.cell-output-display}\n![](chapter5_files/figure-html/unnamed-chunk-57-1.png){width=672}\n:::\n:::\n\n\n\n\n\n## Confidence Intervals\n\nNow the equation for the 95% confidence interval should be more clear. The standard error is a function of the within study sampling variances (depending mainly on $n$), $\\tau^2$ and $k$. As we increase $k$ the standard error tends towards zero. \n\n$$\nCI = \\hat \\mu_{\\theta} \\pm z SE_{\\mu_{\\theta}}\n$$\n\n$$\nSE_{\\mu_{\\theta}} = \\sqrt{\\frac{1}{\\sum^{k}_{i = 1}w^{\\star}_i}}\n$$\n\n$$\nw^{\\star}_i = \\frac{1}{\\sigma^2_i + \\tau^2}\n$$\n\n## Confidence Intervals\n\nWe can also see it analytically, there is a huge impact of $k$.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\n# true parameters (see Borenstein, 2009; Chapter 29)\nvt <- 1/n + 1/n\nvs <- (vt + tau2)/ k\nse <- sqrt(vs)\n\nk <- c(10, 50, 100, 500, 1000, 5000)\nn <- c(10, 50, 100, 500, 1000, 5000)\ntau2 <- c(0, 0.05, 0.1, 0.2)\n\ndd <- expand.grid(k = k, n = n, tau2 = tau2)\n\ndd$vt <- with(dd, 1/n + 1/n)\ndd$vs <- with(dd, (vt + tau2)/ k)\ndd$se <- sqrt(dd$vs)\n\ndd$k <- as_tex_label(dd$k, \"$k = %s$\")\n\nggplot(dd, aes(x = n, y = se, color = factor(tau2))) +\n  geom_line() +\n  facet_wrap(~k, labeller = label_parsed) +\n  labs(color = latex2exp::TeX(\"\\\\tau^2\")) +\n  xlab(\"Sample Size (n)\") +\n  ylab(latex2exp::TeX(\"$SE_{\\\\mu_{\\\\theta}}$\"))\n```\n\n::: {.cell-output-display}\n![](chapter5_files/figure-html/unnamed-chunk-58-1.png){width=672}\n:::\n:::\n\n\n\n\n\n## Prediction intervals (PI)\n\nWe could say that the CI is not completely taking into account the between-study heterogeneity ($\\tau^2$). After a meta-analysis we would like to know how confident we are in the parameters estimation BUT also **what would be the expected effect running a new experiment tomorrow?**.\n\nThe **prediction interval** [@IntHout2016-sz; @Riley2011-hp] is exactly the range of effects that I expect in predicting a new study.\n\n## PI for a sample mean\n\nTo understand the concept, let's assume to have a sample $X$ of size $n$ and we estimate the mean $\\overline X$. The PI is calculated as^[Notice that the equation, in particular the usage of $t$ vs $z$ depends on assuming $s_x$ to be known or estimated. See [https://online.stat.psu.edu/stat501/lesson/3/3.3](https://online.stat.psu.edu/stat501/lesson/3/3.3), [https://en.wikipedia.org/wiki/Prediction_interval](https://en.wikipedia.org/wiki/Prediction_interval) and [https://www.bryanshalloway.com/2021/03/18/intuition-on-uncertainty-of-predictions-introduction-to-prediction-intervals/](https://www.bryanshalloway.com/2021/03/18/intuition-on-uncertainty-of-predictions-introduction-to-prediction-intervals/)]:\n\n$$\nPI = \\overline X \\pm t_{\\alpha/2} s_x \\sqrt{1 + \\frac{1}{n}}\n$$\n\nWhere $s$ is the sample standard deviation. Basically we are combining the uncertainty in estimating $\\overline X$ (i.e, $\\frac{s_x}{n}$) with the standard deviation of the data $s_x$. Compare it with the confidence interval containing only $\\frac{s_x}{n}$.\n\n## PI in meta-analysis\n\nFor meta-analysis the equation^[When a $t$ distribution is assumed, the quantiles are calculated using $k - 2$ degrees of freedom] is conceptually similar but with different quantities.\n\n$$\nPI = \\hat \\mu_{\\theta} \\pm z \\sqrt{\\tau^2 + SE_{\\mu_{\\theta}}}\n$$\n\nBasically we are combining all the sources of uncertainty. As long as $\\tau^2 \\neq 0$ the PI is greater than the CI (in the EE model they are the same). Thus even with very precise $\\mu_{\\theta}$ estimation, large $\\tau^2$ leads to uncertain predictions.\n\n## PI in meta-analysis\n\nIn R the PI can be calculated using `predict()`. By default the model assume a standard normal distribution thus using $z$ scores. To use the @Riley2011-hp approach ($t$ distribution) the model need to be fitted using `test = \"t\"`.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nk <- 100\ndat <- sim_studies(k = k, es = 0.5, tau2 = 0.1, n1 = 30)\nfit_z <- rma(yi, vi, data = dat, test = \"z\") # test = \"z\" is the default\npredict(fit_z) # notice pi.ub/pi.lb vs ci.ub/ci.lb\n#> \n#>    pred     se  ci.lb  ci.ub   pi.lb  pi.ub \n#>  0.4423 0.0424 0.3592 0.5254 -0.2299 1.1145\n# manually\nfit_z$b[[1]] + qnorm(c(0.025, 0.975)) * sqrt(fit_z$se^2 + fit_z$tau2)\n#> [1] -0.229882  1.114516\n\nfit_t <- rma(yi, vi, data = dat, test = \"t\")\npredict(fit_t) # notice pi.ub/pi.lb vs ci.ub/ci.lb\n#> \n#>    pred     se  ci.lb  ci.ub   pi.lb  pi.ub \n#>  0.4423 0.0424 0.3582 0.5265 -0.2382 1.1228\n# manually\nfit_z$b[[1]] + qt(c(0.025, 0.975), k - 2) * sqrt(fit_t$se^2 + fit_t$tau2)\n#> [1] -0.2382858  1.1229198\n```\n:::\n\n\n\n\n\n## References <button class=\"btn\"><i class=\"fa fa-download\"></i><a href=\"data:text/x-bibtex;base64,@ARTICLE{Viechtbauer2005-zt,
  title = {Bias and efficiency of meta-analytic variance estimators in the
  random-effects model},
  author = {Viechtbauer, Wolfgang},
  journaltitle = {Journal of educational and behavioral statistics: a quarterly
  publication sponsored by the American Educational Research Association and the
  American Statistical Association},
  publisher = {American Educational Research Association (AERA)},
  volume = {30},
  issue = {3},
  pages = {261-293},
  date = {2005-09},
  doi = {10.3102/10769986030003261},
  issn = {1076-9986,1935-1054},
  abstract = {The meta-analytic random effects model assumes that the
  variability in effect size estimates drawn from a set of studies can be
  decomposed into two parts: heterogeneity due to random population effects and
  sampling variance. In this context, the usual goal is to estimate the central
  tendency and the amount of heterogeneity in the population effect sizes. The
  amount of heterogeneity in a set of effect sizes has implications regarding
  the interpretation of the meta-analytic findings and often serves as an
  indicator for the presence of potential moderator variables. Five population
  heterogeneity estimators were compared in this article analytically and via
  Monte Carlo simulations with respect to their bias and efficiency.},
  url = {https://scholar.google.com/citations?view_op=view_citation&hl=en&user=J89RXJkAAAAJ&citation_for_view=J89RXJkAAAAJ:qjMakFHDy7sC},
  language = {en}
}

@BOOK{Harrer2021-go,
  title = {Doing meta-analysis with R: A hands-on guide},
  author = {Harrer, Mathias and Cuijpers, Pim and Furukawa, Toshi and Ebert,
  David},
  publisher = {CRC Press},
  location = {London, England},
  edition = {1st},
  date = {2021-09-13},
  pagetotal = {474},
  isbn = {9780367610074},
  language = {en}
}

@ARTICLE{Veroniki2016-nw,
  title = {Methods to estimate the between-study variance and its uncertainty in
  meta-analysis},
  author = {Veroniki, Areti Angeliki and Jackson, Dan and Viechtbauer, Wolfgang
  and Bender, Ralf and Bowden, Jack and Knapp, Guido and Kuss, Oliver and
  Higgins, Julian P T and Langan, Dean and Salanti, Georgia},
  journaltitle = {Research synthesis methods},
  publisher = {Wiley},
  volume = {7},
  issue = {1},
  pages = {55-79},
  date = {2016-03},
  doi = {10.1002/jrsm.1164},
  pmc = {PMC4950030},
  pmid = {26332144},
  issn = {1759-2879,1759-2887},
  abstract = {Meta-analyses are typically used to estimate the overall/mean of
  an outcome of interest. However, inference about between-study variability,
  which is typically modelled using a between-study variance parameter, is
  usually an additional aim. The DerSimonian and Laird method, currently widely
  used by default to estimate the between-study variance, has been long
  challenged. Our aim is to identify known methods for estimation of the
  between-study variance and its corresponding uncertainty, and to summarise the
  simulation and empirical evidence that compares them. We identified 16
  estimators for the between-study variance, seven methods to calculate
  confidence intervals, and several comparative studies. Simulation studies
  suggest that for both dichotomous and continuous data the estimator proposed
  by Paule and Mandel and for continuous data the restricted maximum likelihood
  estimator are better alternatives to estimate the between-study variance.
  Based on the scenarios and results presented in the published studies, we
  recommend the Q-profile method and the alternative approach based on a
  'generalised Cochran between-study variance statistic' to compute
  corresponding confidence intervals around the resulting estimates. Our
  recommendations are based on a qualitative evaluation of the existing
  literature and expert consensus. Evidence-based recommendations require an
  extensive simulation study where all methods would be compared under the same
  scenarios.},
  url = {https://onlinelibrary.wiley.com/doi/10.1002/jrsm.1164},
  file = {Veroniki et al. 2016 - Methods to estimate the between-study variance and its uncertainty in meta-analysis.pdf},
  keywords = {bias; confidence interval; coverage probability; heterogeneity;
  mean squared error},
  language = {en}
}

@MISC{Borenstein2009-mo,
  title = {Introduction to {Meta-Analysis}},
  author = {Borenstein, Michael and Hedges, Larry V and Higgins, Julian P T and
  Rothstein, Hannah R},
  date = {2009},
  doi = {10.1002/9780470743386},
  url = {http://dx.doi.org/10.1002/9780470743386}
}

@ARTICLE{Riley2011-hp,
  title = {Interpretation of random effects meta-analyses},
  author = {Riley, Richard D and Higgins, Julian P T and Deeks, Jonathan J},
  journaltitle = {BMJ},
  volume = {342},
  pages = {d549},
  date = {2011-02-10},
  doi = {10.1136/bmj.d549},
  pmid = {21310794},
  issn = {0959-8138,1756-1833},
  url = {http://dx.doi.org/10.1136/bmj.d549},
  language = {en}
}

@ARTICLE{IntHout2016-sz,
  title = {Plea for routinely presenting prediction intervals in meta-analysis},
  author = {IntHout, Joanna and Ioannidis, John P A and Rovers, Maroeska M and
  Goeman, Jelle J},
  journaltitle = {BMJ open},
  volume = {6},
  issue = {7},
  pages = {e010247},
  date = {2016-07-12},
  doi = {10.1136/bmjopen-2015-010247},
  pmc = {PMC4947751},
  pmid = {27406637},
  issn = {2044-6055},
  abstract = {OBJECTIVES: Evaluating the variation in the strength of the effect
  across studies is a key feature of meta-analyses. This variability is
  reflected by measures like τ(2) or I(2), but their clinical interpretation is
  not straightforward. A prediction interval is less complicated: it presents
  the expected range of true effects in similar studies. We aimed to show the
  advantages of having the prediction interval routinely reported in
  meta-analyses. DESIGN: We show how the prediction interval can help understand
  the uncertainty about whether an intervention works or not. To evaluate the
  implications of using this interval to interpret the results, we selected the
  first meta-analysis per intervention review of the Cochrane Database of
  Systematic Reviews Issues 2009-2013 with a dichotomous (n=2009) or continuous
  (n=1254) outcome, and generated 95\% prediction intervals for them. RESULTS:
  In 72.4\% of 479 statistically significant (random-effects p0), the 95\%
  prediction interval suggested that the intervention effect could be null or
  even be in the opposite direction. In 20.3\% of those 479 meta-analyses, the
  prediction interval showed that the effect could be completely opposite to the
  point estimate of the meta-analysis. We demonstrate also how the prediction
  interval can be used to calculate the probability that a new trial will show a
  negative effect and to improve the calculations of the power of a new trial.
  CONCLUSIONS: The prediction interval reflects the variation in treatment
  effects over different settings, including what effect is to be expected in
  future patients, such as the patients that a clinician is interested to treat.
  Prediction intervals should be routinely reported to allow more informative
  inferences in meta-analyses.},
  url = {http://dx.doi.org/10.1136/bmjopen-2015-010247},
  keywords = {Clinical trial; Cochrane Database of Systematic Reviews;
  Heterogeneity; Meta-analysis; Prediction interval; Random effects;Bayesian
  Statistics},
  language = {en}
}

@ARTICLE{Higgins2002-fh,
  title = {Quantifying heterogeneity in a meta-analysis},
  author = {Higgins, Julian P T and Thompson, Simon G},
  journaltitle = {Statistics in medicine},
  volume = {21},
  issue = {11},
  pages = {1539-1558},
  date = {2002-06-15},
  doi = {10.1002/sim.1186},
  pmid = {12111919},
  issn = {0277-6715},
  abstract = {The extent of heterogeneity in a meta-analysis partly determines
  the difficulty in drawing overall conclusions. This extent may be measured by
  estimating a between-study variance, but interpretation is then specific to a
  particular treatment effect metric. A test for the existence of heterogeneity
  exists, but depends on the number of studies in the meta-analysis. We develop
  measures of the impact of heterogeneity on a meta-analysis, from mathematical
  criteria, that are independent of the number of studies and the treatment
  effect metric. We derive and propose three suitable statistics: H is the
  square root of the chi2 heterogeneity statistic divided by its degrees of
  freedom; R is the ratio of the standard error of the underlying mean from a
  random effects meta-analysis to the standard error of a fixed effect
  meta-analytic estimate, and I2 is a transformation of (H) that describes the
  proportion of total variation in study estimates that is due to heterogeneity.
  We discuss interpretation, interval estimates and other properties of these
  measures and examine them in five example data sets showing different amounts
  of heterogeneity. We conclude that H and I2, which can usually be calculated
  for published meta-analyses, are particularly useful summaries of the impact
  of heterogeneity. One or both should be presented in published meta-analyses
  in preference to the test for heterogeneity.},
  url = {http://dx.doi.org/10.1002/sim.1186},
  language = {en}
}

@ARTICLE{Hedges1989-ip,
  title = {An unbiased correction for sampling error in validity generalization
  studies},
  author = {Hedges, Larry V},
  journaltitle = {The Journal of applied psychology},
  publisher = {American Psychological Association (APA)},
  volume = {74},
  issue = {3},
  pages = {469-477},
  date = {1989-06},
  doi = {10.1037/0021-9010.74.3.469},
  issn = {0021-9010,1939-1854},
  url = {http://dx.doi.org/10.1037/0021-9010.74.3.469},
  language = {en}
}

@ARTICLE{Hedges2019-ry,
  title = {Statistical analyses for studying replication: Meta-analytic
  perspectives},
  author = {Hedges, Larry V and Schauer, Jacob M},
  journaltitle = {Psychological methods},
  publisher = {American Psychological Association (APA)},
  volume = {24},
  issue = {5},
  pages = {557-570},
  date = {2019-10},
  doi = {10.1037/met0000189},
  pmid = {30070547},
  issn = {1082-989X,1939-1463},
  abstract = {Formal empirical assessments of replication have recently become
  more prominent in several areas of science, including psychology. These
  assessments have used different statistical approaches to determine if a
  finding has been replicated. The purpose of this article is to provide several
  alternative conceptual frameworks that lead to different statistical analyses
  to test hypotheses about replication. All of these analyses are based on
  statistical methods used in meta-analysis. The differences among the methods
  described involve whether the burden of proof is placed on replication or
  nonreplication, whether replication is exact or allows for a small amount of
  "negligible heterogeneity," and whether the studies observed are assumed to be
  fixed (constituting the entire body of relevant evidence) or are a sample from
  a universe of possibly relevant studies. The statistical power of each of
  these tests is computed and shown to be low in many cases, raising issues of
  the interpretability of tests for replication. (PsycINFO Database Record (c)
  2019 APA, all rights reserved).},
  url = {http://dx.doi.org/10.1037/met0000189},
  file = {Hedges and Schauer 2019 - Statistical analyses for studying replication - Meta-analytic perspectives.pdf},
  keywords = {replicability-book;methods},
  language = {en}
}

\" download=\"refs_to_download.bib\"> Download .bib file</a></button> {.refs}\n\n::: {#refs}\n:::",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}