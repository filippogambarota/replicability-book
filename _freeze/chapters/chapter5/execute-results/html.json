{
  "hash": "0f080204ae80f6d4535abb676568bb7f",
  "result": {
    "engine": "knitr",
    "markdown": "---\nexecute: \n  echo: true\nknitr:\n  opts_chunk: \n    collapse: true\n    comment: \"#>\"\n---\n\n\n\n\n\n# Meta-analysis and publication bias\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Meta-analysis {.section}\n\n## Meta-analysis\n\n- The meta-analysis is a statistical procedure to combine evidence from a group of studies.\n\n. . .\n\n- The idea is to \"switch\" the statistical unit from e.g., participants to studies\n\n. . .\n\n- The motto could be that (appropriately) combining similar studies with a similar aim is the best way to understand something about a phenomenon\n\n## Meta-analysis and Systematic Review {.smaller}\n\nUsually a meta-analysis work follows these steps:\n\n1. **Identify the research question**: is the treatment *x* effective?, Does the experimental effect *y* exist?\n2. **Define inclusion/exclusion criteria**: From the research question (1), keep only e.g., randomized controlled trials, studies with healthy participants, etc.\n3. **Systematically search for studies**: Analyze the literature to find all relevant studies\n4. **Extract relevant information**: Read, extract and organize relevant information e.g., sample size, treatment type, age, etc.\n5. **Summarize the results**: Create a narrative (flowcharts, tables, etc.) summary of included studies. This is the Systematic review part.\n6. **Choose an effect size**: Choose a way to standardize the effect across included studies\n7. **Meta-analysis model**: Choose and implement a meta-analysis model\n8. **Interpret and report results**\n\n## Before the fun part...\n\n::: {.incremental}\n- We are dealing only with the **statistical part**. The study selection, data extraction, studies evaluation etc. is another story\n- The quality of the meta-analysis is the **quality of included studies**\n:::\n\n. . .\n\n![](img/gigo.png){fig-align=\"center\" width=60%}\n\n## Unstandardized effect sizes\n\nThe basic idea of an effect size is just using the raw measure. For example studies using reaction times we can calculate the difference between two conditions as $\\overline X_1 - \\overline X_2$:\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](chapter5_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\n\n\n\n## Unstandardized effect sizes\n\nBut another study (with the same research question) could use another measure, e.g., accuracy. We can still (not the best strategy but) compute the difference between the group means.\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](chapter5_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n\n\n\n\n\n## Unstandardized effect sizes\n\nClearly we cannot directly compare the two effects but we need to standardize the measure.\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](chapter5_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\n\n\n\n## Standardized effect sizes\n\nTo compare results from different studies, we should use a common metric. Frequently meta-analysts use *standardized* effect sizes. For example the Pearson correlation or the Cohen's $d$.\n\n$$\nr = \\frac{\\sum{(x_i - \\bar{x})(y_i - \\bar{y})}}{\\sqrt{\\sum{(x_i - \\bar{x})^2}\\sum{(y_i - \\bar{y})^2}}}\n$$ {#eq-correlation}\n\n$$\nd = \\frac{\\bar{x_1} - \\bar{x_2}}{s_p}\n$$\n\n$$\ns_p = \\sqrt{\\frac{(n_1 - 1)s_1^2 + (n_2 - 1)s_2^2}{n_1 + n_2 - 2}}\n$$\n\n## Standardized effect sizes\n\nThe advantage of standardized effect size is that regardless the original variable, the interpretation and the scale is the same. For example the pearson correlation ranges between -1 and 1 and the Cohen's $d$ between $- \\infty$ and $\\infty$ and is interpreted as how many standard deviations the two groups/conditions differs.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nS <- matrix(c(1, 0.7, 0.7, 1), nrow = 2)\nX <- MASS::mvrnorm(100, c(0, 2), S, empirical = TRUE)\n\npar(mfrow = c(1,2))\nplot(X, xlab = \"x\", ylab = \"y\", cex = 1.3, pch = 19,\n     cex.lab = 1.2, cex.axis = 1.2,\n     main = latex2exp::TeX(sprintf(\"$r = %.2f$\", cor(X[, 1], X[, 2]))))\nabline(lm(X[, 2] ~ X[, 1]), col = \"firebrick\", lwd = 2)\n\n\nplot(density(X[, 1]), xlim = c(-5, 7), ylim = c(0, 0.5), col = \"dodgerblue\", lwd = 2,\n     main = latex2exp::TeX(sprintf(\"$d = %.2f$\", lsr::cohensD(X[, 1], X[, 2]))),\n     xlab = \"\")\nlines(density(X[, 2]), col = \"firebrick\", lwd = 2)\n```\n\n::: {.cell-output-display}\n![](chapter5_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\n\n\n\n\n## Standardized vs unstandardized\n\nThe main difference is (usually) the absence of a effect-size-variance relationship for unstandardized effects. For example, the variance of the difference between two groups is:\n\n$$\nV_d = \\frac{s^2_1}{n_1} + \\frac{s^2_2}{n_2}\n$$ {#eq-var-umd}\n\nWhile the variance of a Cohen's $d$ can be calculated as:\n\n$$\nV_d = \\frac{n_1 + n_2}{n_1 n_2} + \\frac{d^2}{2(n_1 + n_2)}\n$$\n\n## Standardized vs unstandardized\n\nIn this [amazing blog post](https://www.jepusto.com/alternative-formulas-for-the-smd/) James Pustejovsky explained where the equations comes from. Basically, the $\\frac{n_1 + n_2}{n_1 n_2}$ term is the same as the $\\frac{s^2_1}{n_1} + \\frac{s^2_2}{n_2}$ while the extra $\\frac{d^2}{2(n_1 + n_2)}$ is for the non-centrality induced by the standardized difference.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nn <- c(10, 50, 100)\nd <- seq(0, 2, 0.001)\n\ndd <- expand.grid(n = n, d = d)\n\ndd$vumd <- with(dd, 1/n + 1/n)\ndd$vd <- with(dd, (n + n) / (n * n) + d^2/(2 * (n + n)))\n\ntidyr::pivot_longer(dd, 3:4) |> \n  ggplot(aes(x = d, y = value, color = name, linetype = factor(n))) +\n  geom_line() +\n  labs(linetype = \"Sample Size\",\n       color = NULL)\n```\n\n::: {.cell-output-display}\n![](chapter5_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n\n\n\n\n## Effect size sampling variability {#sec-effsize-se}\n\nCrucially, we can calculate also the **sampling variability** of each effect size. The **sampling variability** is the precision of estimated value.\n\nFor example, there are multiple methods to estimate the Cohen's $d$ sampling variability. For example:\n\n$$\nV_d = \\frac{n_1 + n_2}{n_1 n_2} + \\frac{d^2}{2(n_1 + n_2)}\n$$\n\nEach effect size has a specific formula for the sampling variability. The sample size is usually the most important information. Studies with high sample size have low sampling variability.\n\n## Effect size sampling variability\n\nAs the sample size grows and tends to infinity, the sampling variability approach zero.\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](chapter5_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\n\n\n\n\n## Unstandardized effect sizes\n\nFor the examples and plots I'm going to use simulated data. We simulate *unstandardized* effect sizes (UMD) because the computations are easier and the estimator is unbiased [e.g., @Viechtbauer2005-zt]\n\nMore specifically we simulate hypothetical studies where two independent groups are compared:\n\n$$\n\\Delta = \\overline{X_1} - \\overline{X_2}\n$$ {#eq-umd}\n\n$$\nSE_{\\Delta} = \\sqrt{\\frac{s^2_1}{n_1} + \\frac{s^2_2}{n_2}}\n$$\n\nWith $X_{1_i} \\sim \\mathcal{N}(0, 1)$ and $X_{2_i} \\sim \\mathcal{N}(\\Delta, 1)$\n\nThe main advantage is that, compared to standardized effect size, the sampling variability do not depends on the effect size itself, simplifying the computations.\n\n# Simulating a single study {.section}\n\n## Simulating a single study - UMD\n\nTo simulate a single study using a UMD we need to generate data according to the appropriate model. Here we have a difference between two groups. We can assume that the two groups comes from a normal distribution where group 1 $g_1 \\sim \\mathcal{N}(0, 1)$ and group 2 $g_2 \\sim \\mathcal{N}(D, 1)$ where $D$ is the effect size. Then using Equations [-@eq-var-umd; -@eq-umd] we can estimate the effect size and the variance.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nD <- 1  # effect size\nn <- 50 # sample size\ng1 <- rnorm(n, mean = 0, sd = 1)\ng2 <- rnorm(n, mean = D, sd = 1)\n\n# effect size\nmean(g2) - mean(g1)\n#> [1] 0.7428889\n\n# variance\nvar(g1)/n + var(g2)/n\n#> [1] 0.03372845\n```\n:::\n\n\n\n\n\n## Simulating a single study - UMD\n\nFor simplicity we can wrap everything within a function:\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# default sd = 1\nsim_umd <- function(n1, n2 = NULL, D, sd = 1){\n  if(is.null(n2)) n2 <- n1 # same to n1 if null \n  g1 <- rnorm(n1, mean = 0, sd = sd)\n  g2 <- rnorm(n2, mean = D, sd = sd)\n  yi <- mean(g2) - mean(g1)\n  vi <- var(g1)/n1 + var(g2)/n2\n  data.frame(yi, vi)\n}\n\nsim_umd(100, D = 0.5)\n#>          yi         vi\n#> 1 0.6292466 0.02098998\nsim_umd(50, D = 0.1)\n#>          yi       vi\n#> 1 0.4147609 0.047054\n```\n:::\n\n\n\n\n\n## Simulating a single study - UMD\n\nWe can also generate a large number of studies and check the distribution of effect size and sampling variances. Note that the real $D = 1$ and the real variance $V_D = 1/50 + 1/50 = 0.04$\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstudies <- replicate(1000, sim_umd(n1 = 50, D = 1), simplify = FALSE) # simplify = FALSE return a list\nstudies <- do.call(rbind, studies) # to dataframe\nhead(studies)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#>          yi         vi\n#> 1 1.0974008 0.04093664\n#> 2 1.0471223 0.03207823\n#> 3 1.0283730 0.04529990\n#> 4 1.1099761 0.04421705\n#> 5 0.7004132 0.04450937\n#> 6 0.7143166 0.04797191\n```\n\n\n:::\n:::\n\n\n\n\n\n## Simulating a single study - UMD {#sec-umd-sampling-distribution}\n\nThen we can plot the sampling distributions:\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](chapter5_files/figure-html/unnamed-chunk-12-1.png){width=672}\n:::\n:::\n\n\n\n\n\n## Simulating a single study - SMD\n\nThe idea is the same when simulating a SDM but we need extra steps. Let's adjust the previous function:\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsim_smd <- function(n1, n2 = NULL, D){\n  if(is.null(n2)) n2 <- n1 # same to n1 if null \n  g1 <- rnorm(n1, mean = 0, sd = 1)\n  g2 <- rnorm(n2, mean = D, sd = 1)\n  \n  v1 <- var(g1)\n  v2 <- var(g2)\n  \n  # pooled standard deviation\n  sp <- sqrt((v1 * (n1 - 1) + v2 * (n2 - 1)) / (n1 + n2 - 2))\n  \n  yi <- (mean(g2) - mean(g1)) / sp\n  vi <- (n1 + n2) / (n1 * n2) + yi^2/(2*(n1 + n2))\n  data.frame(yi, vi)\n}\n```\n:::\n\n\n\n\n\n## Simulating a single study - SMD\n\nWhen working with SMD, calculating the sampling variance can be challenging. @Veroniki2016-nw identified 16 different estimators with different properties. Furthermore, it is a common practice to correct the SDM effect and variance using the Hedges's correction [@Hedges1989-ip]. \n\nYou can directly implement another equation for the sampling variance or the Hedges's correction directly in the simulation function.\n\n## Simulating a single study - Pearson $\\rho$\n\nAnother common effect size is the Pearson correlation coefficient $\\rho$ (and the estimate $r$, see @eq-correlation). The variance of the correlation is calculated as:\n\n$$\nV_{r} = \\frac{(1 - r^2)^2}{n - 1}\n$$\n\n## Simulating a single study - Pearson $\\rho$\n\nThere is a huge dependency between $r$ and it's sampling variance (similar to the Cohen's $d$):\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nn <- 50\nr <- seq(0, 1, 0.01)\nv <- (1 - r^2)^2 / (n - 1) \n\nplot(r, v, type = \"l\", main = \"N = 50\", xlab = \"r\", ylab = latex2exp::TeX(\"$V_r$\"))\n```\n\n::: {.cell-output-display}\n![](chapter5_files/figure-html/unnamed-chunk-14-1.png){width=672}\n:::\n:::\n\n\n\n\n\n## Simulating a single study - Pearson $\\rho$\n\nFor this reason the so-called Fisher's $z$ transformation is used to stabilize the relationship.\n\n$$\nz = \\frac{\\log{\\frac{1 + r}{1 - r}}}{2}\n$$\n\n$$\nV_z = \\frac{1}{n - 3}\n$$\n\nNow the variance is completely independent from the correlation value.\n\n## Simulating a single study - Pearson $\\rho$\n\nThis is the relationship between $r$ and $z$:\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nn <- 50\nr <- seq(-1, 1, 0.01)\nv <- (1 - r^2)^2 / (n - 1) \nz <- log((1 + r)/(1 - r))/2\n\nplot(z, r, type = \"l\", xlab = \"Fisher's z\", ylab = \"Correlation\", main = \"Correlation to Fisher's z\")\n```\n\n::: {.cell-output-display}\n![](chapter5_files/figure-html/unnamed-chunk-15-1.png){width=672}\n:::\n:::\n\n\n\n\n\n## Simulating a single study - Pearson $\\rho$\n\nTo simulate a study using correlations we can use the `MASS::mvrnorm()` function that can generate correlated data from a multivariate normal distribution.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsim_r <- function(n, r){\n  R <- r + diag(1 - r, nrow = 2) # 2 x 2 correlation matrix\n  X <- MASS::mvrnorm(n, mu = c(0, 0), Sigma = R) # the means are not relevant here\n  r <- cor(X)[1, 2] # extract correlation\n  vr <- (1 - r^2)^2 / (n - 1)  # variance of r\n  yi <- log((1 + r)/(1 - r))/2 # fisher z\n  vi <- 1 / (n - 3) # fisher z variance\n  data.frame(yi, vi, r, vr) # including also the pearson correlation and variance\n}\n```\n:::\n\n\n\n\n\n## Simulating a single study - Pearson $\\rho$\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsim_r(100, 0.5)\n#>          yi         vi         r          vr\n#> 1 0.4202373 0.01030928 0.3971303 0.007166144\nsim_r(50, 0.8)\n#>         yi        vi       r          vr\n#> 1 1.099418 0.0212766 0.80029 0.002638084\n\n# also here the sampling distributions\nstudies <- replicate(1000, sim_r(50, 0.7), simplify = FALSE)\nstudies <- do.call(rbind, studies)\nsummary(studies)\n#>        yi               vi                r                vr          \n#>  Min.   :0.4589   Min.   :0.02128   Min.   :0.4292   Min.   :0.001249  \n#>  1st Qu.:0.7695   1st Qu.:0.02128   1st Qu.:0.6466   1st Qu.:0.003953  \n#>  Median :0.8686   Median :0.02128   Median :0.7007   Median :0.005289  \n#>  Mean   :0.8726   Mean   :0.02128   Mean   :0.6954   Mean   :0.005534  \n#>  3rd Qu.:0.9690   3rd Qu.:0.02128   3rd Qu.:0.7483   3rd Qu.:0.006910  \n#>  Max.   :1.3230   Max.   :0.02128   Max.   :0.8675   Max.   :0.013583\n```\n:::\n\n\n\n\n\n## More on effect sizes\n\nThe same logic can be applied to any situation. Just understand the data generation process, find the effect size equations and generate data.\n\n- @Borenstein2009-mo for all effect sizes equations. Also with equations to convert among effect sizes (useful in real-world meta-analyses)   \n- the [`metafor::escalc()`](https://wviechtb.github.io/metafor/reference/escalc.html) function implements basically any effect size. You can see also the [source code](https://github.com/wviechtb/metafor/blob/master/R/escalc.r) to see the actual R implementation.\n- [Guide to effect sizes](https://matthewbjane.quarto.pub/guide-to-effect-sizes-and-confidence-intervals): a modern and complete overview of effect sizes\n\n## Simulating from sampling distributions [#extra]\n\nThe previous simulation examples are participant-level simulations. In fact we simulated $n$ observations then we aggregated calculating the effect sizes.\n\n. . .\n\nThis is the most flexible and general data simulation strategy but is computationally not efficient.\n\n. . .\n\nAnother strategy individuate the exact effect size sampling distribution. Then we can sample directly from it. The downside is that we need to derive (or find) the equation.\n\n## Simulating from sampling distributions [#extra]\n\nFor example, when generating UMD we can simulate from the sampling distribution presented in @sec-umd-sampling-distribution.\n\n$$\ny_i \\sim \\mathcal{N}(\\theta, \\sqrt{\\sigma^2_i})\n$$\n$$\n\\sigma^2_i \\sim \\frac{\\chi^2_{n_1 + n_2 - 2}}{n_1 + n_2 - 2} (\\frac{1}{n_1} + \\frac{1}{n_2})\n$$\n\nIn this way we can sample $k$ effects and sampling variances directly from the sampling distributions. Without generating data and then aggregate.\n\n## Simulating from sampling distributions [#extra]\n\nWe can again put everything within a function:\n\n```r\nsim_k_umd <- function(k, D, n1, n2 = NULL){\n  if(is.null(n2)) n2 <- n1\n  yi <- rnorm(k, D, sqrt(1/n1 + 1/n2))\n  vi <- (rchisq(k, n1 + n2 - 2) / (n1 + n2 - 2)) * (1/n1 + 1/n2)\n  data.frame(yi, vi)\n}\n```\n\n## Simulating from sampling distributions [#extra]\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsim_k_umd(k = 10, D = 0.5, n1 = 50)\n#>            yi         vi\n#> 1  0.59626901 0.03651621\n#> 2  0.29367154 0.04243381\n#> 3  0.55988406 0.05268682\n#> 4  0.36752058 0.04880525\n#> 5  0.05553809 0.03851076\n#> 6  0.54829084 0.04382554\n#> 7  0.31989788 0.03740828\n#> 8  0.56871934 0.04222310\n#> 9  0.20966467 0.04416763\n#> 10 0.73415866 0.03596558\n```\n:::\n\n\n\n\n\n## Simulating from sampling distributions [#extra]\n\nWe can compare the two methods and see that we are sampling from the same data generation process.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nk <- 1e4\ns_umd <- sim_k_umd(k, D = 1, n1 = 50)\nip_umd <- replicate(k, sim_umd(n1 = 50, D = 1), simplify = FALSE)\nip_umd <- do.call(rbind, ip_umd)\n```\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![](chapter5_files/figure-html/unnamed-chunk-20-1.png){width=672}\n:::\n:::\n\n\n\n\n\n## Simulating from sampling distributions [#extra]\n\nThe actual advantage is in terms of computational speed. To simulate $k = 10$ studies for 1000 times (similar to a standard Monte Carlo simulation):\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbench <- microbenchmark::microbenchmark(\n  sampling = sim_k_umd(k = 10, n1 = 50, D = 1),\n  participant = replicate(10, sim_umd(n1 = 50, D = 1)),\n  times = 1000 \n)\n\n(bench <- summary(bench))\n#>          expr      min       lq      mean   median        uq      max neval cld\n#> 1    sampling  121.468  127.109  135.6278  131.628  138.0295  263.506  1000  a \n#> 2 participant 1532.892 1566.545 1659.8272 1593.727 1610.3680 6359.687  1000   b\n\nbench$mean[2] / bench$mean[1] # faster\n#> [1] 12.2381\n```\n:::\n\n\n\n\n\n# Simulation setup {.section}\n\n## Notation {.smaller}\n\nMeta-analysis notation is a little bit inconsistent in textbooks and papers. We define here some rules to simplify the work.\n\n- $k$ is the number of studies\n- $n_j$ is the sample size of the group $j$ within a study\n- $y_i$ are the observed effect size included in the meta-analysis\n- $\\sigma_i^2$ are the observed sampling variance of studies and $\\epsilon_i$ are the sampling errors\n- $\\theta$ is the equal-effects parameter (see @eq-ee1)\n- $\\delta_i$ is the random-effect (see @eq-re-mod2)\n- $\\mu_\\theta$ is the average effect of a random-effects model (see @eq-re-mod1)\n- $w_i$ are the meta-analysis weights\n- $\\tau^2$ is the heterogeneity (see @eq-re-mod2)\n- $\\Delta$ is the (generic) population effect size\n- $s_j^2$ is the variance of the group $j$ within a study\n\n## Simulation setup\n\nGiven the introduction to effect sizes, from now we will simulate data using UMD and the individual-level data. \n\nBasically we are simulating an effect size $D$ coming from the comparison of two independent groups $G_1$ and $G_2$.\n\nEach group is composed by $n$ participants measured on a numerical outcome (e.g., reaction times)\n\n## Simulation setup\n\nA more general, clear and realistic approach to simulate data is by generating $k$ studies with same/different sample sizes and (later) true effect sizes.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nk <- 10 # number of studies\nn1 <- n2 <- 10 + rpois(k, 30 - 10) # sample size from poisson distribution with lambda 40 and minimum 10\nD <- 0.5 # effect size\n\nyi <- rep(NA, k)\nvi <- rep(NA, k)\n  \nfor(i in 1:k){\n  g1 <- rnorm(n1[i], 0, 1)\n  g2 <- rnorm(n2[i], D, 1)\n  yi[i] <- mean(g2) - mean(g1)\n  vi[i] <- var(g1)/n1[i] + var(g2)/n2[i]\n}\n  \nsim <- data.frame(id = 1:k, yi, vi)\n\nhead(sim)\n#>   id          yi         vi\n#> 1  1  0.32347839 0.08199370\n#> 2  2 -0.06229478 0.09007477\n#> 3  3  0.42661729 0.06047760\n#> 4  4  0.99254822 0.06154313\n#> 5  5  0.28434245 0.04539959\n#> 6  6  0.06321880 0.08937580\n```\n:::\n\n\n\n\n\n## Simulation setup\n\nWe can again put everything within a function:\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsim_studies <- function(k, es, n1, n2 = NULL){\n  if(length(n1) == 1) n1 <- rep(n1, k)\n  if(is.null(n2)) n2 <- n1\n  if(length(es) == 1) es <- rep(es, k)\n  \n  yi <- rep(NA, k)\n  vi <- rep(NA, k)\n  \n  for(i in 1:k){\n    g1 <- rnorm(n1[i], 0, 1)\n    g2 <- rnorm(n2[i], es[i], 1)\n    yi[i] <- mean(g2) - mean(g1)\n    vi[i] <- var(g1)/n1[i] + var(g2)/n2[i]\n  }\n  \n  sim <- data.frame(id = 1:k, yi, vi, n1 = n1, n2 = n2)\n  \n  # convert to escalc for using metafor methods\n  sim <- metafor::escalc(yi = yi, vi = vi, data = sim)\n  \n  return(sim)\n}\n```\n:::\n\n\n\n\n\n## Simulation setup - Disclaimer\n\nThe proposed simulation approach using a `for` loop and separated vectors. For the purpose of the workshop this is the best option. In real-world meta-analysis simulations you can choose a more functional approach starting from a simulation grid as `data.frame` and mapping the simulation functions.\n\nFor some examples see:\n\n- @Gambarota2023-on\n- [www.jepusto.com/simulating-correlated-smds](https://www.jepusto.com/simulating-correlated-smds)\n\n## Simulation setup - Disclaimer\n\nFor a more extended overview of the simulation setup we have an entire paper. Supplementary materials ([github.com/shared-research/simulating-meta-analysis](https://github.com/shared-research/simulating-meta-analysis)) contains also more examples for complex (multilevel and multivariate models.)\n\n![](img/gambarota2023.png){fig-align=\"center\"}\n\n# Combining studies {.section}\n\n## Combining studies\n\nLet's imagine to have $k = 10$ studies, a $D = 0.5$ and heterogeneous sample sizes in each study.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nk <- 10\nD <- 0.5\nn <- 10 + rpois(k, lambda = 20) \ndat <- sim_studies(k = k, es = D, n1 = n)\nhead(dat)\n#> \n#>   id     yi     vi n1 n2 \n#> 1  1 0.2457 0.0614 32 32 \n#> 2  2 0.4531 0.0645 38 38 \n#> 3  3 0.1710 0.0829 19 19 \n#> 4  4 0.5329 0.0673 31 31 \n#> 5  5 0.4363 0.0539 26 26 \n#> 6  6 0.3225 0.0526 40 40\n```\n:::\n\n\n\n\n\n. . .\n\nWhat is the best way to combine the studies?\n\n## Combining studies\n\nWe can take the average effect size and considering it as a huge study. This can be considered the best way to combine the effects.\n\n$$\n\\hat{D} = \\frac{\\sum^{k}_{i = 1} D_i}{k}\n$$\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmean(dat$yi)\n#> [1] 0.3595696\n```\n:::\n\n\n\n\n\n. . .\n\nIt is appropriate? What do you think? Are we missing something?\n\n## Weighting studies\n\nWe are not considering that some studies, despite providing a similar effect size could give more information. An higher sample size (or lower sampling variance) produce a more reliable estimation.\n\n. . .\n\nWould you trust more a study with $n = 100$ and $D = 0.5$ or a study with $n = 10$ and $D = 0.5$? The \"meta-analysis\" that we did before is completely ignoring this information.\n\n## Weighting studies\n\nWe need to find a value (called weight $w_i$) that allows assigning more trust to a study because it provide more information. \n\n. . .\n\nThe simplest weights are just the sample size, but in practice we use the so-called **inverse-variance weighting**. We use the (inverse) of the sampling variance of the effect size to weight each study. \n\n. . .\n\nThe basic version of a meta-analysis is just a **weighted average**:\n\n$$\n\\overline D_w = \\frac{\\sum^k_{i = 1}{w_iD_i}}{\\sum^k_{i = 1}{w_i}}\n$$\n\n. . .\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwi <- 1/dat$vi\nsum(dat$yi * wi) / sum(wi)\n#> [1] 0.3663024\n# weighted.mean(dat$yi, wi)\n```\n:::\n\n\n\n\n\n## Weighting studies\n\nGraphically, the two models can be represented in this way:\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\ndw <- weighted.mean(dat$yi, 1/dat$vi)\ndunw <- mean(dat$yi)\n\nunw_plot <- ggplot(dat, aes(x = yi, y = factor(id))) +\n  geom_point(size = 3) +\n  xlim(c(-0.5, 1.5)) +\n  geom_vline(xintercept = dunw) +\n  xlab(latex2exp::TeX(\"$y_i$\")) +\n  ylab(\"Study\") +\n  theme_minimal(15) +\n  annotate(\"label\", x = 0.5, y = k + 0.4, label = latex2exp::TeX(sprintf(\"$\\\\bar{D} = %.2f$\", dunw))) +\n  geom_label(aes(x = 1, y = id, label = paste0(\"n = \", n1)))\n\nw_plot <- ggplot(dat, aes(x = yi, y = factor(id))) +\n  geom_point(aes(size = 1/vi),\n             show.legend = FALSE) +\n  xlim(c(-0.5, 1.5)) +\n  geom_vline(xintercept = dw) +\n  xlab(latex2exp::TeX(\"$y_i$\")) +\n  ylab(\"Study\") +\n  theme_minimal(15) +\n  annotate(\"label\", x = 0.5, y = k + 0.4, label = latex2exp::TeX(sprintf(\"$\\\\bar{D}_w = %.2f$\", dw))) +\n  geom_label(aes(x = 1, y = id, label = paste0(\"n = \", n1)))\n\nunw_plot + w_plot\n```\n\n::: {.cell-output-display}\n![](chapter5_files/figure-html/unnamed-chunk-27-1.png){width=672}\n:::\n:::\n\n\n\n\n\n# Equal-effects (EE) meta-analysis {.section}\n\n## EE meta-analysis\n\nWhat we did in the last example (the weighted mean) is the exactly a meta-analysis model called **equal-effects** (or less precisely fixed-effect). The assumptions are very simple:\n\n- there is a unique, true effect size to estimate $\\theta$\n- each study is a more or less precise estimate of $\\theta$\n- there is no TRUE variability among studies. The observed variability is due to studies that are imprecise (i.e., sampling error)\n- assuming that each study has a very large sample size, the observed variability is close to zero.\n\n## EE meta-analysis, formally\n\n$$\ny_i = \\theta + \\epsilon_i\n$$ {#eq-ee1}\n\n$$\n\\epsilon_i \\sim \\mathcal{N}(0, \\sigma^2_i)\n$$ {#eq-ee2}\n\nWhere $\\sigma^2_i$ is the vector of sampling variabilities of $k$ studies. This is a standard linear model but with heterogeneous sampling variances.\n\n## EE meta-analysis\n\n\n\n\n\n::: {.cell}\n\n:::\n\n\n\n\n\n<!-- TODO fix the image, ggplot version -->\n\n## Simulating an EE model\n\nWhat we were doing with the `sim_studies()` function so far was simulating an EE model. In fact, there were a single $\\theta$ parameter and the observed variability was a function of the `rnorm()` randomness.\n\nBased on previous assumptions and thinking a little bit, what could be the result of simulating studies with a very large $n$?\n\n. . .\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nns <- c(10, 50, 100, 1000, 1e4)\nD <- 0.5\ndats <- lapply(ns, function(n) sim_studies(10, es = D, n1 = n))\n```\n:::\n\n\n\n\n\n## Simulating an EE modelm {#sec-ee-impact-n}\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](chapter5_files/figure-html/unnamed-chunk-30-1.png){width=1440}\n:::\n:::\n\n\n\n\n\n## Simulating an EE model\n\nFormulating the model as a intercept-only regression (see Equations [@eq-ee1] and [@eq-ee2]) we can generate data directly:\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nD <- 0.5\nn <- 30\nk <- 10\n\nyi <- D + rnorm(k, 0, sqrt(1/n + 1/n))\n# or equivalently\n# yi <- rnorm(k, D, sqrt(1/n + 1/n))\n```\n:::\n\n\n\n\n\nAs we did for the aggregated data approach. Clearly we need to simulate also the `vi` vector from the appropriate distribution. Given that we simulated data starting from the participant-level the uncertainty of `yi` and `vi` is already included.\n\n## Fitting an EE model\n\nThe model can be fitted using the `metafor::rma()` function, with `method = \"EE\"`^[There is a confusion about the *fixed-effects* vs *fixed-effect* (no *s*) and *equal-effects* models. See [https://wviechtb.github.io/metafor/reference/misc-models.html](https://wviechtb.github.io/metafor/reference/misc-models.html)].\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntheta <- 0.5\nk <- 15\nn <- 10 + rpois(k, 30 - 10)\ndat <- sim_studies(k = k, es = theta, n1 = n)\nfit <- rma(yi, vi, data = dat, method = \"EE\")\nsummary(fit)\n#> \n#> Equal-Effects Model (k = 15)\n#> \n#>   logLik  deviance       AIC       BIC      AICc   \n#>  -3.2628   19.0853    8.5257    9.2337    8.8334   \n#> \n#> I^2 (total heterogeneity / total variability):   26.65%\n#> H^2 (total variability / sampling variability):  1.36\n#> \n#> Test for Heterogeneity:\n#> Q(df = 14) = 19.0853, p-val = 0.1617\n#> \n#> Model Results:\n#> \n#> estimate      se    zval    pval   ci.lb   ci.ub      \n#>   0.4590  0.0667  6.8778  <.0001  0.3282  0.5898  *** \n#> \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n\n\n\n\n\n## Interpreting an EE model\n\n- The first section (`logLik`, `deviance`, etc.) presents some general model statistics and information criteria\n- The $I^2$ and $H^2$ are statistics evaluating the observed heterogeneity (see next slides)\n- The `Test of Heterogeneity` section presents the test of the $Q$ statistics for the observed heterogeneity (see next slides)\n- The `Model Results` section presents the estimation of the $\\theta$ parameter along with the standard error and the Wald $z$ test ($H_0: \\theta = 0$)\n\nThe `metafor` package has a several well documented functions to calculate and plot model results, residuals analysis etc.\n\n## Interpreting an EE model\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(fit) # general plots\n```\n\n::: {.cell-output-display}\n![](chapter5_files/figure-html/unnamed-chunk-33-1.png){width=672}\n:::\n:::\n\n\n\n\n\n## Interpreting an EE Model\n\nThe main function for plotting model results is the `forest()` function that produce the forest plot.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nforest(fit)\n```\n\n::: {.cell-output-display}\n![](chapter5_files/figure-html/unnamed-chunk-34-1.png){width=672}\n:::\n:::\n\n\n\n\n\n## Interpreting an EE Model\n\nWe did not introduced the concept of heterogeneity, but the $I^2$, $H^2$ and $Q$ statistics basically evaluate if the observed heterogeneity should be attributed to **sampling variability** (uncertainty in estimating $\\theta$ because we have a limited $k$ and $n$) or **sampling variability** plus other sources of heterogeneity.\n\n## EE model as a weighted Average\n\nFormally $\\theta$ is estimated as [see @Borenstein2009-mo, p. 66]\n\n$$\n\\hat{\\theta} = \\frac{\\sum^k_{i = 1}{w_iy_i}}{\\sum^k_{i = 1}{w_i}}; \\;\\;\\; w_i = \\frac{1}{\\sigma^2_i}\n$$\n\n$$\nSE_{\\theta} = \\frac{1}{\\sum^k_{i = 1}{w_i}}\n$$\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwi <- 1/dat$vi\ntheta_hat <- with(dat, sum(yi * wi)/sum(wi))\nse_theta_hat <- sqrt(1/sum(wi))\nc(theta = theta_hat, se = se_theta_hat, z = theta_hat / se_theta_hat)\n#>      theta         se          z \n#> 0.45897245 0.06673282 6.87776180\n```\n:::\n\n\n\n\n\n# Random-effects (RE) meta-analysis {.section}\n\n## Are the EE assumptions realistic?\n\nThe EE model is appropriate if our studies are somehow **exact replications** of the exact same effect. We are assuming that there is **no real variability**.\n\n. . .\n\nHowever, meta-analysis rarely report the results of $k$ exact replicates. It is more common to include **studies answering the same research question** but with different methods, participants, etc.\n\n. . .\n\n- people with different ages or other participant-level differences\n- different methodology\n- ...\n\n## Are the EE assumptions realistic?\n\n. . .\n\nIf we relax the previous assumption we are able to combine studies that are not exact replications. \n\n. . .\n\nThus the real effect $\\theta$ is no longer a single **true** value but can be larger or smaller in some conditions.\n\n. . .\n\nIn other terms we are assuming that there could be some variability (i.e., **heterogeneity**) among studies that is independent from the sample size. Even with studies with $\\lim_{n\\to\\infty}$ the observed variability is not zero.\n\n## Random-effects model (RE)\n\nWe can extend the EE model including another source of variability, $\\tau^2$. $\\tau^2$ is the true heterogeneity among studies caused by methdological differences or intrisic variability in the phenomenon.\n\nFormally we can extend @eq-ee1 as:\n$$\ny_i = \\mu_{\\theta} + \\delta_i + \\epsilon_i\n$$ {#eq-re-mod1}\n\n$$\n\\delta_i \\sim \\mathcal{N}(0, \\tau^2)\n$$ {#eq-re-mod2}\n\n$$\n\\epsilon_i \\sim \\mathcal{N}(0, \\sigma^2_i)\n$$\n\nWhere $\\mu_{\\theta}$ is the average effect size and $\\delta_i$ is the study-specific deviation from the average effect (regulated by $\\tau^2$). Clearly each study specific effect is $\\theta_i = \\mu_{\\theta} + \\delta_i$.\n\n## RE model\n\n\n\n\n\n::: {.cell}\n\n:::\n\n\n\n\n\n<!-- TODO fix the image, ggplot version -->\n\n## RE model estimation\n\nGiven that we extended the EE model equation. Also the estimation of the average effect need to be extended. Basically the RE is still a weighted average but weights need to include also $\\tau^2$.\n\n$$\n\\overline y = \\frac{\\sum_{i = 1}^k y_iw^*_i}{\\sum_{i = 1}^k w^*_i}\n$$ {#eq-re1}\n\n$$\nw^*_i = \\frac{1}{\\sigma^2_i + \\tau^2}\n$$ {#eq-re2}\n\nThe weights are different compared to the EE model. Extremely precise/imprecise studies will have less impact in the RE model.\n\n## RE vs EE model\n\nThe crucial difference with the EE model is that even with large $n$, only the $\\mu_{\\theta} + \\delta_i$ are estimated (almost) without error. As long $\\tau^2 \\neq 0$ there will be variability in the effect sizes.\n\n\n\n\n\n::: {.cell}\n\n:::\n\n\n\n\n\n<!-- TODO fix the image, ggplot version -->\n\n## Simulating a RE Model\n\nTo simulate the RE model we simply need to include $\\tau^2$ in the EE model simulation.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nk <- 15 # number of studies\nmu <- 0.5 # average effect\ntau2 <- 0.1 # heterogeneity\nn <- 10 + rpois(k, 30 - 10) # sample size\ndeltai <- rnorm(k, 0, sqrt(tau2)) # random-effects\nthetai <- mu + deltai # true study effect\n\ndat <- sim_studies(k = k, es = thetai, n1 = n)\n\nhead(dat)\n#> \n#>   id     yi     vi n1 n2 \n#> 1  1 0.6838 0.0860 26 26 \n#> 2  2 0.4613 0.0701 34 34 \n#> 3  3 0.5666 0.0631 35 35 \n#> 4  4 0.7017 0.0705 29 29 \n#> 5  5 0.1014 0.0918 26 26 \n#> 6  6 0.6434 0.0840 27 27\n```\n:::\n\n\n\n\n\n## Simulating a RE model\n\nAgain, we can put everything within a function expanding the previous `sim_studies()` by including $\\tau^2$:\n\n\n\n\n\n\n\n\n\n\n\n```r\nsim_studies <- function(k, es, tau2 = 0, n1, n2 = NULL, add = NULL){\n  if(length(n1) == 1) n1 <- rep(n1, k)\n  if(is.null(n2)) n2 <- n1\n  if(length(es) == 1) es <- rep(es, k)\n  \n  yi <- rep(NA, k)\n  vi <- rep(NA, k)\n  \n  # random effects\n  deltai <- rnorm(k, 0, sqrt(tau2))\n  \n  for(i in 1:k){\n    g1 <- rnorm(n1[i], 0, 1)\n    g2 <- rnorm(n2[i], es[i] + deltai[i], 1)\n    yi[i] <- mean(g2) - mean(g1)\n    vi[i] <- var(g1)/n1[i] + var(g2)/n2[i]\n  }\n  \n  sim <- data.frame(id = 1:k, yi, vi, n1 = n1, n2 = n2)\n  \n  if(!is.null(add)){\n    sim <- cbind(sim, add)\n  }\n  \n  # convert to escalc for using metafor methods\n  sim <- metafor::escalc(yi = yi, vi = vi, data = sim)\n  \n  return(sim)\n}\n```\n\n## Simulating a RE model\n\nThe data are similar to the EE simulation but we have an extra source of heterogeneity.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\ndat |>\n  summary() |>\n  qforest()\n```\n\n::: {.cell-output-display}\n![](chapter5_files/figure-html/unnamed-chunk-40-1.png){width=672}\n:::\n:::\n\n\n\n\n\n## Simulating a RE model\n\nTo see the actual impact of $\\tau^2$ we can follow the same approach of @sec-ee-impact-n thus using a large $n$. The sampling variance `vi` of each study is basically 0.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# ... other parameters as before\nn <- 1e4\ndeltai <- rnorm(k, 0, sqrt(tau2)) # random-effects\nthetai <- mu + deltai # true study effect\ndat <- sim_studies(k = k, es = thetai, n1 = n)\n# or equivalently \n# dat <- sim_studies(k = k, es = mu, tau2 = tau2, n1 = n)\n\nhead(dat)\n#> \n#>   id     yi     vi    n1    n2 \n#> 1  1 0.5474 0.0002 10000 10000 \n#> 2  2 0.0975 0.0002 10000 10000 \n#> 3  3 0.9151 0.0002 10000 10000 \n#> 4  4 0.4185 0.0002 10000 10000 \n#> 5  5 0.0186 0.0002 10000 10000 \n#> 6  6 0.1722 0.0002 10000 10000\n```\n:::\n\n\n\n\n\n## Simulating a RE Model\n\nClearly, compared to @sec-ee-impact-n, even with large $n$ the variability is not reduced because $\\tau^2 \\neq 0$. As $\\tau^2$ approach zero the EE and RE models are similar.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\ndat |>\n  summary() |>\n  qforest()\n```\n\n::: {.cell-output-display}\n![](chapter5_files/figure-html/unnamed-chunk-42-1.png){width=672}\n:::\n:::\n\n\n\n\n\n## RE model estimation\n\nGiven that we extended the EE model equation. Also the estimation of the average effect need to be extended. Basically the RE is still a weighted average but weights need to include also $\\tau^2$.\n\n$$\n\\overline y = \\frac{\\sum_{i = 1}^k y_iw^*_i}{\\sum_{i = 1}^k w^*_i}\n$$ {#eq-re1}\n\n$$\nw^*_i = \\frac{1}{\\sigma^2_i + \\tau^2}\n$$ {#eq-re2}\n\nThe weights are different compared to the EE model. Extremely precise/imprecise studies will have less impact in the RE model.\n\n## Fitting a RE model\n\nIn R we can use the `metafor::rma()` function using the `method = \"REML\"`.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit <- rma(yi, vi, data = dat, method = \"REML\")\nsummary(fit)\n#> \n#> Random-Effects Model (k = 15; tau^2 estimator: REML)\n#> \n#>   logLik  deviance       AIC       BIC      AICc   \n#>  -7.8430   15.6860   19.6860   20.9641   20.7769   \n#> \n#> tau^2 (estimated amount of total heterogeneity): 0.1793 (SE = 0.0679)\n#> tau (square root of estimated tau^2 value):      0.4235\n#> I^2 (total heterogeneity / total variability):   99.89%\n#> H^2 (total variability / sampling variability):  898.45\n#> \n#> Test for Heterogeneity:\n#> Q(df = 14) = 12514.0313, p-val < .0001\n#> \n#> Model Results:\n#> \n#> estimate      se    zval    pval   ci.lb   ci.ub      \n#>   0.5501  0.1094  5.0287  <.0001  0.3357  0.7646  *** \n#> \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n\n\n\n\n\n## Intepreting the RE model\n\nThe model output is quite similar to the EE model and also the intepretation is similar.\n\nThe only extra section is `tau^2/tau` that is the estimation of the between-study heterogeneity.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(fit)\n#> \n#> Random-Effects Model (k = 15; tau^2 estimator: REML)\n#> \n#>   logLik  deviance       AIC       BIC      AICc   \n#>  -7.8430   15.6860   19.6860   20.9641   20.7769   \n#> \n#> tau^2 (estimated amount of total heterogeneity): 0.1793 (SE = 0.0679)\n#> tau (square root of estimated tau^2 value):      0.4235\n#> I^2 (total heterogeneity / total variability):   99.89%\n#> H^2 (total variability / sampling variability):  898.45\n#> \n#> Test for Heterogeneity:\n#> Q(df = 14) = 12514.0313, p-val < .0001\n#> \n#> Model Results:\n#> \n#> estimate      se    zval    pval   ci.lb   ci.ub      \n#>   0.5501  0.1094  5.0287  <.0001  0.3357  0.7646  *** \n#> \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n\n\n\n\n\n\n## Estimating $\\tau^2$\n\n\n![](img/langan2017.png){fig-align=\"center\" width=80%}\n\n## Estimating $\\tau^2$\n\nThe **Restricted Maximum Likelihood** (REML) estimator is considered one of the best. We can compare the results using the `all_rma()` custom function that tests all the estimators^[The `filor::compare_rma()` function is similar to the `car::compareCoefs()` function].\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfitl <- all_rma(fit)\nround(filor::compare_rma(fitlist = fitl), 3)\n#>           DL     HE     HS    HSk     SJ     ML   REML     EB     PM    PMM\n#> b      0.550  0.550  0.550  0.550  0.550  0.550  0.550  0.550  0.550  0.550\n#> se     0.109  0.109  0.105  0.109  0.109  0.106  0.109  0.109  0.109  0.112\n#> zval   5.042  5.029  5.219  5.042  5.029  5.205  5.029  5.029  5.029  4.909\n#> pval   0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000\n#> ci.lb  0.336  0.336  0.344  0.336  0.336  0.343  0.336  0.336  0.336  0.330\n#> ci.ub  0.764  0.765  0.757  0.764  0.765  0.757  0.765  0.765  0.765  0.770\n#> I2    99.888 99.889 99.880 99.888 99.889 99.881 99.889 99.889 99.889 99.894\n#> tau2   0.178  0.179  0.166  0.178  0.179  0.167  0.179  0.179  0.179  0.188\n```\n:::\n\n\n\n\n\n## Intepreting heterogeneity $\\tau^2$\n\nLooking at @eq-re-mod2, $\\tau^2$ is essentially the variance of the random-effect. This means that we can intepret it as the variability (or the standard deviation) of the true effect size distribution.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\ntau2s <- c(0.01, 0.05, 0.1, 0.2)\ntau2s_t <- latex2exp::TeX(sprintf(\"$\\\\tau^2 = %.2f$\", tau2s))\n\npar(mfrow = c(1, 3))\nhist(rnorm(1e4, 0, sqrt(tau2s[1])), xlim = c(-2, 2), xlab = latex2exp::TeX(\"$y_i$\"), main = tau2s_t[1], probability = TRUE, ylim = c(0, 4.5), col = \"dodgerblue\")\nhist(rnorm(1e4, 0, sqrt(tau2s[2])), xlim = c(-2, 2), xlab = latex2exp::TeX(\"$y_i$\"), main = tau2s_t[2], probability = TRUE, ylim = c(0, 4.5), col = \"dodgerblue\")\n#hist(rnorm(1e4, 0, sqrt(tau2s[3])), xlim = c(-2, 2), xlab = latex2exp::TeX(\"$y_i$\"), main = tau2s_t[3], probability = TRUE, ylim = c(0, 5))\nhist(rnorm(1e4, 0, sqrt(tau2s[4])), xlim = c(-2, 2), xlab = latex2exp::TeX(\"$y_i$\"), main = tau2s_t[4], probability = TRUE, ylim = c(0, 4.5), col = \"dodgerblue\")\n```\n\n::: {.cell-output-display}\n![](chapter5_files/figure-html/unnamed-chunk-46-1.png){width=672}\n:::\n:::\n\n\n\n\n\n## Intepreting $\\tau^2$\n\nAs in the previus plot we can assume $n = \\infty$ and generate true effects from @eq-re-mod2. In this way we understand the impact of assuming (or estimating) a certain $\\tau^2$.\n\nFor example, a $\\tau = 0.2$ and a $\\mu_{\\theta} = 0.5$, 50% of the true effects ranged between:\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nD <- 0.5\nyis <- D + rnorm(1e5, 0, 0.2)\nquantile(yis, c(0.75, 0.25))\n#>       75%       25% \n#> 0.6334614 0.3648225\n```\n:::\n\n\n\n\n\n## The $Q$ Statistics^[See @Harrer2021-go (Chapter 5) and @Hedges2019-ry for an overview about the Q statistics]\n\nThe Q statistics is used to make inference on the heterogeneity. Can be considered as a weighted sum of squares:\n\n$$\nQ = \\sum^k_{i = 1}w_i(y_i - \\hat \\mu)^2\n$$\n\nWhere $\\hat \\mu$ is EE estimation (regardless if $\\tau^2 \\neq 0$) and $w_i$ are the inverse-variance weights. Note that in the case of $w_1 = w_2 ... = w_i$, Q is just a standard sum of squares (or deviance).\n\n## The $Q$ Statistics\n\n- Given that we are summing up squared distances, they should be approximately $\\chi^2$ with $df = k - 1$. In case of no heterogeneity ($\\tau^2 = 0$) the observed variability is only caused by sampling error and the expectd value of the $\\chi^2$ is just the degrees of freedom ($df = k - 1$).\n- In case of $\\tau^2 \\neq 0$, the expected value is $k - 1 + \\lambda$ where $\\lambda$ is a non-centrality parameter.\n- In other terms, if the expected value of $Q$ exceed the expected value assuming no heterogeneity, we have evidence that $\\tau^2 \\neq 0$.\n\n## The $Q$ Statistics\n\nLet's try a more practical approach. We simulate a lot of meta-analysis with and without heterogeneity and we check the Q statistics.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nget_Q <- function(yi, vi){\n  wi <- 1/vi\n  theta_ee <- weighted.mean(yi, wi)\n  sum(wi*(yi - theta_ee)^2)\n}\n\nk <- 30\nn <- 30\ntau2 <- 0.1\nnsim <- 1e4\n\nQs_tau2_0 <- rep(0, nsim)\nQs_tau2 <- rep(0, nsim)\nres2_tau2_0 <- vector(\"list\", nsim)\nres2_tau2 <- vector(\"list\", nsim)\n\nfor(i in 1:nsim){\n  dat_tau2_0 <- sim_studies(k = 30, es = 0.5, tau2 = 0, n1 = n)\n  dat_tau2 <- sim_studies(k = 30, es = 0.5, tau2 = tau2, n1 = n)\n  \n  theta_ee_tau2_0 <- weighted.mean(dat_tau2_0$yi, 1/dat_tau2_0$vi)\n  theta_ee <- weighted.mean(dat_tau2$yi, 1/dat_tau2$vi)\n  \n  res2_tau2_0[[i]] <- dat_tau2_0$yi - theta_ee_tau2_0\n  res2_tau2[[i]] <- dat_tau2$yi - theta_ee\n  \n  Qs_tau2_0[i] <- get_Q(dat_tau2_0$yi, dat_tau2_0$vi)\n  Qs_tau2[i] <- get_Q(dat_tau2$yi, dat_tau2$vi)\n}\n```\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![](chapter5_files/figure-html/unnamed-chunk-49-1.png){width=672}\n:::\n:::\n\n\n\n\n\n## The $Q$ Statistics\n\nLet's try a more practical approach. We simulate a lot of meta-analysis with and without heterogeneity and we check the Q statistics.\n\n. . .\n\n- clearly, in the presence of heterogeneity, the expected value of the Q statistics is higher (due to $\\lambda \\neq 0$) and also residuals are larger (the $\\chi^2$ is just a sum of squared weighted residuals)\n\n. . .\n\n- we can calculate a p-value for deviation from the $\\tau^2 = 0$ case as evidence agaist the absence of heterogeneity\n\n## $I^2$ [@Higgins2002-fh]\n\nWe have two sources of variability in a random-effects meta-analysis, the sampling variability $\\sigma_i^2$ and true heterogeneity $\\tau^2$. We can use the $I^2$ to express the interplay between the two.\n$$\nI^2 = 100\\% \\times \\frac{\\hat{\\tau}^2}{\\hat{\\tau}^2 + \\tilde{v}}\n$${#eq-i2}\n\n$$\n\\tilde{v} = \\frac{(k-1) \\sum w_i}{(\\sum w_i)^2 - \\sum w_i^2},\n$$\n\nWhere $\\tilde{v}$ is the typical sampling variability. $I^2$ is intepreted as the proportion of total variability due to real heterogeneity (i.e., $\\tau^2$)\n\n## $I^2$ [@Higgins2002-fh]^[see [https://www.meta-analysis-workshops.com/download/common-mistakes1.pdf](https://www.meta-analysis-workshops.com/download/common-mistakes1.pdf)]\n\nNote that we can have the same $I^2$ in two completely different meta-analysis. An high $I^2$ does not represent high heterogeneity. Let's assume to have two meta-analysis with $k$ studies and small ($n = 30$) vs large ($n = 500$) sample sizes. \n\nLet's solve @eq-i2 for $\\tau^2$ (using `filor::tau2_from_I2()`) and we found that the same $I^2$ can be obtained with two completely different $\\tau^2$ values:\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nn_1 <- 30\nvi_1 <- 1/n_1 + 1/n_1\ntau2_1 <- filor::tau2_from_I2(0.8, vi_1)\ntau2_1\n#> [1] 0.2666667\n\nn_2 <- 500\nvi_2 <- 1/n_2 + 1/n_2\ntau2_2 <- filor::tau2_from_I2(0.8, vi_2)\ntau2_2\n#> [1] 0.016\n```\n:::\n\n\n\n\n\n## $I^2$ [@Higgins2002-fh]\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nn_1 <- 30\nvi_1 <- 1/n_1 + 1/n_1\ntau2_1 <- filor::tau2_from_I2(0.8, vi_1)\ntau2_1\n#> [1] 0.2666667\n\nn_2 <- 500\nvi_2 <- 1/n_2 + 1/n_2\ntau2_2 <- filor::tau2_from_I2(0.8, vi_2)\ntau2_2\n#> [1] 0.016\n```\n:::\n\n\n\n\n\n. . .\n\nIn other terms, the $I^2$ can be considered a good index of heterogeneity only when the total variance ($\\tilde{v} + \\tau^2$) is similar.\n\n## What about $\\tilde{v}$?\n\n$\\tilde{v}$ is considered the \"typical\" within-study variability (see [https://www.metafor-project.org/doku.php/tips:i2_multilevel_multivariate](https://www.metafor-project.org/doku.php/tips:i2_multilevel_multivariate)). There are different estimators but @eq-tildev [@Higgins2002-fh] is the most common.\n\n$$\n\\tilde{v} = \\frac{(k-1) \\sum w_i}{(\\sum w_i)^2 - \\sum w_i^2}\n$$ {#eq-tildev}\n\n## What about $\\tilde{v}$?\n\nIn the hypothetical case where $\\sigma^2_1 = \\dots = \\sigma^2_k$, $\\tilde{v}$ is just $\\sigma^2$. This fact is commonly used to calculate the statistical power analytically [@Borenstein2009-mo, Chapter 29].\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nvtilde <- function(wi){\n  k <- length(wi)\n  (k - 1) * sum(wi) / (sum(wi)^2 - sum(wi^2))\n}\n\nk <- 20\n\n# same vi\nvi <- rep((1/30 + 1/30), k)\nhead(vi)\n#> [1] 0.06666667 0.06666667 0.06666667 0.06666667 0.06666667 0.06666667\nvtilde(1/vi)\n#> [1] 0.06666667\n\n# heterogeneous vi\nn <- 10 + rpois(k, 30 - 10)\nvi <- sim_vi(k = k, n1 = n)\nvtilde(1/vi)\n#> [1] 0.06389075\n```\n:::\n\n\n\n\n\n## What about $\\tilde{v}$?\n\nUsing simulations we can see that $\\tilde{v}$ with heterogenenous variances (i.e., sample sizes in this case) can be approximated by the central tendency of the sample size distribution. Note that we are fixing $\\sigma^2 = 1$ thus we are not including uncertainty.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nk <- 100 # number of studies\nn <- 30 # sample size\n\nvti <- rep(NA, 1e5)\n\nfor(i in 1:1e5){\n  ni <- rpois(k, n)\n  vi <- 1/ni + 1/ni\n  vti[i] <- vtilde(1/vi)\n}\n\n# vtilde calculated from lambda\nvt <- 1/n + 1/n\n```\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![](chapter5_files/figure-html/unnamed-chunk-54-1.png){width=672}\n:::\n:::\n\n\n\n\n\n## $H^2$\n\nThe $H^2$ is an alternative index of heterogeneity. Is calculated as:\n\n$$\nH^2 = \\frac{Q}{k - 1}\n$$\n\nWe defined $Q$ as the weighted sum of squares representing the total variability. $k - 1$ is the expected value of the $\\chi^2$ statistics (i.e., sum of squares) when $\\tau^2 = 0$ (or $\\lambda = 0$). \n\nThus $H^2$ is the ratio between total heterogeneity and sampling variability. Higher $H^2$ is associated with higher heterogeneity **relative** to the sampling variability. $H^2$ is not a measure of absolute heterogeneity.\n\n## $H^2$\n\nWhen we are fitting a RE model, the $I^2$ and $H^2$ equations are slightly different [@Higgins2002-fh]. See also the `metafor` [source code](https://github.com/cran/metafor/blob/994d26a65455fac90760ad6a004ec1eaca5856b1/R/rma.uni.r#L2459C30-L2459C30).\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nk <- 100\nmu <- 0.5\ntau2 <- 0.1\nn <- 30\n\ndat <- sim_studies(k = k, es = mu, tau2 = tau2, n1 = n)\nfit_re <- rma(yi, vi, data = dat, method = \"REML\")\nfit_ee <- rma(yi, vi, data = dat, method = \"EE\")\n\n# H2 with EE model\n\ntheta_ee <- fit_ee$b[[1]] # weighted.mean(dat$yi, 1/dat$vi)\nwi <- 1/dat$vi\nQ <- with(dat, sum((1/vi)*(yi - theta_ee)^2))\nc(Q, fit_ee$QE) # same\n#> [1] 350.1654 350.1654\n\nc(H2 = fit_ee$QE / (fit_ee$k - fit_ee$p), H2_model = fit_ee$H2) # same\n#>       H2 H2_model \n#> 3.537024 3.537024\n\n# H2 with RE model\n\nvt <- vtilde(1/dat$vi)\nc(H2 = fit_re$tau2 / vt + 1, H2_model = fit_re$H2) # same\n#>       H2 H2_model \n#> 3.495119 3.495119\n```\n:::\n\n\n\n\n\n## Confidence Intervals\n\nWhat is reported in the model summary as `ci.lb` and `ci.ub` refers to the 95% confidence interval representing the uncertainty in estimating the effect (or a meta-regression parameter).\n\nWithout looking at the equations, let's try to implement this idea using simulations.\n\n- choose $k$, $\\tau^2$ and $n$\n- simulate data (several times) accordingly and fit the RE model\n- extract the estimated effect size\n- compare the simulated sampling distribution with the analytical result\n\n## Confidence Intervals\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nk <- 30\nn <- 30\ntau2 <- 0.05\nmu <- 0.5\nnsim <- 5e3\n\n# true parameters (see Borenstein, 2009; Chapter 29)\nvt <- 1/n + 1/n\nvs <- (vt + tau2)/ k\nse <- sqrt(vs)\n\nmui <- rep(NA, nsim)\n\nfor(i in 1:nsim){\n  dat <- sim_studies(k = k, es = mu, tau2 = tau2, n1 = n)\n  fit <- rma(yi, vi, data = dat)\n  mui[i] <- coef(fit)[1]\n}\n\n# standard error\nc(simulated = sd(mui), analytical = fit$se)\n#>  simulated analytical \n#> 0.06322640 0.06508356\n\n# confidence interval\nrbind(\n  \"simulated\"  = quantile(mui, c(0.05, 0.975)),\n  \"analytical\" = c(\"2.5%\" = fit$ci.lb, \"97.5%\" = fit$ci.ub)\n)\n#>                   5%     97.5%\n#> simulated  0.3958958 0.6226868\n#> analytical 0.3721610 0.6272838\n```\n:::\n\n\n\n\n\n## Confidence Intervals\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhist(mui, breaks = 50, freq = FALSE, main = \"Sampling Distribution\", xlab = latex2exp::TeX(\"$\\\\mu_{\\\\theta}$\"))\ncurve(dnorm(x, mu, se), add = TRUE, col = \"firebrick\", lwd = 1.5)\n```\n\n::: {.cell-output-display}\n![](chapter5_files/figure-html/unnamed-chunk-57-1.png){width=672}\n:::\n:::\n\n\n\n\n\n## Confidence Intervals\n\nNow the equation for the 95% confidence interval should be more clear. The standard error is a function of the within study sampling variances (depending mainly on $n$), $\\tau^2$ and $k$. As we increase $k$ the standard error tends towards zero. \n\n$$\nCI = \\hat \\mu_{\\theta} \\pm z SE_{\\mu_{\\theta}}\n$$\n\n$$\nSE_{\\mu_{\\theta}} = \\sqrt{\\frac{1}{\\sum^{k}_{i = 1}w^{\\star}_i}}\n$$\n\n$$\nw^{\\star}_i = \\frac{1}{\\sigma^2_i + \\tau^2}\n$$\n\n## Confidence Intervals\n\nWe can also see it analytically, there is a huge impact of $k$.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\n# true parameters (see Borenstein, 2009; Chapter 29)\nvt <- 1/n + 1/n\nvs <- (vt + tau2)/ k\nse <- sqrt(vs)\n\nk <- c(10, 50, 100, 500, 1000, 5000)\nn <- c(10, 50, 100, 500, 1000, 5000)\ntau2 <- c(0, 0.05, 0.1, 0.2)\n\ndd <- expand.grid(k = k, n = n, tau2 = tau2)\n\ndd$vt <- with(dd, 1/n + 1/n)\ndd$vs <- with(dd, (vt + tau2)/ k)\ndd$se <- sqrt(dd$vs)\n\ndd$k <- as_tex_label(dd$k, \"$k = %s$\")\n\nggplot(dd, aes(x = n, y = se, color = factor(tau2))) +\n  geom_line() +\n  facet_wrap(~k, labeller = label_parsed) +\n  labs(color = latex2exp::TeX(\"\\\\tau^2\")) +\n  xlab(\"Sample Size (n)\") +\n  ylab(latex2exp::TeX(\"$SE_{\\\\mu_{\\\\theta}}$\"))\n```\n\n::: {.cell-output-display}\n![](chapter5_files/figure-html/unnamed-chunk-58-1.png){width=672}\n:::\n:::\n\n\n\n\n\n## Prediction intervals (PI)\n\nWe could say that the CI is not completely taking into account the between-study heterogeneity ($\\tau^2$). After a meta-analysis we would like to know how confident we are in the parameters estimation BUT also **what would be the expected effect running a new experiment tomorrow?**.\n\nThe **prediction interval** [@IntHout2016-sz; @Riley2011-hp] is exactly the range of effects that I expect in predicting a new study.\n\n## PI for a sample mean\n\nTo understand the concept, let's assume to have a sample $X$ of size $n$ and we estimate the mean $\\overline X$. The PI is calculated as^[Notice that the equation, in particular the usage of $t$ vs $z$ depends on assuming $s_x$ to be known or estimated. See [https://online.stat.psu.edu/stat501/lesson/3/3.3](https://online.stat.psu.edu/stat501/lesson/3/3.3), [https://en.wikipedia.org/wiki/Prediction_interval](https://en.wikipedia.org/wiki/Prediction_interval) and [https://www.bryanshalloway.com/2021/03/18/intuition-on-uncertainty-of-predictions-introduction-to-prediction-intervals/](https://www.bryanshalloway.com/2021/03/18/intuition-on-uncertainty-of-predictions-introduction-to-prediction-intervals/)]:\n\n$$\nPI = \\overline X \\pm t_{\\alpha/2} s_x \\sqrt{1 + \\frac{1}{n}}\n$$\n\nWhere $s$ is the sample standard deviation. Basically we are combining the uncertainty in estimating $\\overline X$ (i.e, $\\frac{s_x}{n}$) with the standard deviation of the data $s_x$. Compare it with the confidence interval containing only $\\frac{s_x}{n}$.\n\n## PI in meta-analysis\n\nFor meta-analysis the equation^[When a $t$ distribution is assumed, the quantiles are calculated using $k - 2$ degrees of freedom] is conceptually similar but with different quantities.\n\n$$\nPI = \\hat \\mu_{\\theta} \\pm z \\sqrt{\\tau^2 + SE_{\\mu_{\\theta}}}\n$$\n\nBasically we are combining all the sources of uncertainty. As long as $\\tau^2 \\neq 0$ the PI is greater than the CI (in the EE model they are the same). Thus even with very precise $\\mu_{\\theta}$ estimation, large $\\tau^2$ leads to uncertain predictions.\n\n## PI in meta-analysis\n\nIn R the PI can be calculated using `predict()`. By default the model assume a standard normal distribution thus using $z$ scores. To use the @Riley2011-hp approach ($t$ distribution) the model need to be fitted using `test = \"t\"`.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nk <- 100\ndat <- sim_studies(k = k, es = 0.5, tau2 = 0.1, n1 = 30)\nfit_z <- rma(yi, vi, data = dat, test = \"z\") # test = \"z\" is the default\npredict(fit_z) # notice pi.ub/pi.lb vs ci.ub/ci.lb\n#> \n#>    pred     se  ci.lb  ci.ub   pi.lb  pi.ub \n#>  0.4423 0.0424 0.3592 0.5254 -0.2299 1.1145\n# manually\nfit_z$b[[1]] + qnorm(c(0.025, 0.975)) * sqrt(fit_z$se^2 + fit_z$tau2)\n#> [1] -0.229882  1.114516\n\nfit_t <- rma(yi, vi, data = dat, test = \"t\")\npredict(fit_t) # notice pi.ub/pi.lb vs ci.ub/ci.lb\n#> \n#>    pred     se  ci.lb  ci.ub   pi.lb  pi.ub \n#>  0.4423 0.0424 0.3582 0.5265 -0.2382 1.1228\n# manually\nfit_z$b[[1]] + qt(c(0.025, 0.975), k - 2) * sqrt(fit_t$se^2 + fit_t$tau2)\n#> [1] -0.2382858  1.1229198\n```\n:::\n\n\n\n\n\n# Meta-analysis as (weighted) linear regression {.section}\n\n## MA as (weighted) linear regression\n\nBoth the EE and RE model can be seen as standard (weighted) linear regression models. Precisely, there is a difference in fitting a meta-analysis using `lm` or `lme4::lmer()` and `rma` (see [https://www.metafor-project.org/doku.php/tips:rma_vs_lm_lme_lmer](https://www.metafor-project.org/doku.php/tips:rma_vs_lm_lme_lmer)).\n\n. . .\n\nBeyond these differences a general the EE and RE models are intercept-only linear regressions.\n\n$$\n\\boldsymbol{Y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\epsilon}\n$$\n\nThe EE model:\n\n$$\ny_i = \\beta_0 + \\epsilon_i \n$$\n\nThe RE model:\n\n$$\ny_i = \\beta_0 + \\beta_{0_i} + \\epsilon_i \n$$\n\n## MA as (weighted) linear regression\n\nIn the EE model $\\beta_0$ is $\\theta$ and $\\epsilon_i \\sim \\mathcal{N}(0, \\sigma^2_i)$\n\n$$\ny_i = \\beta_0 + \\epsilon_i \n$$\n\nIn the RE model $\\beta_0$ is $\\mu_{\\theta}$ and $\\beta_{0_i}$ are the $\\delta_i$.\n\n## Explaining $\\tau^2$\n\nSo far we simply assumed $\\tau^2 = 0$ (for the EE model) or estimated it using the RE model.\n\n. . .\n\nWe can extend the intercept-only meta-analysis by including study-level predictors (as in standard linear regression) to explain the estimated true heterogeneity.\n\n## Explaining $\\tau^2$\n\nLet's make an example where we simulate a meta-analysis with $k = 100$ studies. Beyond the effect size, we extracted an experimental condition where 50 studies where lab-based experiments $x_{lab}$ and 50 studies where online experiments.\n\nWe assume that there could be a **lab effect** thus we included a predictor in the model.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nk <- 100\nn <- 10 + rpois(k, 40 - 10)\nexp <- rep(c(\"lab\", \"online\"), each = k/2)\n```\n:::\n\n\n\n\n\n## Explaining $\\tau^2$\n\nNow the model have a predictor $x$ (the type of experiment) and two parameters $\\beta_0$ and $\\beta_1$. Depending on the contrast coding (default to `contr.treatment()` in R) the $\\beta_0$ is different. Coding `exp` as 0 for lab-based experiments and 1 for online experiments:\n\n$$\ny_i = \\beta_0 + \\beta_1X_{1_i} + \\epsilon_i\n$$\n\n$$\ny_{\\text{lab}_i} = \\beta_0 + \\epsilon_i\n$$\n\n$$\ny_{\\text{online}_i} = \\beta_0 + \\beta_1 + \\epsilon_i\n$$\n\n## Explaining $\\tau^2$\n\nWhat is missing is the random-effect. Basically we still have $\\tau^2$ determining the $\\delta_i \\sim \\mathcal{N}(0, \\tau^2)$ but now is the residual $\\tau^2_r$. The heterogeneity after including the predictor.\n\n$$\ny_i = \\beta_0 + \\beta_{0_i} + \\beta_1X_{1_i} + \\epsilon_i\n$$ {#eq-metareg-cat}\n\n$$\n\\beta_{0_i} \\sim \\mathcal{N}(0, \\tau^2_r)\n$$\n\nClearly the difference between $\\tau^2$ (the total heterogeneity) and $\\tau^2_r$ (residual heterogeneity) is an index of the impact of $X$.\n\n## Simulating the $X$ effect\n\nTo simulate a meta-regression we just need to choose the parameters values ($\\beta_0$ and $\\beta_1$) and implement @eq-metareg-cat. Using treatment coding, $\\beta_0$ is the effect size when $X = 0$ (i.e., lab-based experiments) and $\\beta_1$ is the difference between lab and online experiments.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nb0 <- 0.3 # lab-based effect size\nb1 <- 0.5 # online - lab-based --> online = b0 + b1\nexp_dummy <- ifelse(exp == \"lab\", 0, 1) # dummy version\nes <- b0 + b1 * exp_dummy\nht(data.frame(exp, exp_dummy, es))\n#>        exp exp_dummy  es\n#> 1      lab         0 0.3\n#> 2      lab         0 0.3\n#> 3      lab         0 0.3\n#> 4      lab         0 0.3\n#> 5      lab         0 0.3\n#> 95  online         1 0.8\n#> 96  online         1 0.8\n#> 97  online         1 0.8\n#> 98  online         1 0.8\n#> 99  online         1 0.8\n#> 100 online         1 0.8\n```\n:::\n\n\n\n\n\n## Simulating the $X$ effects\n\nNow we can use the `sim_studies()` function as usual. The difference is that `es` is no longer a single value but a vector (with different values according to the $X$ level) and `tau2` is $\\tau^2_r$ (this the leftover heterogeneity after including the $X$ effect)\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntau2r <- 0.05 # residual heterogeneity\ndat <- sim_studies(k = k, es = es, tau2 = tau2r, n1 = n, add = list(exp = exp))\nht(dat)\n#> \n#>      id      yi     vi n1 n2    exp \n#> 1     1  0.3868 0.0634 40 40    lab \n#> 2     2  0.7162 0.0620 40 40    lab \n#> 3     3 -0.0864 0.0601 36 36    lab \n#> 4     4  0.0536 0.0516 40 40    lab \n#> 5     5  0.2547 0.0573 34 34    lab \n#> 95   95  1.4493 0.0530 31 31 online \n#> 96   96  0.9304 0.0527 41 41 online \n#> 97   97  1.0688 0.0420 47 47 online \n#> 98   98  0.6288 0.0511 38 38 online \n#> 99   99  1.2018 0.0462 41 41 online \n#> 100 100  0.6452 0.0702 29 29 online\n```\n:::\n\n\n\n\n\n## Fitting a meta-regression Model\n\nTo fit a meta-regression we still use the `metafor::rma()` function, adding the `mods = ~` parameter with the model formula (same as the right-hand side of a `y ~ x` call in `lm`). The name of the predictor in the formula need to match a column of the `data = ` dataframe.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit <- rma(yi, vi, mods = ~ exp, data = dat, method = \"REML\")\nsummary(fit)\n#> \n#> Mixed-Effects Model (k = 100; tau^2 estimator: REML)\n#> \n#>   logLik  deviance       AIC       BIC      AICc   \n#> -30.6576   61.3152   67.3152   75.0701   67.5706   \n#> \n#> tau^2 (estimated amount of residual heterogeneity):     0.0610 (SE = 0.0158)\n#> tau (square root of estimated tau^2 value):             0.2471\n#> I^2 (residual heterogeneity / unaccounted variability): 55.34%\n#> H^2 (unaccounted variability / sampling variability):   2.24\n#> R^2 (amount of heterogeneity accounted for):            48.11%\n#> \n#> Test for Residual Heterogeneity:\n#> QE(df = 98) = 220.5886, p-val < .0001\n#> \n#> Test of Moderators (coefficient 2):\n#> QM(df = 1) = 53.2544, p-val < .0001\n#> \n#> Model Results:\n#> \n#>            estimate      se    zval    pval   ci.lb   ci.ub      \n#> intrcpt      0.3301  0.0472  6.9990  <.0001  0.2377  0.4226  *** \n#> exponline    0.4871  0.0667  7.2976  <.0001  0.3563  0.6179  *** \n#> \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n\n\n\n\n\n## Intepreting a meta-regression Model\n\nThe output is similar to the RE model with few additions:\n\n- Everything related to the heterogeneity ($H^2$, $I^2$, $Q$, etc.) is now about **residual heterogeneity**\n- There is the (pseudo) $R^2$\n- There is an overall test for the moderators $Q_M$\n- There is a section (similar to standard regression models) with the estimated parameters, standard error and Wald test\n\n## Model parameters\n\n`intrcpt` and `exponline` are the estimates of $\\beta_0$ and $\\beta_1$. The interpretation depends on the scale of the effect size and the contrast coding.\n\nWe can plot the model results using the `metafor::regplot()`^[The functions is made for numerical variables thus is less appropriate for categorical variables].\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nregplot(fit)\n```\n\n::: {.cell-output-display}\n![](chapter5_files/figure-html/unnamed-chunk-64-1.png){width=672}\n:::\n:::\n\n\n\n\n\n## Omnibus Moderator Test\n\nThe `Test of Moderators` section report the so-called omnibus test for model coeffiecients. Is a simultaneous test for 1 or more coefficients where $H_0: \\beta_j = 0$.\n\nIn this case, **coefficient 2** means that we are testing only the 2nd coefficient $\\beta_1$. By default, the intercept is ignored. In fact, the `exponline` line and the omnibus test are the same (the $\\chi^2$ is just the $z^2$)\n\n\n\n\n\n::: {.cell}\n\n```\n#> Test of Moderators (coefficient 2):\n#> QM(df = 1) = 53.2544, p-val < .0001\n#>            estimate      se    zval    pval   ci.lb   ci.ub      \n#> intrcpt      0.3301  0.0472  6.9990  <.0001  0.2377  0.4226  *** \n#> exponline    0.4871  0.0667  7.2976  <.0001  0.3563  0.6179  ***\n```\n:::\n\n\n\n\n\n## General Linear Hypotheses Testing (GLHT)\n\nWe can also test any combination of parameters. For example we could test if lab-based experiments and online experiments are both different from 0. This is the same as fitting a model without the intercept^[see [https://www.metafor-project.org/doku.php/tips:models_with_or_without_intercept](https://www.metafor-project.org/doku.php/tips:models_with_or_without_intercept) on removing the intercept] thus estimating the cell means [see @Schad2020-ht].\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# now we are testing two coefficients\nfit_no_int <- rma(yi, vi, mods = ~ 0 + exp, data = dat)\n```\n:::\n\n\n\n\n\n## General Linear Hypotheses Testing (GLHT)\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit_no_int\n#> \n#> Mixed-Effects Model (k = 100; tau^2 estimator: REML)\n#> \n#> tau^2 (estimated amount of residual heterogeneity):     0.0610 (SE = 0.0158)\n#> tau (square root of estimated tau^2 value):             0.2471\n#> I^2 (residual heterogeneity / unaccounted variability): 55.34%\n#> H^2 (unaccounted variability / sampling variability):   2.24\n#> \n#> Test for Residual Heterogeneity:\n#> QE(df = 98) = 220.5886, p-val < .0001\n#> \n#> Test of Moderators (coefficients 1:2):\n#> QM(df = 2) = 348.4548, p-val < .0001\n#> \n#> Model Results:\n#> \n#>            estimate      se     zval    pval   ci.lb   ci.ub      \n#> explab       0.3301  0.0472   6.9990  <.0001  0.2377  0.4226  *** \n#> exponline    0.8172  0.0472  17.3052  <.0001  0.7247  0.9098  *** \n#> \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n\n\n\n\n\n## General Linear Hypotheses Testing (GLHT)\n\nA more elegant way is by using the GLHT framework. Basically we provide a contrast matrix expressing linear combinations of model parameters to be tested. In our case $\\text{lab} = \\beta_0 = 0$ and $\\text{online} = \\beta_0 + \\beta_1 = 0$.\n\nPractically, the matrix formulation is the following:\n\n$$\n\\begin{pmatrix}  \n1 & 0 \\\\\n1 & 1\n\\end{pmatrix}\n\\begin{pmatrix}  \n\\beta_0\\\\\n\\beta_1\n\\end{pmatrix}\n=\n\\begin{pmatrix}  \n0\\\\\n0\n\\end{pmatrix}\n$$\n\nIn R:\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nC <- rbind(c(1, 0), c(1, 1))\nB <- coef(fit)\nC %*% B # same as coef(fit)[1] and coef(fit)[1] +  coef(fit)[2]\n#>           [,1]\n#> [1,] 0.3301324\n#> [2,] 0.8172112\n```\n:::\n\n\n\n\n\n## General Linear Hypotheses Testing (GLHT)\n\nWe can use the `anova()` function providing the model and the hypothesis matrix.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nanova(fit) # the default\n#> \n#> Test of Moderators (coefficient 2):\n#> QM(df = 1) = 53.2544, p-val < .0001\nanova(fit, X = C)\n#> \n#> Hypotheses:                           \n#> 1:             intrcpt = 0 \n#> 2: intrcpt + exponline = 0 \n#> \n#> Results:\n#>    estimate     se    zval   pval \n#> 1:   0.3301 0.0472  6.9990 <.0001 \n#> 2:   0.8172 0.0472 17.3052 <.0001 \n#> \n#> Omnibus Test of Hypotheses:\n#> QM(df = 2) = 348.4548, p-val < .0001\n```\n:::\n\n\n\n\n\nNotice that is the same as the model without the intercept.\n\n## Likelihood Ratio Test (LRT)\n\nAs in standard regression modelling, we can also compare models using LRT. The `anova()` function will compute the LRT when two (nested) models are provided. In this case we compared a null (intercept-only) model with the model including the predictor.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# the null model\nfit0 <- rma(yi, vi, data = dat, method = \"REML\")\nanova(fit0, fit, refit = TRUE) # refit = TRUE because LRT with REML is not meaningful, using ML instead\n#> \n#>         df      AIC      BIC     AICc   logLik     LRT   pval       QE  tau^2 \n#> Full     3  66.5817  74.3972  66.8317 -30.2908                220.5886 0.0589 \n#> Reduced  2 108.5228 113.7331 108.6465 -52.2614 43.9411 <.0001 336.6655 0.1160 \n#>              R^2 \n#> Full             \n#> Reduced 49.2018%\n```\n:::\n\n\n\n\n\n## $R^2$\n\nThe $R^2$ value reported in the model output is not calculated as in standard regression analysis.\n\n$$\nR^2 = 1 - \\frac{\\tau^2_r}{\\tau^2}\n$$\n\nBasically is the percentage of heterogeneity reduction from the intercept-only model to the model including predictors.\n\nIn R:\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n(1 - fit$tau2/fit0$tau2)*100\n#> [1] 48.1096\nfit$R2\n#> [1] 48.1096\n```\n:::\n\n\n\n\n\n## $R^2$\n\nDespite useful, the $R^2$ has some limitations:\n\n- @Lopez-Lopez2014-it showed that precise estimations require a large number of studies $k$ \n- Sometimes could results in negative values (usually truncated to zero)\n- Depends on the $\\tau^2$ estimator\n\nMore about $R^2$ and limitations can be found:\n\n- [https://www.metafor-project.org/doku.php/faq#for_mixed-effects_models_how_i](https://www.metafor-project.org/doku.php/faq#for_mixed-effects_models_how_i)\n- [https://www.metafor-project.org/doku.php/tips:ci_for_r2](https://www.metafor-project.org/doku.php/tips:ci_for_r2)\n\n## Numerical predictor\n\nThe same logic of simulating a meta-regression can be applied to numerical predictors. We still have $\\beta_0$ and $\\beta_1$ but $X$ has more levels. Let's simulate an impact of the average participants' age on the effect size.\n\n- $\\beta_0$ is the effect size when **age** is zero\n- $\\beta_1$ is the expected increase in the effect size for a unit increase in `age`\n\nHow we can choose plausible values for the parameters and parametrize the model correctly?\n\n## Parametrize $\\beta_0$\n\nThe intepretation (and the inference) of $\\beta_0$ is strongly dependent on the type of numerical predictor. An age of zero is (probably) empirically meaningless thus the  $\\beta_0$ is somehow not useful.\n\nWe can for example mean-center (or other type of centering procedure) moving the zero on a meaningful value.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nage <- 10:50 # the raw vector\nage0 <- age - mean(age) # centering on the mean\nage20 <- age - min(age) # centering on the minimum\n\nht(data.frame(age, age0, age20))\n#>    age age0 age20\n#> 1   10  -20     0\n#> 2   11  -19     1\n#> 3   12  -18     2\n#> 4   13  -17     3\n#> 5   14  -16     4\n#> 36  45   15    35\n#> 37  46   16    36\n#> 38  47   17    37\n#> 39  48   18    38\n#> 40  49   19    39\n#> 41  50   20    40\n```\n:::\n\n\n\n\n\n## Parametrize $\\beta_0$\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](chapter5_files/figure-html/unnamed-chunk-73-1.png){width=672}\n:::\n:::\n\n\n\n\n\n## Parametrize $\\beta_0$\n\nUsing different parametrizations will only affect the estimation (and the interpretation) of $\\beta_0$. Other parameters and indexes will be the same.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nk <- 100\nb0 <- 0.2 # effect size when age 0\nb1 <- 0.05 # slope (random for now)\nage <- round(runif(k, 20, 50)) # sampling from uniform distribution\ntau2r <- 0.05\nn <- 10 + rpois(k, 30 - 10)\n\nes <- b0 + b1 * age # raw\n\nage0 <- age - mean(age)\nage20 <- age - 20\n\ndat <- sim_studies(k = k, es = es, tau2 = tau2r, n1 = n, add = list(age = age, age0 = age0, age20 = age20))\n\nfit <- rma(yi, vi, mods = ~ age, data = dat)\nfit0 <- rma(yi, vi, mods = ~ age0, data = dat)\nfit20 <- rma(yi, vi, mods = ~ age20, data = dat)\n\n# showing the intercept\ncompare_rma(fit, fit0, fit20, extra_params = \"R2\") |> \n  round(3)\n#> fit: rma(yi = yi, vi = vi, mods = ~age, data = dat)\n#> fit0: rma(yi = yi, vi = vi, mods = ~age0, data = dat)\n#> fit20: rma(yi = yi, vi = vi, mods = ~age20, data = dat)\n#>                fit   fit0  fit20\n#> b (intrcpt)  0.371  2.026  1.290\n#> se           0.153  0.036  0.076\n#> zval         2.427 56.591 17.076\n#> pval         0.015  0.000  0.000\n#> ci.lb        0.071  1.956  1.142\n#> ci.ub        0.671  2.096  1.438\n#> R2          71.443 71.443 71.443\n#> I2          49.486 49.486 49.486\n#> tau2         0.063  0.063  0.063\n\n  # showing the intercept\ncompare_rma(fit, fit0, fit20, b = \"age\", extra_params = \"R2\") |> \n  round(3)\n#> fit: rma(yi = yi, vi = vi, mods = ~age, data = dat)\n#> fit0: rma(yi = yi, vi = vi, mods = ~age0, data = dat)\n#> fit20: rma(yi = yi, vi = vi, mods = ~age20, data = dat)\n#>            fit   fit0  fit20\n#> b (age)  0.046  0.046  0.046\n#> se       0.004  0.004  0.004\n#> zval    11.152 11.152 11.152\n#> pval     0.000  0.000  0.000\n#> ci.lb    0.038  0.038  0.038\n#> ci.ub    0.054  0.054  0.054\n#> R2      71.443 71.443 71.443\n#> I2      49.486 49.486 49.486\n#> tau2     0.063  0.063  0.063\n```\n:::\n\n\n\n\n\n## Choosing $\\beta_1$\n\nThe core of the model is $\\beta_1$ that is the **age** effect. Compared to the categorical case where $\\beta_1$ is just the standardized difference between two conditions, with numerical $X$ choosing a meaningful $\\beta_1$ is more challenging.\n\nTwo (maybe more) strategies:\n\n- simulating a lot of effects sizes fixing $beta_0$ and $\\beta_1$ and see the expected range of $y_i$\n- fixing a certain $R^2$ and choose the $\\beta_1$ producing that $R^2$\n- ...\n\n## $\\beta_1$ by simulations\n\nA strategy could be to simulate from the generative model a large number of studies and see the expected range of effect size [@Gelman2020-tg, Chapter 5 and p. 97]. A large number of unplausible values suggest that the chosen $\\beta_1$ is probably not appropriate.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nk <- 1e3\nn <- 30\ntau2 <- 0\nx <- runif(k, 20, 50) # age\nb0 <- 0.1\nb1 <- c(0.001, 0.05, 0.2)\nesl <- lapply(b1, function(b) b0 + b*x)\ndatl <- lapply(esl, function(es) sim_studies(k = k, es = es, tau2 = tau2, n1 = n, add = list(x = x)))\nnames(datl) <- b1\ndat <- dplyr::bind_rows(datl, .id = \"b1\")\nht(dat)\n#> \n#>         b1   id      yi     vi n1 n2        x \n#> 1    0.001    1 -0.0234 0.0649 30 30 36.66325 \n#> 2    0.001    2 -0.2818 0.0528 30 30 37.59875 \n#> 3    0.001    3 -0.1208 0.0969 30 30 37.89483 \n#> 4    0.001    4  0.3004 0.0509 30 30 24.06663 \n#> 5    0.001    5 -0.1076 0.0560 30 30 22.65956 \n#> 2995   0.2  995  8.9069 0.0616 30 30 43.62926 \n#> 2996   0.2  996  4.6568 0.0618 30 30 21.95671 \n#> 2997   0.2  997  5.9464 0.0631 30 30 30.73639 \n#> 2998   0.2  998  6.6676 0.0712 30 30 31.20466 \n#> 2999   0.2  999  9.8789 0.0994 30 30 48.16696 \n#> 3000   0.2 1000  5.7442 0.0702 30 30 28.71515\n```\n:::\n\n\n\n\n\n## $\\beta_1$ by simulations\n\nClearly given the limited range of the $x$ variable (`age`) some $\\beta_1$ values are implausible leading to effect sizes that are out of a meaningful empirical range.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\ndat$b1 <- factor(dat$b1, labels = latex2exp::TeX(sprintf(\"$\\\\beta_1 = %s$\", unique(dat$b1))))\ndat |> \n  ggplot(aes(x = x, y = yi)) +\n  geom_point() +\n  facet_wrap(~b1, scales = \"free_y\", labeller = label_parsed) +\n  xlab(\"Age\") +\n  ylab(latex2exp::TeX(\"$y_i$\"))\n```\n\n::: {.cell-output-display}\n![](chapter5_files/figure-html/unnamed-chunk-76-1.png){width=672}\n:::\n:::\n\n\n\n\n\n## Fixing $R^2$\n\nWe can use the approach by @Lopez-Lopez2014-it where predictors $x$ are sampled from a standard normal distribution (or standardized). $\\beta_1$ is calculated as $\\beta_1 = \\sqrt{\\tau^2 R^2}$ and the residual heterogeneity as $\\tau^2_r = \\tau^2 - \\beta^2_1$.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nk <- 100\nn <- 30\ntau2 <- 0.3\nR2 <- 0.4\nb0 <- 0.1\nb1_2 <- tau2 * R2\nb1 <- sqrt(b1_2)\ntau2r <- tau2 - b1_2\n```\n:::\n\n\n\n\n\n## Fixing $R^2$\n\nWe can check the simulation approach:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nk <- 1e3\n1 - tau2r/tau2\n#> [1] 0.4\nx <- rnorm(k)\nes <- b0 + b1 * x\ndat <- sim_studies(k, es, tau2r, n1 = 1e3, add = list(x = x))\nfit <- rma(yi, vi, data = dat, mods = ~x)\nsummary(fit)\n#> \n#> Mixed-Effects Model (k = 1000; tau^2 estimator: REML)\n#> \n#>    logLik   deviance        AIC        BIC       AICc   \n#> -562.0096  1124.0193  1130.0193  1144.7365  1130.0434   \n#> \n#> tau^2 (estimated amount of residual heterogeneity):     0.1786 (SE = 0.0081)\n#> tau (square root of estimated tau^2 value):             0.4226\n#> I^2 (residual heterogeneity / unaccounted variability): 98.89%\n#> H^2 (unaccounted variability / sampling variability):   90.40\n#> R^2 (amount of heterogeneity accounted for):            43.09%\n#> \n#> Test for Residual Heterogeneity:\n#> QE(df = 998) = 90168.4129, p-val < .0001\n#> \n#> Test of Moderators (coefficient 2):\n#> QM(df = 1) = 749.0935, p-val < .0001\n#> \n#> Model Results:\n#> \n#>          estimate      se     zval    pval   ci.lb   ci.ub      \n#> intrcpt    0.0830  0.0134   6.1759  <.0001  0.0567  0.1094  *** \n#> x          0.3628  0.0133  27.3696  <.0001  0.3368  0.3888  *** \n#> \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n\n\n\n\n\n## $R^2$ using simulations\n\nThe results from @Lopez-Lopez2014-it (and also our previous simulation) suggested that we need a large number of studies for precise $R^2$ estimations. Let's check using simulations the sampling distribution of $R^2$ using a plausible meta-analysis scenario.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nk <- 40 # number of studies\nn <- 10 + rpois(k, 40 - 10) # sample size\ntau2 <- 0.05 # tau ~ 0.22\nR2 <- 0.3\nb0 <- 0.1\nb1_2 <- tau2 * R2\nb1 <- sqrt(b1_2)\ntau2r <- tau2 - b1_2\nnsim <- 1e3\n\nR2i <- rep(NA, nsim)\n\nfor(i in 1:nsim){\n  x <- rnorm(k)\n  dat <- sim_studies(k = k, es = b0 + b1*x, tau2 = tau2r, n1 = n, add = list(x))\n  fit <- rma(yi, vi, data = dat, mods = ~x)\n  R2i[i] <- fit$R2\n}\n```\n:::\n\n\n\n\n\n## $R^2$ using simulations\n\nWe estimated the true $R^2$ correctly but there is a lot of uncertainty with a plausible meta-analysis scenario. There are a lot of meta-analysis also with lower $k$ worsening the results.\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](chapter5_files/figure-html/unnamed-chunk-80-1.png){width=672}\n:::\n:::\n\n\n\n\n\n# Publication Bias (PB) {.section}\n\n# What do you think about PB? What do you know? Causes? Remedies?  {.question .smaller}\n\n## Publication Bias (PB)\n\nThe PB is a very critical **most problematic aspects** of meta-analysis. Essentially **the probability of publishing a paper** (~and thus including into the meta-analysis) [is not the same regardless of the result]{.imp}.\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](chapter5_files/figure-html/unnamed-chunk-81-1.png){width=672}\n:::\n:::\n\n\n\n\n\n## Publication Bias Disclaimer!\n\n**We cannot (completely) solve the PB using statistical tools**. The PB is a problem related to the publishing process and publishing incentives\n\n. . .\n\n- **pre-registrations**, **multi-lab studies**, etc. can (almost) completely solve the problem filling the literature with unbiased studies\n\n. . .\n\n- there are **statistical tools to detect, estimate and correct** for the publication bias. As every statistical method, they are influenced by statistical assumptions, number of studies and sample size, heterogeneity, etc.\n\n## Publication Bias (PB) - The Big Picture^[Thanks to the Wolfgang Viechtbauer's course [https://www.wvbauer.com/doku.php/course_ma](https://www.wvbauer.com/doku.php/course_ma)]\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](img/big-picture.svg)\n:::\n:::\n\n\n\n\n\n## PB under an EE model\n\nThe easiest way to understand the PB is by simulating what happen without the PB. Let's simulate a lot of studies (under a EE model) keeping all the results without selection (the ideal world).\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(2023)\nk <- 1e3\nn <- round(runif(k, 10, 100))\ntheta <- 0.3\ndat <- sim_studies(k = k, es = theta, tau2 = 0, n1 = n)\ndat <- summary(dat)\n# compute 1 tail pvalue\ndat$pval1 <- 1 - pnorm(dat$zi)\nht(dat)\n#> \n#>        id      yi     vi n1 n2    sei      zi   pval   ci.lb  ci.ub       pval1 \n#> 1       1  0.3483 0.0406 52 52 0.2015  1.7287 0.0839 -0.0466 0.7431 0.041927744 \n#> 2       2  0.3762 0.0605 40 40 0.2460  1.5291 0.1262 -0.1060 0.8585 0.063119366 \n#> 3       3  0.0634 0.0911 25 25 0.3018  0.2101 0.8336 -0.5281 0.6549 0.416800171 \n#> 4       4  0.4101 0.0487 46 46 0.2206  1.8588 0.0631 -0.0223 0.8426 0.031530757 \n#> 5       5 -0.0476 0.1160 13 13 0.3405 -0.1398 0.8888 -0.7151 0.6199 0.555581669 \n#> 995   995  0.3584 0.0950 24 24 0.3083  1.1625 0.2450 -0.2458 0.9625 0.122511729 \n#> 996   996  0.3939 0.1697 17 17 0.4120  0.9562 0.3390 -0.4135 1.2014 0.169484321 \n#> 997   997  0.5205 0.0486 35 35 0.2204  2.3610 0.0182  0.0884 0.9525 0.009112230 \n#> 998   998  0.4206 0.0815 29 29 0.2854  1.4737 0.1406 -0.1388 0.9801 0.070283435 \n#> 999   999  0.0625 0.0244 83 83 0.1562  0.3999 0.6893 -0.2438 0.3687 0.344631179 \n#> 1000 1000  0.5547 0.0416 37 37 0.2039  2.7201 0.0065  0.1550 0.9544 0.003263493\n```\n:::\n\n\n\n\n\n## PB under an EE model\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npar(mfrow = c(1, 3))\nhist(dat$yi, breaks = 50, col = \"dodgerblue\", main = \"Effect Size\")\nplot(dat$yi, dat$pval1, pch = 19, col = ifelse(dat$pval1 <= 0.05, scales::alpha(\"firebrick\", 0.5), scales::alpha(\"black\", 0.5)),\n     main = \"P value (one tail) ~ Effect size\")\nabline(h = 0.05)\nplot(dat$yi, dat$pval, pch = 19, col = ifelse(dat$pval <= 0.05, scales::alpha(\"firebrick\", 0.5), scales::alpha(\"black\", 0.5)),\n     main = \"P value (two tails) ~ Effect size\")\nabline(h = 0.05)\n```\n\n::: {.cell-output-display}\n![](chapter5_files/figure-html/unnamed-chunk-84-1.png){width=672}\n:::\n:::\n\n\n\n\n\n## PB under an EE model\n\nThen, let's assume that our publishing system is very strict (extreme). You can publish only if $p \\leq 0.05$ on the expected direction. Then the true population of effect sizes will be truncated. Essentially we are assuming that $P(1|p \\leq 0.05) = 1$ and $P(1|p \\leq 0.05) = 0$.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# selecting\nsign <- dat$pval1 <= 0.05 & dat$zi > 0\ndat_pb <- dat[sign, ]\ndat_un <- dat[sample(1:nrow(dat), sum(sign)), ]\n\n# fitting EE model for the full vs selected (ignore k differences)\nfit <- rma(yi, vi, method = \"EE\", data = dat_un)\nfit_pb <- rma(yi, vi, method = \"EE\", data = dat_pb)\n```\n:::\n\n\n\n\n\n## PB under an EE model\n\nThen, let's assume that our publishing system is very strict (extreme). You can publish only if $p \\leq 0.05$ on the expected direction. Then the true population of effect sizes will be truncated. Essentially we are assuming that $P(1|p \\leq 0.05) = 1$ and $P(1|p \\leq 0.05) = 0$.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nround(compare_rma(fit, fit_pb), 3)\n#> fit: rma(yi = yi, vi = vi, data = dat_un, method = \"EE\")\n#> fit_pb: rma(yi = yi, vi = vi, data = dat_pb, method = \"EE\")\n#>                fit fit_pb\n#> b (intrcpt)  0.289  0.436\n#> se           0.009  0.008\n#> zval        32.003 51.811\n#> pval         0.000  0.000\n#> ci.lb        0.271  0.420\n#> ci.ub        0.307  0.453\n#> I2           0.955  0.000\n#> tau2         0.000  0.000\n```\n:::\n\n\n\n\n\n## PB under an EE model\n\nThe situation is even worse when we simulate a null effect. This strict selection results in committing type-1 error:\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(2023)\nk <- 1e3\nn <- round(runif(k, 10, 100))\ndat0 <- sim_studies(k = k, es = 0, tau2 = 0, n1 = n)\ndat0 <- summary(dat0)\n# compute 1 tail pvalue\ndat0$pval1 <- 1 - pnorm(dat0$zi)\n# selecting\nsign <- dat0$pval1 <= 0.05 & dat0$zi > 0\ndat_pb0 <- dat0[sign, ]\ndat_un0 <- dat0[sample(1:nrow(dat0), sum(sign)), ]\n\n# fitting EE model for the full vs selected (ignore k differences)\nfit0 <- rma(yi, vi, method = \"EE\", data = dat_un0)\nfit_pb0 <- rma(yi, vi, method = \"EE\", data = dat_pb0)\nround(compare_rma(fit0, fit_pb0), 3)\n#> fit0: rma(yi = yi, vi = vi, data = dat_un0, method = \"EE\")\n#> fit_pb0: rma(yi = yi, vi = vi, data = dat_pb0, method = \"EE\")\n#>               fit0 fit_pb0\n#> b (intrcpt) -0.006   0.363\n#> se           0.027   0.026\n#> zval        -0.220  14.015\n#> pval         0.826   0.000\n#> ci.lb       -0.059   0.312\n#> ci.ub        0.047   0.414\n#> I2           0.000   0.000\n#> tau2         0.000   0.000\n```\n:::\n\n\n\n\n\n## PB under an EE model\n\nAssuming to pick a very precise ($n = 1000$) and a very unprecise ($n = 20$) study, which one is more likely to have an effect size close to the true value?\n\n. . .\n\nThe precise study has a lower $\\epsilon_i$ thus is closer to $\\theta$. This relationship create a very insightful visual representation.\n\n. . .\n\nWhat could be the shape of the plot when plotting the precision (e.g., the sample size or the inverse of the variance) as a function of the effect size?\n\n## PB under an EE model\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#|code-fold: true\nplot(dat$yi, sqrt(dat$vi), ylim=rev(range(dat$vi)), pch = 19, col = scales::alpha(\"black\", 0.5), cex = 1.5)\nabline(v = theta, lwd = 3, col = \"firebrick\")\n```\n\n::: {.cell-output-display}\n![](chapter5_files/figure-html/unnamed-chunk-88-1.png){width=672}\n:::\n:::\n\n\n\n\n\n## Publication Bias (PB) - Funnel Plot\n\nWe created a **funnel plot**. This is a **visual tool** to check the presence of asymmetry that could be caused by publication bias. If meta-analysis assumptions are respected, and there is no publication bias:\n\n- effects should be normally distributed around the average effect\n- more precise studies should be closer to the average effect\n- less precise studies could be equally distributed around the average effect\n\n## Publication Bias (PB) - Funnel Plot\n\nThe plot assume the typical funnel shape and there are not missing spots on the at the bottom. The presence of missing spots is a potential index of publication bias.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nfit <- rma(yi, vi, method = \"EE\", data = dat)\ndat$pb <- dat$pval <= 0.05\n\nwith(dat[dat$pb, ],\n     plot(yi, sei,\n          ylim = rev(range(dat$sei)),\n          xlab = latex2exp::TeX(\"$y_i$\"),\n          ylab = latex2exp::TeX(\"$\\\\sqrt{\\\\sigma_i^2}$\"),\n          xlim = range(dat$yi),\n          pch = 19,\n          col = scales::alpha(\"firebrick\", 0.5))\n)\n\nwith(dat[!dat$pb, ],\n     points(yi, sei, col = scales::alpha(\"black\", 0.5), pch = 19)\n)\n\nabline(v = fit$b[[1]], col = \"black\", lwd = 1.2)\n```\n\n::: {.cell-output-display}\n![](chapter5_files/figure-html/unnamed-chunk-89-1.png){width=672}\n:::\n:::\n\n\n\n\n\n## Publication Bias (PB) - Funnel Plot\n\nThe plot assume the typical funnel shape and there are not missing spots on the at the bottom. The presence of missing spots is a potential index of publication bias.\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](chapter5_files/figure-html/unnamed-chunk-90-1.png){width=672}\n:::\n:::\n\n\n\n\n\n## Robustness to PB - Fail-safe N\n\nThe Fail-safe N [@Rosenthal1979-yx] idea is very simple. Given a meta-analysis with a significant result (i.e., $p \\leq \\alpha$). How many null studies (i.e., $\\hat \\theta = 0$) do I need to obtain $p > \\alpha$?\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmetafor::fsn(yi, vi, data = dat)\n#> \n#> Fail-safe N Calculation Using the Rosenthal Approach\n#> \n#> Observed Significance Level: <.0001\n#> Target Significance Level:   0.05\n#> \n#> Fail-safe N: 832741\n```\n:::\n\n\n\n\n\n## Robustness to PB - Fail-safe N\n\nThere are several criticism to the Fail-safe N procedure:\n\n. . .\n\n- is not actually *detecting* the PB but suggesting the required PB size to remove the effect. A very large N suggest that even with PB, it is unlikely that the results could be completely changed by actually reporting null studies\n\n. . .\n\n- @Orwin1983-vu proposed a new method calculating the number of studies required to reduce the effect size to a given target\n\n. . .\n\n- @Rosenberg2005-ie proposed a method similar to Rosenthal [-@Rosenthal1979-yx] but combining the (weighted) effect sizes and not the p-values.\n\n\n## Detecting PB - Egger Regression\n\nA basic method to test the funnel plot asymmetry is using an the **Egger regression test**. Basically we calculate the relationship between $y_i$ and $\\sqrt{\\sigma^2_i}$. In the absence of asimmetry, the line slope should be not different from 0.\n\nWe can use the `metafor::regtest()` function:\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\negger <- regtest(fit)\negger\n#> \n#> Regression Test for Funnel Plot Asymmetry\n#> \n#> Model:     fixed-effects meta-regression model\n#> Predictor: standard error\n#> \n#> Test for Funnel Plot Asymmetry: z = -0.3621, p = 0.7173\n#> Limit Estimate (as sei -> 0):   b =  0.3061 (CI: 0.2601, 0.3520)\n```\n:::\n\n\n\n\n\n## Publication Bias (PB) - Egger Regression\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](chapter5_files/figure-html/unnamed-chunk-93-1.png){width=672}\n:::\n:::\n\n\n\n\n\nThis is a standard (meta) regression thus the number of studies, the precision of each study and heterogeneity influence the reliability (power, type-1 error rate, etc.) of the procedure.\n\n## Correcting PB - Trim and Fill\n\nThe Trim and Fill method [@Duval2000-ym] is used to impute the hypothetical missing studies according to the funnel plot and recomputing the meta-analysis effect. Shi and Lin [@Shi2019-pj] provide an updated overview of the method with some criticisms.\n\n. . .\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(2023)\nk <- 100 # we increased k to better show the effect\ntheta <- 0.5\ntau2 <- 0.1\nn <- runif(k, 10, 100)\ndat <- sim_studies(k, theta, tau2, n)\ndat <- summary(dat)\ndatpb <- dat[dat$pval <= 0.1 & dat$zi > 0, ]\nfit <- rma(yi, vi, data = datpb, method = \"REML\")\n```\n:::\n\n\n\n\n\n## Correcting PB - Trim and Fill\n\nNow we can use the `metafor::trimfill()` function:\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntaf <- metafor::trimfill(fit)\ntaf\n#> \n#> Estimated number of missing studies on the left side: 15 (SE = 5.4670)\n#> \n#> Random-Effects Model (k = 84; tau^2 estimator: REML)\n#> \n#> tau^2 (estimated amount of total heterogeneity): 0.0562 (SE = 0.0146)\n#> tau (square root of estimated tau^2 value):      0.2371\n#> I^2 (total heterogeneity / total variability):   61.61%\n#> H^2 (total variability / sampling variability):  2.60\n#> \n#> Test for Heterogeneity:\n#> Q(df = 83) = 210.3501, p-val < .0001\n#> \n#> Model Results:\n#> \n#> estimate      se     zval    pval   ci.lb   ci.ub      \n#>   0.5934  0.0339  17.5084  <.0001  0.5270  0.6598  *** \n#> \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n\n\n\n\n\nThe trim-and-fill estimates that 15 are missing. The new effect size after including the studies is reduced and closer to the simulated value (but in this case still significant).\n\n## Correcting PB - Trim and Fill\n\nWe can also visualize the funnel plot highligting the points that are included by the method.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfunnel(taf)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nfunnel(taf)\negg <- regtest(fit)\negg_npb <- regtest(taf)\nse <- seq(0,1.8,length=100)\nlines(coef(egg$fit)[1] + coef(egg$fit)[2]*se, se, lwd=3, col = \"black\")\nlines(coef(egg_npb$fit)[1] + coef(egg_npb$fit)[2]*se, se, lwd=3, col = \"firebrick\")\nlegend(\"topleft\", legend = c(\"Original\", \"Corrected\"), fill = c(\"black\", \"firebrick\"))\n```\n\n::: {.cell-output-display}\n![](chapter5_files/figure-html/unnamed-chunk-97-1.png){width=672}\n:::\n:::\n\n\n\n\n\n## Why the funnel plot can be misleading?\n\nThis funnel plot show an evident asymmetry on the left side. Is there evidence of publication bias? What do you think?\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(2024)\nk <- 50\nn1 <- round(runif(k, 10, 200))\nn2 <- round(runif(k, 10, 50))\ndat1 <- sim_studies(k, 0, 0, n1, add = list(x = 0))\ndat2 <- sim_studies(k, 0.5, 0.05, n2, add = list(x = 1))\ndat <- rbind(dat1, dat2)\nfit <- rma(yi, vi, dat = dat)\nfunnel(fit)\n```\n\n::: {.cell-output-display}\n![](chapter5_files/figure-html/unnamed-chunk-98-1.png){width=672}\n:::\n:::\n\n\n\n\n\n## Why the funnel plot can be misleading?\n\nThe data are of course simulated and this is the code. What do you think now?\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(2024)\nk <- 50\nn1 <- round(runif(k, 10, 200))\nn2 <- round(runif(k, 10, 50))\ndat1 <- sim_studies(k, 0, 0, n1, add = list(x = 0))\ndat2 <- sim_studies(k, 0.5, 0.05, n2, add = list(x = 1))\ndat <- rbind(dat1, dat2)\nfit <- rma(yi, vi, dat = dat)\nfunnel(fit)\n```\n:::\n\n\n\n\n\n## Why the funnel plot can be misleading?\n\nIn fact, these are two **unbiased** population of effect sizes. Extra source of heterogeneity could create asymmetry not related to PB.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npar(mfrow = c(1, 2))\nfunnel(fit)\nfunnel(fit, col = dat$x + 1)\n```\n\n::: {.cell-output-display}\n![](chapter5_files/figure-html/unnamed-chunk-100-1.png){width=672}\n:::\n:::\n\n\n\n\n\n## Why the funnel plot can be misleading?\n\nAlso the methods to detect/correct for PB are committing a false alarm:\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nregtest(fit)\n#> \n#> Regression Test for Funnel Plot Asymmetry\n#> \n#> Model:     mixed-effects meta-regression model\n#> Predictor: standard error\n#> \n#> Test for Funnel Plot Asymmetry: z =  6.2569, p < .0001\n#> Limit Estimate (as sei -> 0):   b = -0.2067 (CI: -0.3460, -0.0675)\n```\n:::\n\n\n\n\n\n## Why the funnel plot can be misleading?\n\nAlso the methods to detect/correct for PB are committing a false alarm:\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntrimfill(fit)\n#> \n#> Estimated number of missing studies on the left side: 32 (SE = 6.3553)\n#> \n#> Random-Effects Model (k = 132; tau^2 estimator: REML)\n#> \n#> tau^2 (estimated amount of total heterogeneity): 0.2183 (SE = 0.0337)\n#> tau (square root of estimated tau^2 value):      0.4672\n#> I^2 (total heterogeneity / total variability):   86.77%\n#> H^2 (total variability / sampling variability):  7.56\n#> \n#> Test for Heterogeneity:\n#> Q(df = 131) = 639.7238, p-val < .0001\n#> \n#> Model Results:\n#> \n#> estimate      se    zval    pval    ci.lb   ci.ub    \n#>   0.0281  0.0457  0.6142  0.5391  -0.0615  0.1177    \n#> \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n\n\n\n\n\n## Why the funnel plot can be misleading?\n\nThe `regtest` can be applied also with moderators. The idea should be to take into account the moderators effects and then check for asymmetry.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfitm <- rma(yi, vi, mods = ~x, data = dat)\nregtest(fitm)\n#> \n#> Regression Test for Funnel Plot Asymmetry\n#> \n#> Model:     mixed-effects meta-regression model\n#> Predictor: standard error\n#> \n#> Test for Funnel Plot Asymmetry: z = -1.0277, p = 0.3041\n```\n:::\n\n\n\n\n\n## Why the funnel plot can be misleading?\n\nIn fact, the funnel plot on the raw dataset and on residuals looks quite different because the asymmetry was caused by the moderator.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\ndat$ri <- residuals(fitm)\nfitr <- rma(ri, vi, data = dat)\npar(mfrow = c(1, 2))\nplot_regtest(fit, main = \"Full model\")\nplot_regtest(fitr, main = \"Residuals\")\n```\n\n::: {.cell-output-display}\n![](chapter5_files/figure-html/unnamed-chunk-104-1.png){width=672}\n:::\n:::\n\n\n\n\n\n## Correcting PB - Selection Models (SM)\n\nSM are more than a tool for correcting for the PB. SM are formal models of PB that can help us understanding and simulating the PB.\n\nThe SM are composed by two parts:\n\n1. **Effect size model**: the unbiased data generation process. In our case basically the `sim_studies()` function.\n2. **Selection model**: the assumed process generating the biased selection of effect sizes\n\n**Selection models** can be based on the p-value (e.g., *p-curve* or *p-uniform*) and/or the effect size and variance (*Copas* model). We will see only models based on the p-value.\n\n## Correcting PB - Selection Models (SM)\n\nFormally, the random-effect meta-analysis probability density function (PDF) can be written as [e.g., @Citkowicz2017-ox]:\n\n$$\nf\\left(y_i \\mid \\beta, \\tau^2 ; \\sigma_i^2\\right)=\\frac{\\phi\\left(\\frac{y_i-\\Delta_i}{\\sqrt{\\sigma_i^2+\\tau^2}}\\right)}{\\int_{-\\infty}^{\\infty}  \\phi\\left(\\frac{Y_i-\\Delta_i}{\\sqrt{\\sigma_i^2+\\tau^2}}\\right) d y_i}\n$$\n\nWithout going into details, this is the PDF without any selection process (i.e., the **effect sizes model**).\n\n## Correcting PB - Selection Models (SM)\n\nIf we have a function linking the p-value with the probability of publishing (a **weight function**) $w(p_i)$ we can include it in the previous PDF, creating a weighted PDF.\n\n$$\nf\\left(y_i \\mid \\beta, \\tau^2 ; \\sigma_i^2\\right)=\\frac{\\mathrm{w}\\left(p_i\\right) \\phi\\left(\\frac{y_i-\\Delta_i}{\\sqrt{\\sigma_i^2+\\tau^2}}\\right)}{\\int_{-\\infty}^{\\infty} \\mathrm{w}\\left(p_i\\right) \\phi\\left(\\frac{Y_i-\\Delta_i}{\\sqrt{\\sigma_i^2+\\tau^2}}\\right)d y_i}\n$$\n\nEssentially, this new model take into account the selection process (the **weight function**) to estimate a new meta-analysis. In case of no selection (all weigths are the same) the model is the standard random-effects meta-analysis.\n\n## SM - Weigth function\n\nThe weigth function is a simple function that links the p-value with the probability of publishing. The simple example at the beginning (publishing only significant p-values) is a step weigth function.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\np <- c(0, 0.05, 0.05, 1)\nw <- c(1, 1, 0, 0)\n\nplot(p, w, type = \"l\", xlab = \"p value\", ylab = \"Probability of Publishing\")\n```\n\n::: {.cell-output-display}\n![](chapter5_files/figure-html/unnamed-chunk-105-1.png){width=672}\n:::\n:::\n\n\n\n\n\n## SM - Weigth function\n\nWe can add more steps to express a more complex selection process:\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\np <- c(0.001, 0.01, 0.05, 0.1, 0.8, 1)\nw <- c(1, 1, 0.9, 0.5, 0.1, 0.1)\nplot(p, w, type = \"s\", xlab = \"p value\", ylab = \"Probability of Publishing\")\n```\n\n::: {.cell-output-display}\n![](chapter5_files/figure-html/unnamed-chunk-106-1.png){width=672}\n:::\n:::\n\n\n\n\n\n## SM - Weigth function\n\nOr we can draw a smooth function assuming certain parameters:\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwbeta <- function(p, a = 1, b = 1) p^(a - 1) * (1 - p)^(b - 1)\npval <- seq(0, 1, 0.01)\n\nplot(pval, wbeta(pval, 1, 1), type = \"l\", ylim = c(0, 1), col = 1, lwd = 2,\n     xlab = \"p value\", ylab = \"Probability of Publishing\")\nlines(pval, wbeta(pval, 1, 2), col = 2, lwd = 2)\nlines(pval, wbeta(pval, 1, 5), col = 3, lwd = 2)\nlines(pval, wbeta(pval, 1, 50), col = 4, lwd = 2)\n```\n\n::: {.cell-output-display}\n![](chapter5_files/figure-html/unnamed-chunk-107-1.png){width=672}\n:::\n:::\n\n\n\n\n\n## SM - Weigth function\n\nWhatever the function, the SM estimate the parameters of the function and the meta-analysis parameters taking into account the weigths. \n\nClearly, in the presence of no bias the two models (with and without weights) are the same while with PB the estimation is different, probably reducing the effect size.\n\nIf the SM is correct (not possible in reality), the SM estimate the true effect even in the presence of bias. This is the strenght and elegance of the SM.\n\n## SM - Weigth functions\n\nThere are several weight functions:\n\n- the step model\n- the negative-exponential model\n- the beta model\n- ...\n\nFor an overview see the `metafor` documentation https://wviechtb.github.io/metafor/reference/selmodel.html\n\n## SM - Step model\n\nThe step model approximate the selection process with thresholds $\\alpha$ and the associated weight $w(p_i)$. For example:\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsteps <- c(0.005, 0.01, 0.05, 0.10, 0.25, 0.35, 0.50, 0.65, 0.75, 0.90, 0.95, 0.99, 0.995, 1)\nmoderate_pb <- c(1, 0.99, 0.95, 0.80, 0.75, 0.65, 0.60, 0.55, 0.50, 0.50, 0.50, 0.50, 0.50, 0.50)\nsevere_pb <- c(1, 0.99, 0.90, 0.75, 0.60, 0.50, 0.40, 0.35, 0.30, 0.25, 0.10, 0.10, 0.10, 0.10)\n\npar(mfrow = c(1, 2))\nplot(steps, moderate_pb, type = \"s\", xlab = \"p value\", ylab = \"Probability of Selection\", main = \"Moderate PB\", ylim = c(0, 1))\nabline(v = steps, col = \"grey\")\nplot(steps, severe_pb, type = \"s\", xlab = \"p value\", ylab = \"Probability of Selection\", main = \"Severe PB\", ylim = c(0, 1))\nabline(v = steps, col = \"grey\")\n```\n\n::: {.cell-output-display}\n![](chapter5_files/figure-html/unnamed-chunk-108-1.png){width=672}\n:::\n:::\n\n\n\n\n\n## SM - Negative-Exponential\n\nThe Negative-Exponential model is very simple and intuitive. The weight function is $e^{-\\delta p_i}$ thus the single parameter $\\delta$ is the amount of bias. When $\\delta = 0$ there is no bias.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwnegexp <- function(p, delta){\n  exp((-delta)*p)\n}\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ncurve(wnegexp(x, 0), ylim = c(0, 1), col = 1, lwd = 2, xlab = \"p value\", ylab = \"Probability of Selection\")\ncurve(wnegexp(x, 1), add = TRUE, col = 2, lwd = 2)\ncurve(wnegexp(x, 5), add = TRUE, col = 3, lwd = 2)\ncurve(wnegexp(x, 30), add = TRUE, col = 4, lwd = 2)\n\nlegend(\"topright\", legend = latex2exp::TeX(sprintf(\"$\\\\delta = %s$\", c(1, 2, 3, 4))), fill = 1:4)\n```\n\n::: {.cell-output-display}\n![](chapter5_files/figure-html/unnamed-chunk-110-1.png){width=672}\n:::\n:::\n\n\n\n\n\n## Simulating data with PB\n\nThe strategy to simulate biased data is to sample from the `sim_studies()` function but to keep the studies using a probabilistic sampling based on the weight function.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(2024)\n\nk <- 500 # high number to check the results\nes <- 0 # H0 true\ntau2 <- 0.1\ndelta <- 5\ndat <- vector(mode = \"list\", k)\n\ni <- 1\nwhile(i <= k){\n  # generate data\n  n <- runif(1, 10, 100)\n  d <- summary(sim_studies(1, es, tau2, n))\n  # get one-tail p-value\n  pi <- 1 - pnorm(d$zi)\n  # get wi\n  wpi <- wnegexp(pi, delta)\n  keep <- rbinom(1, 1, wpi) == 1\n  if(keep){\n    dat[[i]] <- d\n    i <- i + 1\n  }\n}\n\ndat <- do.call(rbind, dat)\nfit <- rma(yi, vi, data = dat)\n```\n:::\n\n\n\n\n\n## Simulating data with PB\n\nLet's see some plots:\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\npar(mfrow = c(1, 3))\nhist(dat$yi, breaks = 50, col = \"dodgerblue\")\nhist(1 - pnorm(dat$zi), breaks = 50, col = \"dodgerblue\")\nplot(dat$yi, dat$sei, ylim = c(rev(range(dat$sei)[2]), 0), xlim = c(-1, 2))\n```\n\n::: {.cell-output-display}\n![](chapter5_files/figure-html/unnamed-chunk-112-1.png){width=672}\n:::\n:::\n\n\n\n\n\n## Simulating data with PB\n\nLet's see the model result:\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit <- rma(yi, vi, data = dat)\n```\n:::\n\n\n\n\n\n\n\n## Simulating data with PB\n\nLet's see the Egger regression test and the trim-and-fill procedure:\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nregtest(fit)\n#> \n#> Regression Test for Funnel Plot Asymmetry\n#> \n#> Model:     mixed-effects meta-regression model\n#> Predictor: standard error\n#> \n#> Test for Funnel Plot Asymmetry: z = 4.4739, p < .0001\n#> Limit Estimate (as sei -> 0):   b = 0.2017 (CI: 0.1235, 0.2799)\ntrimfill(fit)\n#> \n#> Estimated number of missing studies on the left side: 106 (SE = 14.6182)\n#> \n#> Random-Effects Model (k = 606; tau^2 estimator: REML)\n#> \n#> tau^2 (estimated amount of total heterogeneity): 0.0474 (SE = 0.0049)\n#> tau (square root of estimated tau^2 value):      0.2176\n#> I^2 (total heterogeneity / total variability):   56.94%\n#> H^2 (total variability / sampling variability):  2.32\n#> \n#> Test for Heterogeneity:\n#> Q(df = 605) = 1399.3991, p-val < .0001\n#> \n#> Model Results:\n#> \n#> estimate      se     zval    pval   ci.lb   ci.ub      \n#>   0.2927  0.0121  24.1497  <.0001  0.2689  0.3164  *** \n#> \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n\n\n\n\n\n## Simulating data with PB\n\nThe two methods are detecting the PB but not correcting it appropriately. Let's see the SM using a `negexp` method:\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsel <- selmodel(fit, type = \"negexp\", alternative = \"greater\")\nsel\n#> \n#> Random-Effects Model (k = 500; tau^2 estimator: ML)\n#> \n#> tau^2 (estimated amount of total heterogeneity): 0.0800 (SE = 0.0118)\n#> tau (square root of estimated tau^2 value):      0.2828\n#> \n#> Test for Heterogeneity:\n#> LRT(df = 1) = 115.9867, p-val < .0001\n#> \n#> Model Results:\n#> \n#> estimate      se    zval    pval    ci.lb   ci.ub    \n#>   0.0519  0.0381  1.3609  0.1735  -0.0228  0.1266    \n#> \n#> Test for Selection Model Parameters:\n#> LRT(df = 1) = 46.6783, p-val < .0001\n#> \n#> Selection Model Results:\n#> \n#> estimate      se     zval    pval   ci.lb   ci.ub      \n#>   4.7526  0.3818  12.4494  <.0001  4.0044  5.5008  *** \n#> \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n\n\n\n\n\n## Simulating data with PB\n\nWe can also plot the results:\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(sel)\n```\n\n::: {.cell-output-display}\n![](chapter5_files/figure-html/unnamed-chunk-117-1.png){width=672}\n:::\n:::\n\n\n\n\n\n## PB Sensitivity analysis\n\n- The SM is correctly detecting, estimating and correcting the PB. But we simulated a pretty strong bias with $k = 500$ studies. In reality meta-analyses have few studies.\n- @Vevea2005-xc proposed to fix the weight function parameters to certain values representing different degree of selection and check how the model changes.\n- If the model parameters are affected after taking into account the SM, this could be considered as an index of PB.\n- This approach is really interesting in general but especially when $k$ is too small for estimating the SM\n- see `?selmodel` for information about performing sensitivity analysis with pre-specified weight functions\n\n## More on SM and Publication Bias\n\n- The SM documentation of `metafor::selmodel()` [https://wviechtb.github.io/metafor/reference/selmodel.html](https://www.youtube.com/watch?v=ucmOCuyCk-c)\n- Wolfgang Viechtbauer overview of PB [https://www.youtube.com/watch?v=ucmOCuyCk-c](https://www.youtube.com/watch?v=ucmOCuyCk-c)\n- @Harrer2021-go - Doing Meta-analysis in R - [Chapter 9](https://bookdown.org/MathiasHarrer/Doing_Meta_Analysis_in_R/pub-bias.html)\n- @McShane2016-bk for a nice introduction about publication bias and SM\n- Another good overview by @Jin2015-ik\n- See also @Guan2016-kn, @Maier2023-js and @Bartos2022-im for Bayesian approaches to PB\n\n## References <button class=\"btn\"><i class=\"fa fa-download\"></i><a href=\"data:text/x-bibtex;base64,@ARTICLE{Shi2019-pj,
  title = {The trim-and-fill method for publication bias: practical guidelines
  and recommendations based on a large database of meta-analyses: practical
  guidelines and recommendations based on a large database of meta-analyses},
  author = {Shi, Linyu and Lin, Lifeng},
  journaltitle = {Medicine},
  publisher = {Ovid Technologies (Wolters Kluwer Health)},
  volume = {98},
  issue = {23},
  pages = {e15987},
  date = {2019-06},
  doi = {10.1097/MD.0000000000015987},
  pmc = {PMC6571372},
  pmid = {31169736},
  issn = {0025-7974,1536-5964},
  abstract = {Publication bias is a type of systematic error when synthesizing
  evidence that cannot represent the underlying truth. Clinical studies with
  favorable results are more likely published and thus exaggerate the
  synthesized evidence in meta-analyses. The trim-and-fill method is a popular
  tool to detect and adjust for publication bias. Simulation studies have been
  performed to assess this method, but they may not fully represent realistic
  settings about publication bias. Based on real-world meta-analyses, this
  article provides practical guidelines and recommendations for using the
  trim-and-fill method. We used a worked illustrative example to demonstrate the
  idea of the trim-and-fill method, and we reviewed three estimators (R0, L0,
  and Q0) for imputing missing studies. A resampling method was proposed to
  calculate P values for all 3 estimators. We also summarized available
  meta-analysis software programs for implementing the trim-and-fill method.
  Moreover, we applied the method to 29,932 meta-analyses from the Cochrane
  Database of Systematic Reviews, and empirically evaluated its overall
  performance. We carefully explored potential issues occurred in our analysis.
  The estimators L0 and Q0 detected at least one missing study in more
  meta-analyses than R0, while Q0 often imputed more missing studies than L0.
  After adding imputed missing studies, the significance of heterogeneity and
  overall effect sizes changed in many meta-analyses. All estimators generally
  converged fast. However, L0 and Q0 failed to converge in a few meta-analyses
  that contained studies with identical effect sizes. Also, P values produced by
  different estimators could yield different conclusions of publication bias
  significance. Outliers and the pre-specified direction of missing studies
  could have influential impact on the trim-and-fill results. Meta-analysts are
  recommended to perform the trim-and-fill method with great caution when using
  meta-analysis software programs. Some default settings (e.g., the choice of
  estimators and the direction of missing studies) in the programs may not be
  optimal for a certain meta-analysis; they should be determined on a
  case-by-case basis. Sensitivity analyses are encouraged to examine effects of
  different estimators and outlying studies. Also, the trim-and-fill estimator
  should be routinely reported in meta-analyses, because the results depend
  highly on it.},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6571372/},
  language = {en}
}

@ARTICLE{Viechtbauer2005-zt,
  title = {Bias and efficiency of meta-analytic variance estimators in the
  random-effects model},
  author = {Viechtbauer, Wolfgang},
  journaltitle = {Journal of educational and behavioral statistics: a quarterly
  publication sponsored by the American Educational Research Association and the
  American Statistical Association},
  publisher = {American Educational Research Association (AERA)},
  volume = {30},
  issue = {3},
  pages = {261-293},
  date = {2005-09},
  doi = {10.3102/10769986030003261},
  issn = {1076-9986,1935-1054},
  abstract = {The meta-analytic random effects model assumes that the
  variability in effect size estimates drawn from a set of studies can be
  decomposed into two parts: heterogeneity due to random population effects and
  sampling variance. In this context, the usual goal is to estimate the central
  tendency and the amount of heterogeneity in the population effect sizes. The
  amount of heterogeneity in a set of effect sizes has implications regarding
  the interpretation of the meta-analytic findings and often serves as an
  indicator for the presence of potential moderator variables. Five population
  heterogeneity estimators were compared in this article analytically and via
  Monte Carlo simulations with respect to their bias and efficiency.},
  url = {https://scholar.google.com/citations?view_op=view_citation&hl=en&user=J89RXJkAAAAJ&citation_for_view=J89RXJkAAAAJ:qjMakFHDy7sC},
  language = {en}
}

@ARTICLE{Rosenthal1979-yx,
  title = {The file drawer problem and tolerance for null results},
  author = {Rosenthal, Robert},
  journaltitle = {Psychological bulletin},
  publisher = {American Psychological Association (APA)},
  volume = {86},
  issue = {3},
  pages = {638-641},
  date = {1979-05},
  doi = {10.1037/0033-2909.86.3.638},
  issn = {0033-2909,1939-1455},
  url = {https://psycnet.apa.org/record/1979-27602-001},
  language = {en}
}

@ARTICLE{Orwin1983-vu,
  title = {A fail-SafeN for effect size in meta-analysis},
  author = {Orwin, Robert G},
  journaltitle = {Journal of educational statistics},
  publisher = {American Educational Research Association (AERA)},
  volume = {8},
  issue = {2},
  pages = {157-159},
  date = {1983-06},
  doi = {10.3102/10769986008002157},
  issn = {0362-9791,2328-0735},
  abstract = {Rosenthan’s (1979) concept of fail-safe N has thus far been
  applied to probability levels exclusively. This note introduces a fail-safe N
  for effect size.},
  url = {https://journals.sagepub.com/doi/abs/10.3102/10769986008002157},
  language = {en}
}

@ARTICLE{Rosenberg2005-ie,
  title = {The file-drawer problem revisited: a general weighted method for
  calculating fail-safe numbers in meta-analysis},
  author = {Rosenberg, Michael S},
  journaltitle = {Evolution; international journal of organic evolution},
  publisher = {Wiley},
  volume = {59},
  issue = {2},
  pages = {464-468},
  date = {2005-02},
  doi = {10.1111/j.0014-3820.2005.tb01004.x},
  pmid = {15807430},
  issn = {0014-3820,1558-5646},
  abstract = {Quantitative literature reviews such as meta-analysis are becoming
  common in evolutionary biology but may be strongly affected by publication
  biases. Using fail-safe numbers is a quick way to estimate whether publication
  bias is likely to be a problem for a specific study. However, previously
  suggested fail-safe calculations are unweighted and are not based on the
  framework in which most meta-analyses are performed. A general, weighted
  fail-safe calculation, grounded in the meta-analysis framework, applicable to
  both fixed- and random-effects models, is proposed. Recent meta-analyses
  published in Evolution are used for illustration.},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.0014-3820.2005.tb01004.x},
  language = {en}
}

@ARTICLE{Citkowicz2017-ox,
  title = {A parsimonious weight function for modeling publication bias},
  author = {Citkowicz, Martyna and Vevea, Jack L},
  journaltitle = {Psychological methods},
  publisher = {psycnet.apa.org},
  volume = {22},
  issue = {1},
  pages = {28-41},
  date = {2017-03},
  doi = {10.1037/met0000119},
  pmid = {28252998},
  issn = {1082-989X,1939-1463},
  abstract = {Quantitative research literature is often biased because studies
  that fail to find a significant effect (or that demonstrate effects in an
  undesired or unexpected direction) are less likely to be published. This
  phenomenon, termed publication bias, can cause problems when researchers
  attempt to synthesize results using meta-analytic methods. Various techniques
  exist that attempt to estimate and correct meta-analyses for publication bias.
  However, there is no single method that can (a) account for continuous
  moderators by including them within the model, (b) allow for substantial data
  heterogeneity, (c) produce an adjusted mean effect size, (d) include a formal
  test for publication bias, and (e) allow for correction when only a small
  number of effects is included in the analysis. This article describes a method
  that we believe helps fill that gap. The model uses the beta density as a
  weight function that represents the selection process and provides adjusted
  parameter estimates that account for publication bias. Use of the beta density
  allows us to represent selection using fewer parameters than similar models so
  that the proposed model is suitable for meta-analyses that include relatively
  few studies. We explain the model and its rationale, illustrate its use with a
  real data set, and describe the results of a simulation study that shows the
  model's utility. (PsycINFO Database Record},
  url = {https://psycnet.apa.org/journals/met/22/1/28/},
  language = {en}
}

@ARTICLE{McShane2016-bk,
  title = {Adjusting for publication bias in meta-analysis: An evaluation of
  selection methods and some cautionary notes: An evaluation of selection
  methods and some cautionary notes},
  author = {McShane, Blakeley B and Böckenholt, Ulf and Hansen, Karsten T},
  journaltitle = {Perspectives on psychological science: a journal of the
  Association for Psychological Science},
  publisher = {SAGE Publications},
  volume = {11},
  issue = {5},
  pages = {730-749},
  date = {2016-09},
  doi = {10.1177/1745691616662243},
  pmid = {27694467},
  issn = {1745-6916,1745-6924},
  abstract = {We review and evaluate selection methods, a prominent class of
  techniques first proposed by Hedges (1984) that assess and adjust for
  publication bias in meta-analysis, via an extensive simulation study. Our
  simulation covers both restrictive settings as well as more realistic settings
  and proceeds across multiple metrics that assess different aspects of model
  performance. This evaluation is timely in light of two recently proposed
  approaches, the so-called p-curve and p-uniform approaches, that can be viewed
  as alternative implementations of the original Hedges selection method
  approach. We find that the p-curve and p-uniform approaches perform reasonably
  well but not as well as the original Hedges approach in the restrictive
  setting for which all three were designed. We also find they perform poorly in
  more realistic settings, whereas variants of the Hedges approach perform well.
  We conclude by urging caution in the application of selection methods: Given
  the idealistic model assumptions underlying selection methods and the
  sensitivity of population average effect size estimates to them, we advocate
  that selection methods should be used less for obtaining a single estimate
  that purports to adjust for publication bias ex post and more for sensitivity
  analysis-that is, exploring the range of estimates that result from assuming
  different forms of and severity of publication bias.},
  url = {https://journals.sagepub.com/doi/abs/10.1177/1745691616662243},
  keywords = {effect size; meta-analysis; p-curve; p-uniform; selection methods},
  language = {en}
}

@ARTICLE{Bartos2022-im,
  title = {Adjusting for publication bias in JASP and R: Selection models,
  PET-PEESE, and robust Bayesian meta-analysis},
  author = {Bartoš, František and Maier, Maximilian and Quintana, Daniel S and
  Wagenmakers, Eric-Jan},
  journaltitle = {Advances in methods and practices in psychological science},
  publisher = {SAGE Publications},
  volume = {5},
  issue = {3},
  pages = {251524592211092},
  date = {2022-07},
  doi = {10.1177/25152459221109259},
  issn = {2515-2459,2515-2467},
  abstract = {Meta-analyses are essential for cumulative science, but their
  validity can be compromised by publication bias. To mitigate the impact of
  publication bias, one may apply publication-bias-adjustment techniques such as
  precision-effect test and precision-effect estimate with standard errors
  (PET-PEESE) and selection models. These methods, implemented in JASP and R,
  allow researchers without programming experience to conduct state-of-the-art
  publication-bias-adjusted meta-analysis. In this tutorial, we demonstrate how
  to conduct a publication-bias-adjusted meta-analysis in JASP and R and
  interpret the results. First, we explain two frequentist bias-correction
  methods: PET-PEESE and selection models. Second, we introduce robust Bayesian
  meta-analysis, a Bayesian approach that simultaneously considers both
  PET-PEESE and selection models. We illustrate the methodology on an example
  data set, provide an instructional video ( https://bit.ly/pubbias ) and an
  R-markdown script ( https://osf.io/uhaew/ ), and discuss the interpretation of
  the results. Finally, we include concrete guidance on reporting the
  meta-analytic results in an academic article.},
  url = {https://journals.sagepub.com/doi/abs/10.1177/25152459221109259},
  language = {en}
}

@ARTICLE{Maier2023-js,
  title = {Robust Bayesian meta-analysis: Addressing publication bias with
  model-averaging},
  author = {Maier, Maximilian and Bartoš, František and Wagenmakers, Eric-Jan},
  journaltitle = {Psychological methods},
  volume = {28},
  issue = {1},
  pages = {107-122},
  date = {2023-02},
  doi = {10.1037/met0000405},
  pmid = {35588075},
  issn = {1082-989X,1939-1463},
  abstract = {Meta-analysis is an important quantitative tool for cumulative
  science, but its application is frustrated by publication bias. In order to
  test and adjust for publication bias, we extend model-averaged Bayesian
  meta-analysis with selection models. The resulting robust Bayesian
  meta-analysis (RoBMA) methodology does not require all-or-none decisions about
  the presence of publication bias, can quantify evidence in favor of the
  absence of publication bias, and performs well under high heterogeneity. By
  model-averaging over a set of 12 models, RoBMA is relatively robust to model
  misspecification and simulations show that it outperforms existing methods. We
  demonstrate that RoBMA finds evidence for the absence of publication bias in
  Registered Replication Reports and reliably avoids false positives. We provide
  an implementation in R so that researchers can easily use the new methodology
  in practice. (PsycInfo Database Record (c) 2023 APA, all rights reserved).},
  url = {https://psycnet.apa.org/record/2022-62552-001},
  language = {en}
}

@ARTICLE{Guan2016-kn,
  title = {A Bayesian approach to mitigation of publication bias},
  author = {Guan, Maime and Vandekerckhove, Joachim},
  journaltitle = {Psychonomic bulletin \& review},
  publisher = {Springer Science and Business Media LLC},
  volume = {23},
  issue = {1},
  pages = {74-86},
  date = {2016-02},
  doi = {10.3758/s13423-015-0868-6},
  pmid = {26126776},
  issn = {1069-9384,1531-5320},
  abstract = {The reliability of published research findings in psychology has
  been a topic of rising concern. Publication bias, or treating positive
  findings differently from negative findings, is a contributing factor to this
  "crisis of confidence," in that it likely inflates the number of
  false-positive effects in the literature. We demonstrate a Bayesian model
  averaging approach that takes into account the possibility of publication bias
  and allows for a better estimate of true underlying effect size. Accounting
  for the possibility of bias leads to a more conservative interpretation of
  published studies as well as meta-analyses. We provide mathematical details of
  the method and examples.},
  url = {https://dx.doi.org/10.3758/s13423-015-0868-6},
  keywords = {Bayesian inference and parameter estimation; Bayesian statistics;
  Math modeling and model selection; Meta-analysis;Bayesian
  Statistics;replicability-book;extra},
  language = {en}
}

@ARTICLE{Schad2020-ht,
  title = {How to capitalize on a priori contrasts in linear (mixed) models: A
  tutorial},
  author = {Schad, Daniel J and Vasishth, Shravan and Hohenstein, Sven and
  Kliegl, Reinhold},
  journaltitle = {Journal of memory and language},
  volume = {110},
  pages = {104038},
  date = {2020-02-01},
  doi = {10.1016/j.jml.2019.104038},
  issn = {0749-596X},
  abstract = {Factorial experiments in research on memory, language, and in
  other areas are often analyzed using analysis of variance (ANOVA). However,
  for effects with more than one numerator degrees of freedom, e.g., for
  experimental factors with more than two levels, the ANOVA omnibus F-test is
  not informative about the source of a main effect or interaction. Because
  researchers typically have specific hypotheses about which condition means
  differ from each other, a priori contrasts (i.e., comparisons planned before
  the sample means are known) between specific conditions or combinations of
  conditions are the appropriate way to represent such hypotheses in the
  statistical model. Many researchers have pointed out that contrasts should be
  “tested instead of, rather than as a supplement to, the ordinary ‘omnibus’ F
  test” (Hays, 1973, p. 601). In this tutorial, we explain the mathematics
  underlying different kinds of contrasts (i.e., treatment, sum, repeated,
  polynomial, custom, nested, interaction contrasts), discuss their properties,
  and demonstrate how they are applied in the R System for Statistical Computing
  (R Core Team, 2018). In this context, we explain the generalized inverse which
  is needed to compute the coefficients for contrasts that test hypotheses that
  are not covered by the default set of contrasts. A detailed understanding of
  contrast coding is crucial for successful and correct specification in linear
  models (including linear mixed models). Contrasts defined a priori yield far
  more useful confirmatory tests of experimental hypotheses than standard
  omnibus F-tests. Reproducible code is available from https://osf.io/7ukf6/.},
  url = {https://www.sciencedirect.com/science/article/pii/S0749596X19300695},
  keywords = {Contrasts; Null hypothesis significance testing; Linear models; A
  priori hypotheses;Bayesian Statistics}
}

@BOOK{Harrer2021-go,
  title = {Doing meta-analysis with R: A hands-on guide},
  author = {Harrer, Mathias and Cuijpers, Pim and Furukawa, Toshi and Ebert,
  David},
  publisher = {CRC Press},
  location = {London, England},
  edition = {1st},
  date = {2021-09-13},
  pagetotal = {474},
  isbn = {9780367610074},
  language = {en}
}

@ARTICLE{Duval2000-ym,
  title = {Trim and fill: A simple funnel-plot-based method of testing and
  adjusting for publication bias in meta-analysis},
  author = {Duval, S and Tweedie, R},
  journaltitle = {Biometrics},
  publisher = {Wiley},
  volume = {56},
  issue = {2},
  pages = {455-463},
  date = {2000-06},
  doi = {10.1111/j.0006-341x.2000.00455.x},
  pmid = {10877304},
  issn = {0006-341X,1541-0420},
  abstract = {We study recently developed nonparametric methods for estimating
  the number of missing studies that might exist in a meta-analysis and the
  effect that these studies might have had on its outcome. These are simple
  rank-based data augmentation techniques, which formalize the use of funnel
  plots. We show that they provide effective and relatively powerful tests for
  evaluating the existence of such publication bias. After adjusting for missing
  studies, we find that the point estimate of the overall effect size is
  approximately correct and coverage of the effect size confidence intervals is
  substantially improved, in many cases recovering the nominal confidence levels
  entirely. We illustrate the trim and fill method on existing meta-analyses of
  studies in clinical trials and psychometrics.},
  url = {https://onlinelibrary.wiley.com/doi/10.1111/j.0006-341X.2000.00455.x},
  keywords = {Meta-analysis},
  language = {en}
}

@ARTICLE{Veroniki2016-nw,
  title = {Methods to estimate the between-study variance and its uncertainty in
  meta-analysis},
  author = {Veroniki, Areti Angeliki and Jackson, Dan and Viechtbauer, Wolfgang
  and Bender, Ralf and Bowden, Jack and Knapp, Guido and Kuss, Oliver and
  Higgins, Julian P T and Langan, Dean and Salanti, Georgia},
  journaltitle = {Research synthesis methods},
  publisher = {Wiley},
  volume = {7},
  issue = {1},
  pages = {55-79},
  date = {2016-03},
  doi = {10.1002/jrsm.1164},
  pmc = {PMC4950030},
  pmid = {26332144},
  issn = {1759-2879,1759-2887},
  abstract = {Meta-analyses are typically used to estimate the overall/mean of
  an outcome of interest. However, inference about between-study variability,
  which is typically modelled using a between-study variance parameter, is
  usually an additional aim. The DerSimonian and Laird method, currently widely
  used by default to estimate the between-study variance, has been long
  challenged. Our aim is to identify known methods for estimation of the
  between-study variance and its corresponding uncertainty, and to summarise the
  simulation and empirical evidence that compares them. We identified 16
  estimators for the between-study variance, seven methods to calculate
  confidence intervals, and several comparative studies. Simulation studies
  suggest that for both dichotomous and continuous data the estimator proposed
  by Paule and Mandel and for continuous data the restricted maximum likelihood
  estimator are better alternatives to estimate the between-study variance.
  Based on the scenarios and results presented in the published studies, we
  recommend the Q-profile method and the alternative approach based on a
  'generalised Cochran between-study variance statistic' to compute
  corresponding confidence intervals around the resulting estimates. Our
  recommendations are based on a qualitative evaluation of the existing
  literature and expert consensus. Evidence-based recommendations require an
  extensive simulation study where all methods would be compared under the same
  scenarios.},
  url = {https://onlinelibrary.wiley.com/doi/10.1002/jrsm.1164},
  file = {Veroniki et al. 2016 - Methods to estimate the between-study variance and its uncertainty in meta-analysis.pdf},
  keywords = {bias; confidence interval; coverage probability; heterogeneity;
  mean squared error},
  language = {en}
}

@MISC{Borenstein2009-mo,
  title = {Introduction to {Meta-Analysis}},
  author = {Borenstein, Michael and Hedges, Larry V and Higgins, Julian P T and
  Rothstein, Hannah R},
  date = {2009},
  doi = {10.1002/9780470743386},
  url = {http://dx.doi.org/10.1002/9780470743386}
}

@ARTICLE{Riley2011-hp,
  title = {Interpretation of random effects meta-analyses},
  author = {Riley, Richard D and Higgins, Julian P T and Deeks, Jonathan J},
  journaltitle = {BMJ},
  volume = {342},
  pages = {d549},
  date = {2011-02-10},
  doi = {10.1136/bmj.d549},
  pmid = {21310794},
  issn = {0959-8138,1756-1833},
  url = {http://dx.doi.org/10.1136/bmj.d549},
  language = {en}
}

@ARTICLE{IntHout2016-sz,
  title = {Plea for routinely presenting prediction intervals in meta-analysis},
  author = {IntHout, Joanna and Ioannidis, John P A and Rovers, Maroeska M and
  Goeman, Jelle J},
  journaltitle = {BMJ open},
  volume = {6},
  issue = {7},
  pages = {e010247},
  date = {2016-07-12},
  doi = {10.1136/bmjopen-2015-010247},
  pmc = {PMC4947751},
  pmid = {27406637},
  issn = {2044-6055},
  abstract = {OBJECTIVES: Evaluating the variation in the strength of the effect
  across studies is a key feature of meta-analyses. This variability is
  reflected by measures like τ(2) or I(2), but their clinical interpretation is
  not straightforward. A prediction interval is less complicated: it presents
  the expected range of true effects in similar studies. We aimed to show the
  advantages of having the prediction interval routinely reported in
  meta-analyses. DESIGN: We show how the prediction interval can help understand
  the uncertainty about whether an intervention works or not. To evaluate the
  implications of using this interval to interpret the results, we selected the
  first meta-analysis per intervention review of the Cochrane Database of
  Systematic Reviews Issues 2009-2013 with a dichotomous (n=2009) or continuous
  (n=1254) outcome, and generated 95\% prediction intervals for them. RESULTS:
  In 72.4\% of 479 statistically significant (random-effects p0), the 95\%
  prediction interval suggested that the intervention effect could be null or
  even be in the opposite direction. In 20.3\% of those 479 meta-analyses, the
  prediction interval showed that the effect could be completely opposite to the
  point estimate of the meta-analysis. We demonstrate also how the prediction
  interval can be used to calculate the probability that a new trial will show a
  negative effect and to improve the calculations of the power of a new trial.
  CONCLUSIONS: The prediction interval reflects the variation in treatment
  effects over different settings, including what effect is to be expected in
  future patients, such as the patients that a clinician is interested to treat.
  Prediction intervals should be routinely reported to allow more informative
  inferences in meta-analyses.},
  url = {http://dx.doi.org/10.1136/bmjopen-2015-010247},
  keywords = {Clinical trial; Cochrane Database of Systematic Reviews;
  Heterogeneity; Meta-analysis; Prediction interval; Random effects;Bayesian
  Statistics},
  language = {en}
}

@ARTICLE{Vevea2005-xc,
  title = {Publication bias in research synthesis: sensitivity analysis using a
  priori weight functions},
  author = {Vevea, Jack L and Woods, Carol M},
  journaltitle = {Psychological methods},
  publisher = {American Psychological Association (APA)},
  volume = {10},
  issue = {4},
  pages = {428-443},
  date = {2005-12},
  doi = {10.1037/1082-989X.10.4.428},
  pmid = {16392998},
  issn = {1082-989X,1939-1463},
  abstract = {Publication bias, sometimes known as the "file-drawer problem" or
  "funnel-plot asymmetry," is common in empirical research. The authors review
  the implications of publication bias for quantitative research synthesis
  (meta-analysis) and describe existing techniques for detecting and correcting
  it. A new approach is proposed that is suitable for application to
  meta-analytic data sets that are too small for the application of existing
  methods. The model estimates parameters relevant to fixed-effects,
  mixed-effects or random-effects meta-analysis contingent on a hypothetical
  pattern of bias that is fixed independently of the data. The authors
  illustrate this approach for sensitivity analysis using 3 data sets adapted
  from a commonly cited reference work on research synthesis (H. M. Cooper \& L.
  V. Hedges, 1994).},
  url = {https://psycnet.apa.org/record/2005-16136-006},
  urldate = {2024-02-03},
  language = {en}
}

@ARTICLE{Higgins2002-fh,
  title = {Quantifying heterogeneity in a meta-analysis},
  author = {Higgins, Julian P T and Thompson, Simon G},
  journaltitle = {Statistics in medicine},
  volume = {21},
  issue = {11},
  pages = {1539-1558},
  date = {2002-06-15},
  doi = {10.1002/sim.1186},
  pmid = {12111919},
  issn = {0277-6715},
  abstract = {The extent of heterogeneity in a meta-analysis partly determines
  the difficulty in drawing overall conclusions. This extent may be measured by
  estimating a between-study variance, but interpretation is then specific to a
  particular treatment effect metric. A test for the existence of heterogeneity
  exists, but depends on the number of studies in the meta-analysis. We develop
  measures of the impact of heterogeneity on a meta-analysis, from mathematical
  criteria, that are independent of the number of studies and the treatment
  effect metric. We derive and propose three suitable statistics: H is the
  square root of the chi2 heterogeneity statistic divided by its degrees of
  freedom; R is the ratio of the standard error of the underlying mean from a
  random effects meta-analysis to the standard error of a fixed effect
  meta-analytic estimate, and I2 is a transformation of (H) that describes the
  proportion of total variation in study estimates that is due to heterogeneity.
  We discuss interpretation, interval estimates and other properties of these
  measures and examine them in five example data sets showing different amounts
  of heterogeneity. We conclude that H and I2, which can usually be calculated
  for published meta-analyses, are particularly useful summaries of the impact
  of heterogeneity. One or both should be presented in published meta-analyses
  in preference to the test for heterogeneity.},
  url = {http://dx.doi.org/10.1002/sim.1186},
  language = {en}
}

@ARTICLE{Lopez-Lopez2014-it,
  title = {Estimation of the predictive power of the model in mixed-effects
  meta-regression: A simulation study},
  author = {López-López, José Antonio and Marín-Martínez, Fulgencio and
  Sánchez-Meca, Julio and Van den Noortgate, Wim and Viechtbauer, Wolfgang},
  journaltitle = {The British journal of mathematical and statistical psychology},
  publisher = {Wiley},
  volume = {67},
  issue = {1},
  pages = {30-48},
  date = {2014-02},
  doi = {10.1111/bmsp.12002},
  pmid = {23297709},
  issn = {0007-1102,2044-8317},
  abstract = {Several methods are available to estimate the total and residual
  amount of heterogeneity in meta-analysis, leading to different alternatives
  when estimating the predictive power in mixed-effects meta-regression models
  using the formula proposed by Raudenbush (1994, 2009). In this paper, a
  simulation study was conducted to compare the performance of seven estimators
  of these parameters under various realistic scenarios in psychology and
  related fields. Our results suggest that the number of studies (k) exerts the
  most important influence on the accuracy of the results, and that precise
  estimates of the heterogeneity variances and the model predictive power can
  only be expected with at least 20 and 40 studies, respectively. Increases in
  the average within-study sample size (N¯) also improved the results for all
  estimators. Some differences among the accuracy of the estimators were
  observed, especially under adverse (small k and N¯) conditions, while the
  results for the different methods tended to convergence for more optimal
  scenarios.},
  url = {https://bpspsychub.onlinelibrary.wiley.com/doi/abs/10.1111/bmsp.12002},
  language = {en}
}

@BOOK{Gelman2020-tg,
  title = {Regression and Other Stories},
  author = {Gelman, Andrew and Hill, Jennifer and Vehtari, Aki},
  publisher = {Cambridge University Press},
  date = {2020-07-23},
  doi = {10.1017/9781139161879},
  isbn = {9781139161879},
  abstract = {Most textbooks on regression focus on theory and the simplest of
  examples. Real statistical problems, however, are complex and subtle. This is
  not a book about the theory of regression. It is about using regression to
  solve real problems of comparison, estimation, prediction, and causal
  inference. Unlike other books, it focuses on practical issues such as sample
  size and missing data and a wide range of goals and techniques. It jumps right
  in to methods and computer code you can use immediately. Real examples, real
  stories from the authors' experience demonstrate what regression can do and
  its limitations, with practical advice for understanding assumptions and
  implementing methods for experiments and observational studies. They make a
  smooth transition to logistic regression and GLM. The emphasis is on
  computation in R and Stan rather than derivations, with code available online.
  Graphics and presentation aid understanding of the models and model fitting.},
  url = {https://www.cambridge.org/highereducation/books/regression-and-other-stories/DD20DD6C9057118581076E54E40C372C},
  urldate = {2023-02-14}
}

@ARTICLE{Hedges1989-ip,
  title = {An unbiased correction for sampling error in validity generalization
  studies},
  author = {Hedges, Larry V},
  journaltitle = {The Journal of applied psychology},
  publisher = {American Psychological Association (APA)},
  volume = {74},
  issue = {3},
  pages = {469-477},
  date = {1989-06},
  doi = {10.1037/0021-9010.74.3.469},
  issn = {0021-9010,1939-1854},
  url = {http://dx.doi.org/10.1037/0021-9010.74.3.469},
  language = {en}
}

@ARTICLE{Hedges2019-ry,
  title = {Statistical analyses for studying replication: Meta-analytic
  perspectives},
  author = {Hedges, Larry V and Schauer, Jacob M},
  journaltitle = {Psychological methods},
  publisher = {American Psychological Association (APA)},
  volume = {24},
  issue = {5},
  pages = {557-570},
  date = {2019-10},
  doi = {10.1037/met0000189},
  pmid = {30070547},
  issn = {1082-989X,1939-1463},
  abstract = {Formal empirical assessments of replication have recently become
  more prominent in several areas of science, including psychology. These
  assessments have used different statistical approaches to determine if a
  finding has been replicated. The purpose of this article is to provide several
  alternative conceptual frameworks that lead to different statistical analyses
  to test hypotheses about replication. All of these analyses are based on
  statistical methods used in meta-analysis. The differences among the methods
  described involve whether the burden of proof is placed on replication or
  nonreplication, whether replication is exact or allows for a small amount of
  "negligible heterogeneity," and whether the studies observed are assumed to be
  fixed (constituting the entire body of relevant evidence) or are a sample from
  a universe of possibly relevant studies. The statistical power of each of
  these tests is computed and shown to be low in many cases, raising issues of
  the interpretability of tests for replication. (PsycINFO Database Record (c)
  2019 APA, all rights reserved).},
  url = {http://dx.doi.org/10.1037/met0000189},
  file = {Hedges and Schauer 2019 - Statistical analyses for studying replication - Meta-analytic perspectives.pdf},
  keywords = {replicability-book;methods},
  language = {en}
}

\" download=\"refs_to_download.bib\"> Download .bib file</a></button> {.refs}\n\n::: {#refs}\n:::",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}