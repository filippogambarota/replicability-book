# Statistical methods for replication assessment {#sec-statmethodrep}

```{r}
#| include: false
library(tidyverse)
library(metafor)
library(rstanarm)
devtools::load_all()
funs <- read_all_funs()
knitr::opts_chunk$set(echo = TRUE)
```

## How to assess the replication outcome?

In earlier chapters, we introduced the concept of replication from both statistical and theoretical perspectives. We now focus on analyzing the outcomes of replication experiments. Just as there are many definitions of replication, there are also multiple ways of assessing the outcomes of replication studies.

Much of the discussion will focus on replication metrics, which is a burgeoning area of statistics. @Heyard2024-hv published an extensive systematic review, supported by an online catalog.  They also maintain an up-to-date database with more than 50 replication measures organized and classified according to a common scheme.

The database is available at the following link [http://rachelheyard.com/reproducibility_metrics/](http://rachelheyard.com/reproducibility_metrics/). Here, we have selected methods from the database, according to the following criteria:

- we have included methods not only for answering the question *"Has a study been replicated?"*, but also methods which instead combine evidence and estimate the size of the effect 
- we have selected methods that are not linked to a specific research area or topic. (Some methods are specific for a given area of research or type of experiment such as replicating fMRI studies at the voxel level.)
- we have included both bayesian and frequentist approaches
- where methods are very similar, we have focused on the most recent or general version

In general, choosing a single "measure of replicability" is probably not the best approach Typically, large-scale replication projects [e.g., @Collaboration2015-ep; @Errington2021-fb; @The-Brazilian-Reproducibility-Initiative2025-iw]  evaluate results across different metrics.

## Replication studies as meta-analysis

@Hedges2019-ry point out that replication studies can be seen --- from a statistical point of view --- as meta-analyses. Typically, there is a single initial study and one or more attempts to replicate its initial findings. For the simple one-to-one replication design we have a meta-analysis with two studies while in the one-to-many design we have a meta-analysis with $R$ studies. When it comes to evaluating the results of replication studies, we can choose between several methods [see @Heyard2024-hv]. Some of these methods will explictly involve meta-analyses, which pool evidence from original and replication studies.  While we consider meta-analytic thinking to be crucial for understanding replication, not all methods are (strictly speaking) meta-analyses in the usual sense.

Specifically, the methodology of the initial and/or replication studies is important when it comes to evaluating a single result.  However, at the aggregate level,  we evaluate how a certain focal parameter (e.g., the difference between two groups or conditions) varies across replications with an eye toward determining whether  there is evidence --- whatever the criterion --- for a successful replication.  In this sense, whatever model happens to be used in the original study, we can (without loss of generality) fruitfully model the situation from the aggregative perspective.

## Simulating data

While real-world examples are important, simulating simplified examples ("toy models") is useful for understanding replication methods from a statistical point of view. Indeed, in general, simulating data is an important technique for teaching and understanding statistical methods [@Hardin2015-ss]. Moreover, in practice, Monte Carlo simulations are necessary for estimating statistical properties (e.g., statistical power or type-1 error rate) of complex models.

For our simulated examples, we adpot the following methodological assumptions:

- primary studies always compare two independent groups on a certain response variable
- within a study, the two groups are assumed to come from two normal distributions with unit variance. For one group $y_0$ the mean is zero and for the other group $y_1$ the mean is the value representing the effect size $\theta_r$ for that specific study.
- the sample size can vary between the two groups

For example we can simulate a single study:

```{r}
#| echo: true
#| collapse: true

theta <- 0.3
n0 <- 30
n1 <- 30

y0 <- rnorm(n0, 0, 1)
y1 <- rnorm(n1, theta, 1)

mean(y1) - mean(y0) # this is the effect size
var(y1)/n1 + var(y0)/n0 # this is the sampling variability
```

```{r}
dd <- data.frame(y = c(y0, y1),
                 x = factor(rep(0:1, c(n0, n1))))

ggplot(dd, aes(x = x, y = y)) +
    geom_boxplot(aes(fill = x))
```


Now we can simply iterate the process for $R$ studies to create a series of replications with a common true parameter $\mu_{\theta}$. We can also generate a set of different sample sizes by sampling from a probability distribution (e.g., Poisson). Then, if we want to include variability in the true effects (as would be expected in the case of *extensions* of an experinent), we can sample $R$ effects from, say, a normal distribution with variance $\tau^2$.

```{r}
#| echo: true

R <- 10
mu <- 0.3 # average true effect
tau2 <- 0.1 # heterogeneity
deltai <- rnorm(R, mu, sqrt(tau2)) # true effects for R studies

yi <- vi <- rep(NA, R)
n0 <- n1 <- 10 + rpois(R, 40 - 10)

for(i in 1:R){
    y0 <- rnorm(n0[i], 0, 1)
    y1 <- rnorm(n1[i], deltai[i], 1)
    yi[i] <- mean(y1) - mean(y0)
    vi[i] <- var(y1)/n1[i] + var(y0)/n0[i]
}

sim <- data.frame(id = 1:R, yi, vi, n0, n1)
sim
```

Here, we put everything into a single function, which can be used to simulate different scenarios.

```{r}
#| echo: false
#| output: asis

filor::print_fun(c(funs$sim_study, funs$sim_studies))
```

```{r}
sim_studies(R = 10, mu = 0.5, tau2 = 0, n0 = 30)
```

## Methods

The database available at the following link [http://rachelheyard.com/reproducibility_metrics/](http://rachelheyard.com/reproducibility_metrics/) can be consulted for a more complete overview. Among all the available methods, we have selected the following metrics^[The name of the method corresponds to the first column of the online table], each of which will be illustrated below, with suitable examples.

- Prediction interval: is the replication effect inside the original 95% prediction interval?
- Proportion of population effects agreeing in direction with the original (vote counting)
- Confidence interval (CI): original effect in replication 95% CI
- Confidence interval (CI): replication effect in original 95% CI
- Prediction interval (CI): replication effect in original 95% PI
- Replication Bayes Factor (BF)
- Small Telescopes
- Consistency of original with replications, $P_{\mbox{orig}}$
- Proportion of population effects agreeing in direction with the original, $\hat{P}_{>0}$
- Meta-analysis and $Q$ statistics

## Vote Counting based on significance or direction

The simplest method is called **vote counting** [@Valentine2011-yq; @Hedges1980-gd]. A replication attempt $\theta_{r}$ is considered to be successful if the result is is statistically significant i.e., $p_{\theta_{r}} \leq \alpha$ and has the same direction as the original study $\theta_{0}$. Similarly, we can count the number of replications with the *same sign* as the original study.  This method has the following <span style="color: green;">pros</span> and <span style="color: red;">cons</span>.

:::{.pros}
- Easy to understand, communicate and compute
:::

:::{.cons}
- Does not consider the size of the effect
- Depends on the power of $\theta_{rep}$
:::

Here is an example which illustrates the vote counting method  First, we simulate a replication:

```{r}
#| echo: true
## original study
n_orig <- 30
theta_orig <- theta_from_z(2, n_orig)

orig <- data.frame(
  yi = theta_orig,
  vi = 4/(n_orig*2)
)

orig$sei <- sqrt(orig$vi)
orig <- summary_es(orig)

orig

## replications

R <- 10
reps <- sim_studies(R = R, 
                    mu = theta_orig, 
                    tau2 = 0, 
                    n_orig)

reps <- summary_es(reps)

head(reps)
```

Then, we compute the proportions of replication studies that are statistically significant:

```{r}
#| echo: true
mean(reps$pval <= 0.05)
```

And, the proportions of replication studies with the same sign as the original:

```{r}
#| echo: true
mean(sign(orig$yi) == sign(reps$yi))
```

At this stage, we could also perform some statistical tests in order to complement the vote counting approach. See @Bushman2009-zv and @Hedges1980-gd for vote-counting methods in meta-analysis.

Here is an extreme example:

```{r}
#| include: false
z_orig <- 2
n_orig <- 30

# z = g1/sqrt(4/(n_orig*2))
theta_orig <- theta_from_z(z_orig, n_orig)

2 * pnorm(z_orig, lower.tail = FALSE)

n_rep <- 350
theta_rep <- 0.15

z_rep <- theta_rep/sqrt(4/(n_rep * 2))
2 * pnorm(z_rep, lower.tail = FALSE)
```

Imagine an original experiment with $n_{0} = 30$ and $\hat \theta_{0} = 0.5$ that is statistically significant $p \approx 0.045$. Now, consider a direct replication study (thus assume $\tau^2 = 0$) with $n_{1} = 350$ which found $\hat \theta_{1} = 0.15$, which is statistically significant at $p\approx 0.047$.

```{r, echo = FALSE}
se_rep <- sqrt(4/(n_rep*2))
curve(dnorm(x, theta_rep, se_rep), -0.2, 0.7, 
      main = "Can be considered as a successful replication?",
      ylab = "Density", 
      xlab = latex2exp::TeX("$\\theta$"))
abline(v = theta_rep + qnorm(c(0.025, 0.975)) * se_rep, lty = "dashed")
points(theta_orig, 0, pch = 19, cex = 2)
points(theta_rep, 0, pch = 19, cex = 2, col = "firebrick")
text(theta_orig, 1, labels = "Original Study")
text(theta_rep, 1, labels = "Replication Study")
```

The most problematic aspect of using only the information about the sign of the effect (or the p value) is that *we lose information* about the **size of the effect** and the **precision**.

## Confidence/prediction interval methods

An advantage of confidence/prediction interval methods is that they take into account the size of the effect and the precision. The confidence interval represents the sampling uncertainty around the estimated value. It is interpreted as the proportion of confidence intervals (under hypothetical repetition of the same sampling procedure) which contain the true value.

The confidence interval around an estimated effect $\theta_r$ can be written as follows:

$$
95\%\;\mbox{CI} = \hat\theta_r \pm t_{\alpha}\sqrt{\sigma^2_{r}}
$$

Here, $t_{\alpha}$ is the critical test statistic for significance level $\alpha$. This version of the confidence interval is symmetric around the estimated value and its width is a function of the estimation precision.

```{r}
alpha <- 0.05
n <- c(10, 50, 100, 500)
theta <- rep(0.3, length(n))
se <- sqrt(1/n + 1/n)
tc <- abs(qnorm(alpha/2))
lb <- theta - se * tc
ub <- theta + se * tc

plot(n, theta, ylim = c(min(lb), max(ub)), type = "b", pch = 19,
     xlab = "Sample Size",
     ylab = latex2exp::TeX("$\\theta\\;_{r}$"))
lines(n, ub, lty = "dashed")
lines(n, lb, lty = "dashed")
```

### Replication effect within the original CI

We can check whether a replication effect falls within the original study's confidence interval.  That is, whether the following inequalities are satisfied.

$$
\theta_{0} - t_{\alpha} \sqrt{\sigma^2_{0}} < \theta_{r} < \theta_{0} + t_{\alpha} \sqrt{\sigma^2_{0}}
$$

The <span style="color: green;">pros</span> and <span style="color: red;">cons</span> of this approach include:

:::{.pros}
- Takes into account the size of the effect and the precision of $\theta_{0}$
:::

:::{.cons}
- Assumes the original study provides a reliable estimate
- Has no extension for *many-to-one* designs
- Original studies with limited precision will tend to lead to higher success rates (i.e., original studies with wide intervals will be easier to replicate)
:::

```{r}
z_orig <- 2
n_orig <- 30
theta_orig <- theta_from_z(z_orig, n_orig)
n_rep <- 350
theta_rep <- 0.15
se_orig <- sqrt(4 / (2 * n_orig))
ci_orig <- theta_orig + qnorm(c(0.025, 0.975)) * se_orig
curve(dnorm(x, theta_orig, se_orig), 
      -1, 2, 
      ylab = "Density", 
      xlab = latex2exp::TeX("$\\theta$"))
abline(v = ci_orig, lty = "dashed")
points(theta_orig, 0, pch = 19, cex = 2)
points(theta_rep, 0, pch = 19, cex = 2, col = "firebrick")
legend("topleft", 
       legend = c("Original", "Replication"), 
       fill = c("black", "firebrick"))
```

Here is an illustration of the last <span style="color: red;">con</span> of this method (i.e., that original studies with wide intervals will be easier to replicate).

```{r}
#| echo: false
theta_orig <- 0.5

ci_plot <- data.frame(
  yi = 0.5,
  ni = c(200, 50, 10)
)

ci_plot$sei <- sqrt(4 / (2 * ci_plot$ni))

ci_plot$lower <- ci_plot$yi + qnorm(0.025)*ci_plot$sei
ci_plot$upper <- ci_plot$yi + qnorm(0.975)*ci_plot$sei

curve(dnorm(x, ci_plot$yi[1], ci_plot$sei[1]), -1, 2, 
      ylab = "Density", 
      xlab = latex2exp::TeX("$\\theta$"), col = 1, lwd = 2)

curve(dnorm(x, ci_plot$yi[2], ci_plot$sei[2]), -1, 2, 
      ylab = "Density", 
      xlab = latex2exp::TeX("$\\theta$"), col = 2, lwd = 2,
      add = TRUE)

curve(dnorm(x, ci_plot$yi[3], ci_plot$sei[3]), -1, 2, 
      ylab = "Density", 
      xlab = latex2exp::TeX("$\\theta$"), col = 3, lwd = 2,
      add = TRUE)

legend("topleft", 
       legend = c("N = 10", "N = 50", "N = 200"), 
       fill = 3:1)

abline(v = ci_plot$lower, lty = "dashed", col = 1:3)
abline(v = ci_plot$upper, lty = "dashed", col = 1:3)

points(theta_orig, 0, pch = 19, cex = 2)
points(0.1, 0, pch = 19, cex = 2, col = "firebrick")
legend("topright", 
       legend = c("Original", "Replication"), 
       fill = c("black", "firebrick"))
```

### Original study within replication CI

A similar approach is checking whether the original effect size is contained within the replication confidence interval, i.e., whether the following inequalities are satisfied.

$$
\theta_{r} - t_{\alpha} \sqrt{\sigma_{r}^2} < \theta_{0} < \theta_{r} + t_{\alpha} \sqrt{\sigma_{r}^2}
$$

This method has the same <span style="color: green;">pros</span> and <span style="color: red;">cons</span> as the previous approach. One advantage of this method is that replication studies are usually more precise (because they tend to have higher sample sizes).  As a result, the parameter and the %CI tends to be more reliable.

## Prediction interval PI

The CI methods described so far only take into account uncertainty from either the original or the replication study. The prediction interval PI allows us to account for uncertainty regarding a future observation (in this case, a future study).

If we compute the difference between the original and the replication study, the sampling distribution of the difference represents the range of values that are expected *in the absence of other sources of variability*. In other words, assuming that $\tau^2 = 0$.

If the original and replication studies come from the same population, the sampling distribution of the difference is centered on zero with a certain standard error $\theta_{0} - \theta_{r} \sim \mathcal{N}\left(0, \sqrt{\sigma^2_{\hat \theta_{0} - \hat \theta_{r}}} \right)$ (the subscript $0$ is to indicate that the effect is expected to be sampled from the same population as $\theta_{0}$)

$$
\hat \theta_{0} \pm t_{\alpha} \sqrt{\sigma^2_{\theta_{0} - \theta_{r}}}
$$

The new standard error is the sum (assuming that the two studies are independent) of the two standard errors. Thus, we are taking into account the estimation uncertainty of the original and replication studies.

```{r}
#| echo: true
set.seed(2025)

o1 <- rnorm(50, 0.5, 1) # group 1
o2 <- rnorm(50, 0, 1) # group 2
od <- mean(o1) - mean(o2) # effect size
se_o <- sqrt(var(o1)/50 + var(o2)/50) # standard error of the difference

n_r <- 100 # sample size replication

se_o_r <- sqrt(se_o^2 + (var(o1)/100 + var(o2)/100))

od + qnorm(c(0.025, 0.975)) * se_o_r
```

```{r}
#| code-fold: true
#| out-width: 70%

par(mar = c(4, 4, 0.1, 0.1))  
curve(dnorm(x, od, se_o_r), od - se_o_r*4, od + se_o_r*4, lwd = 2, xlab = latex2exp::TeX("$\\theta$"), ylab = "Density")
abline(v = od + qnorm(c(0.025, 0.975)) * se_o_r, lty = "dashed")
```

The <span style="color: green;">pros</span> and <span style="color: red;">cons</span> of this method include:

:::{.pros}
- Takes into account uncertainty in both studies
- Allows us to plan a replication using the standard deviation of the original study and the expected sample size of the replication study
:::

:::{.cons}
- Low precision in original studies leads to wider PI. For a replication study, it is difficult to fall outside the PI
- Mainly useful for *one-to-one* replication designs
:::

## Mathur & VanderWeele [-@Mathur2020-nw] $p_{orig}$

Mathur & VanderWeele [-@Mathur2020-nw] proposed a new method based on the prediction interval to calculate a p value $p_{orig}$ representing the probability that $\theta_{0}$ is consistent with the replications. This method is suited for *many-to-one* replication designs. Formally:

$$
p_{orig} = 2 \left[ 1 - \Phi \left( \frac{|\hat \theta_{0} - \hat \mu_{\theta_{R}}|}{\sqrt{\hat \tau^2 + \sigma^2_{\theta_{0}} + \hat{SE}^2_{\hat \mu_{\theta_{R}}}}} \right) \right]
$$

- $\hat \mu_{\theta_{R}}$ is the pooled (i.e., meta-analytic) estimation of the $k$ replications
- $\tau^2$ is the variance of effects across replications (i.e., heterogeneity)

$p_{orig}$ is interpreted as the probability that $\theta_{0}$ is at least as extreme as what is observed. A low $p_{orig}$ suggests that the original study is inconsistent with replications.  This method as three key <span style="color: green;">pros</span>.

:::{.pros}
- Suited for *many-to-one* designs
- Takes into account all sources of uncertainty
- Provides a p-value (to gauge consistency of the original study with replications)
:::

The code for this approach is implemented in the `Replicate` and `MetaUtility` R packages:

```{r}
tau2 <- 0.05
theta_rep <- 0.2
theta_orig <- 0.7

n_orig <- 30
n_rep <- 100
k <- 20

replications <- sim_studies(k, theta_rep, tau2, n_rep, n_rep)
original <- sim_studies(1, theta_orig, 0, n_orig, n_orig)

fit_rep <- metafor::rma(yi, vi, data = replications) # random-effects meta-analysis

Replicate::p_orig(original$yi, original$vi, fit_rep$b[[1]], fit_rep$tau2, fit_rep$se^2)
```

### Simulation

```{r}
# standard errors assuming same n and variance 1
se_orig <- sqrt(4/(n_orig * 2))
se_rep <- sqrt(4/(n_rep * 2))
se_theta_rep <- sqrt(1/((1/(se_rep^2 + tau2)) * k)) # standard error of the random-effects estimate

sep <- sqrt(tau2 + se_orig^2 + se_theta_rep^2) # z of p-orig denominator

curve(dnorm(x, theta_rep, sep), theta_rep - 4*sep, theta_rep + 4*sep, ylab = "Density", xlab = latex2exp::TeX("\\theta"))
points(theta_orig, 0.02, pch = 19, cex = 2)
abline(v = qnorm(c(0.025, 0.975), theta_rep, sep), lty = "dashed", col = "firebrick")
```

```{r, include = FALSE}
# checking the calculation above with metafor
# simulating a fixed dataset

yi <- rep(theta_rep, k)
vi <- rep(se_rep^2, k)

fit <- metafor::rma(yi, vi, tau2 = tau2) # fixing the tau value

c(se_theta_rep, fit$se)
```

### Mathur & VanderWeele [-@Mathur2020-nw] $\hat P_{> 0}$

Another related metric is the $\hat P_{> 0}$, which represents the proportion of replications having the same direction as the original effect. Note: before computing the proportions, they adjust the estimated $\theta_{r}$ ($r = 1, \dots, R$) with a shrinkage factor (to account for regression to the mean):

$$
\tilde{\theta}_{r} = (\theta_{r} - \mu_{\theta_{r}}) \sqrt{\frac{\hat \tau^2}{\hat \tau^2 + \sigma^2_{r}}}
$$

Here $\sigma^2_r$ is the sampling variance for the replication effect size $r$. This method is  similar to vote counting; but, we are adjusting the effects with a shrinkage factor based on $\tau^2$.

```{r}
#| code-fold: false
# compute calibrated estimation for the replications
# use restricted maximum likelihood to estimate tau2 under the hood
theta_sh <- MetaUtility::calib_ests(replications$yi, replications$sei, method = "REML")
mean(theta_sh > 0)
```

The authors suggest a bootstrapping approach for inference on $\hat P_{> 0}$

```{r}
#| code-fold: false
#| cache: true
nboot <- 1e3
theta_boot <- matrix(0, nrow = nboot, ncol = k)

for(i in 1:nboot){
  idx <- sample(1:nrow(replications), nrow(replications), replace = TRUE)
  replications_boot <- replications[idx, ]
  theta_cal <- MetaUtility::calib_ests(replications_boot$yi, 
                                       replications_boot$sei, 
                                       method = "REML")
  theta_boot[i, ] <- theta_cal
}

# calculate
p_greater_boot <- apply(theta_boot, 1, function(x) mean(x > 0))
```

```{r}
#| echo: false
hist(p_greater_boot,
     xlab = latex2exp::TeX("$\\hat{P}_{> 0}$"),
     breaks = 10,
     col = "#FA9494",
     main = latex2exp::TeX("Bootstrapped $\\hat{P}_{> 0}$"))
abline(v = quantile(p_greater_boot, c(0.025, 0.975)))
```

### Mathur & VanderWeele [-@Mathur2020-nw] $\hat P_{\gtrless q*}$

Instead of using 0 as threshold, we can use a practically meaningful effect size, considered to be low but different from 0. $\hat P_{\gtrless q*}$ is the proportion of (calibrated) replications greater or lower than the $q*$ value. This framework is similar to equivalence and minimum effect size testing [@Lakens2018-ri].

```{r}
#| code-fold: false
q <- 0.2 # minimum non zero effect

fit <- metafor::rma(yi, vi, data = replications)

# see ?MetaUtility::prop_stronger
MetaUtility::prop_stronger(q = q,
                           M = fit$b[[1]],
                           t2 = fit$tau2,
                           tail = "above",
                           estimate.method = "calibrated",
                           ci.method = "calibrated",
                           dat = replications,
                           yi.name = "yi",
                           vi.name = "vi")
```

## Meta-analytic methods

Another approach is to combine the original and replication results using a meta-analysis model. This is applicable to both *one-to-one* and *many-to-one* designs. In this case, we can test whether the pooled estimate is different from 0 (or another practically relevant value).  The meta-analytic approach has the following <span style="color: green;">pros</span> and <span style="color: red;">cons</span>.

:::{.pros}
- Uses all the available information, especially when fitting a random-effects model
- Takes into account the precision of individual studies (e.g., via inverse-variance weighting)
:::

:::{.cons}
- Does not take into account publication bias
- For *one-to-one* designs, only a fixed-effects model can be used
:::

### Fixed-effects Model

```{r}
# fixed-effects
fit_fixed <- rma(yi, vi, method = "FE")
summary(fit_fixed)
```

### Random-Effects model

```{r}
# fixed-effects
fit_random <- rma(yi, vi, method = "REML")
summary(fit_random)
```

### Pooling replications

The previous approach can also incorporate the combination/pooling of replications into a single effect (then comparing the original study and the combined/pooled replication study).

This is similar to using the CI or PI approaches; but, the replication effect is likely to be more precise, since we are pooling multiple replication studies.

### The Q Statistic

An interesting proposal involves the use of the Q statistic [@Hedges2019-pr; @Hedges2019-ar; @Hedges2021-of; @Schauer2022-mj; @Schauer2021-ja; @Schauer2020-tw; @Hedges2019-ry], which is commonly applied in meta-analysis to assess the presence of heterogeneity. Formally:

$$
Q = \sum_{r = 0}^{R} \frac{(\theta_r - \bar \theta_R)^2}{\sigma^2_r}
$$

where $\bar \theta_R$ is the inverse-variance weighted average (e.g., from a fixed-effect model). The $Q$ statistic is a weighted sum of squares. Under the null hypothesis where all studies are equal $\theta_0 = \theta_1, ... = \theta_R$, the $Q$ statistic has a $\chi^2$ distribution with $R - 1$ degrees of freedom. Under the alternative hypothesis the distribution is a non-central $\chi^2$ with non centrality parameter $\lambda$. The expected value of $Q$ is $E(Q) = \nu + \lambda$, where $\nu$ are the degrees of freedom.

Hedges & Schauer proposed the use of $Q$ to evaluate the consistency of a series of replications:

- In case of a **replication**, $\lambda = 0$ because $\theta_0 = \theta_1, ... = \theta_R$.
- In case of an **extension**, $\lambda < \lambda_0$ where $\lambda_0$ is the maximum value considered equivalent to zero. In other words, $\lambda_0$ is the threshold above which the variability is too high to be considered negligible for a replication [in the sense of @Machery2020-sv].

This approach is testing the consistency (i.e., homogeneity) of replications. A successful replication should minimize the heterogeneity and the presence of a significant $Q$ statistic would suggest a failed replication attempt. @Hedges2019-pr and @Mathur2019-vh discuss this approach in detail.  Their discussion addresses various issues surrounding the proper use of this method, including:

- variations of the apporoach, which cover different replication setups (burden of proof on replicating vs non-replicating, many-to-one and one-to-one, etc.)
- the choice (and interpretation) of the $\lambda_0$ parameter
- evaluating the power and statistical properties under different replication scenarios

When evaluating a replication, we can use the `Qrep()` function which calculates the p-value based on the Q sampling distribution.

```{r}
#| echo: false
#| output: asis
filor::print_fun(funs$Qrep)
```

```{r}
#| echo: false
#| message: false
dat <- sim_studies(100, 0.5, 0.1, 50, 50)
Qres <- Qrep(dat$yi, dat$vi)
```

```{r}
Qres <- Qrep(dat$yi, dat$vi)
```

```{r}
plot.Qrep(Qres)
```

#### Q Statistic for an extension study

In the case of an extension of an intial experiment, we will need to set $\lambda_0$ to a meaningful value. The critical $Q$ is no longer evaluated with a central $\chi^2$ but is now a non-central $\chi^2$ with non-centrality parameter $\lambda_0$.

@Hedges2019-ry provide different strategies for choosing $\lambda_0$. They found that (under some mild assumptions), $\lambda = (R - 1) \frac{\tau^2}{\tilde{v}}$ seems to be a suitable choice.

Similarly to a meta-analysis, in a many-to-one replication design we have mainly two sources of variability: the true heterogeneity $\tau^2$ and the within-studies veriability $\sigma_i^2$. The sum of these two quantities is the total variability and the proportion of total variability due to heterogeneity is a relative index of (in)consistency among effects. This proportion is called $I^2$ [@Higgins2002-fh] in the meta-analysis literature is calculated as: 
$$
I^2 = 100\% \times \frac{\hat{\tau}^2}{\hat{\tau}^2 + \tilde{v}}
$$

Where $\tilde{v}$ can be considered as a typical within-study variance value.

$$
\tilde{v} = \frac{(k-1) \sum w_i}{(\sum w_i)^2 - \sum w_i^2},
$$

Having introduced the $I^2$ statistic, we can derive a $\lambda_0$ based on $I^2$. @Schmidt2014-kw proposed that when $\tilde{v}$ is at least 75% of the total variance $\tilde{v} + \tau^2$,  $\tau^2$ can be considered to be negligible. This corresponds to a $I^2 = 25%$ and a ratio $\frac{\tau^2}{\tilde{v}} = 1/3$.  Hence,  $\lambda_0 = \frac{(R - 1)}{3}$ can be considered to be a negligible amount of heterogeneity.

```{r}
k <- 100
dat <- sim_studies(k, 0.5, 0, 50, 50)
Qrep(dat$yi, dat$vi, lambda0 = (k - 1)/3)
```

## Small Telescopes [@Simonsohn2015-kg]

The idea is simple but quite powerful and insightful. An original study $\theta_{0}$ is conducted estimating a certain effect size with a certain sample size. This study has a certain statistical power level assuming a plausible effect size. Then we estimate the effect size that would have a certain low power level (e.g., 33%, $d_{33\%}$). Then we run the replication study $\theta_r$ testing if the replication effect size is lower (one-sided test) than $d_{33\%}$. If upper bound of the confidence interval of the replication study is lower this means that the original study is probably seriously underpowered suggesting evidence of non-replication.

As a practical example, let's assume that an original study found an effect of $\theta_{0} = 0.7$ on a two-sample design with $n = 20$ per group. We define a threshold as the effect size that is associated with a certain low statistical power level e.g., $33\%$ given the sample size i.e. $d_{33\%} = 0.5$. The replication study found an effect of $\theta_{r} = 0.2$ with $n = 100$ subjects.

If the $\theta_{r}$ is statistically significantly lower than $d_{33\%} = 0.5$ we conclude that the effect is probably so small that could not have been detected by the original study.
    
We can use the custom `small_telescope()` function on simulated data:

```{r}
#| echo: false
#| output: asis
filor::print_fun(funs$small_telescope)
```

```{r}
#| echo: true
#| code-fold: false

set.seed(2025)

d <- 0.2 # real effect

# original study
or_n <- 20
or_d <- 0.7
or_se <- sqrt(1/20 + 1/20)
d_small <- pwr::pwr.t.test(or_n, power = 0.33)$d

# replication
rep_n <- 100 # sample size of replication study
g0 <- rnorm(rep_n, 0, 1)
g1 <- rnorm(rep_n, d, 1)

rep_d <- mean(g1) - mean(g0)
rep_se <- sqrt(var(g1)/rep_n + var(g0)/rep_n)
```

Here we are using the `pwr::pwr.t.test()` to compute the effect size $\theta_{small}$ (in code `d`) associated with 33% power.

```{r}
small_telescope(or_d, or_se, rep_d, rep_se, d_small, ci = 0.95)
```

And a plot:

```{r}
#| echo: false
#| message: false
quiet(small_telescope(or_d, or_se, rep_d, rep_se, d_small, ci = 0.95)) |> 
  telescope_plot()
```

## Replication Bayes factor

@Verhagen2014-tx proposed a method to estimate the evidence of a *replication* study. The core topics to understand the method are:

- Bayesian hypothesis testing using the Bayes Factor [see, @Rouder2009-jh]
- Bayes Factor using the Savage-Dickey density ratio [SDR, @Wagenmakers2010-fj]

### Bayesian inference

Bayesian inference is a statistical procedure where **prior beliefs** about a phenomenon are combined, using Bayes's theorem, with **evidence from data** to obtain  **posterior beliefs**. 

If the researcher can express their prior beliefs in probabilistic terms, then evidence from the experiment is taken into account (via Bayesian conditionalization), thus increasing or decreasing the plausibility of prior beliefs.

Let's consider a simple example. We need to evaluate the fairness of a coin. The parameter is the probability $\pi$ of success (i.e., heads). We have our prior belief about the coin (e.g., fair but with some uncertainty). We toss the coin $k$ times, and we observe $x$ heads. How should we update our prior beliefs?  We apply Bayes's Theorem, which, in this case, is:

$$
p(\pi|D) = \frac{p(D|\pi) \; p(\pi)}{p(D)}
$$

Here, $\pi$ is our parameter and $D$ is our data. $p(\pi|D)$ is the posterior distribution, which is the normalized product of $p(D|\pi)$ --- which is called the **likelihood** of the hypothesis $\pi$, relative to the data $D$ ---  and the prior $p(\pi)$. $p(D)$ is the prior probability of the data (aka marginal likelihood) and is necessary for the posterior to be a (proper) probability distribution.

We can "read" the formula as: *The probability of the parameter given the data is the product of the probability of the data given the parameter and the prior probability of the parameter*.

Let's express our **prior** belief in probabilistic terms:

```{r}
#| echo: false

k = 50 # number of trials
x = 0:k # theta
p = x/k # p
d = 40 # observed data

prior = dbeta(p, 15, 15) 
likelihood = dbinom(d, k, p)
posterior = (prior * likelihood) / sum(prior * likelihood)

prior = prior / sum(prior)
likelihood = likelihood/ sum(likelihood)

ylim = c(0, max(c(prior, likelihood, posterior)))

plot(p, prior, 
     type = "l", 
     ylim = ylim, 
     lwd = 2, 
     xlab = latex2exp::TeX("$\\pi$"), 
     ylab = "Probability")
```

Now we collect data and we observe $x = 40$ tails out of $k = 50$ trials thus $\hat{\pi} = 0.8$ and compute the *likelihood*:

```{r}
#| echo: false

plot(p, prior, 
     type = "l", 
     ylim = ylim, 
     lwd = 2, 
     xlab = latex2exp::TeX("$\\pi$"), 
     ylab = "Probability", col = "darkgreen")
lines(p, likelihood, col = "black", lwd = 2)
points(d/k, 0, pch = 19, col = "firebrick", cex = 1.5)
```

Finally we combine, using the Bayes rule, **prior** and **likelihood** to obtain the **posterior** distribution:

```{r}
#| echo: false

plot(p, prior, 
     type = "l", 
     ylim = ylim, 
     lwd = 2, 
     xlab = latex2exp::TeX("$\\pi$"), 
     ylab = "Probability")
lines(p, likelihood, col = "black", lwd = 2)
points(d/k, 0, pch = 19, col = "firebrick", cex = 1.5)
lines(p, posterior, lwd = 2, col = "dodgerblue")
legend("topleft", legend = c("Prior", "Likelihood", "Posterior"), fill = c("darkgreen", "black", "dodgerblue"))
points(d/k, 0, pch = 19, col = "firebrick", cex = 1.5)
```

The **Bayes Factor** (BF) --- also called the **likelihood ratio**, for obvious reasons --- is a measure of the relative support that the evidence provides for two competing hypotheses, $H_0$ and $H_1$ (~ $\pi$ in our previous example).  It plays a key role in the following *odds form* of Bayes's theorem.

$$
\frac{p(H_0|D)}{p(H_1|D)} = \frac{p(D|H_0)}{p(D|H_1)} \times \frac{p(H_0)}{p(H_1)}
$$

The ratio of the priors $\frac{p(H_0)}{p(H_1)}$ is called the **prior odds** of the hypotheses; and, the ratio of the poosteriors $\frac{p(H_0| D)}{p(H_1 | D)}$ is called the **posterior odds** of the hypotheses. Thus, the above (odds form) of Bayes's Theorem can be paraphrased as follows

$$
\text{posterior odds} = \text{Bayes Factor} \times \text{prior odds}
$$
In this way, it can be seen that the Bayes Factor captures the relative empirical support that the evidence $D$ provides in favor of $H_0$ over $H_1$.  If the Bayes Factor is greater than 1, then $D$ favors $H_0$ over $H_1$.  If the BF is less than one, then  $D$ favors $H_1$ over $H_0$.  If the BF is 1, then $D$ is neutral regarding $H_0$ vs $H_1$ [@Royall1997-ye].  For this reason, the BF is also sometimes called the **weight of evidence** in favor of $H_0$ (*vs* $H_1$) [@Good1950-go].

### Calculating the Bayes Factor using the SDR

The Savage-Dickey density ratio (SDR) is a convenient shortcut to calculate the Bayes Factor [see @Wagenmakers2010-fj; @Marin2010-oh] in a situation where the null hypothesis ($H_0$) is a single parameter value (e.g., $H_0: \theta = 0$). With SDR the Bayes Factor can be calculated as the ratio between the prior and posterior density distribution under the alternative hypothesis $H_1$.

$$
BF_{01} = \frac{p(D|H_0)}{p(D|H_1)} \approx \frac{p(\pi = x|D, H_1)}{p(\pi = x | H_1)}
$$

Where $\pi$ is the parameter of interest and $x$ is the null value under $H_0$ (e.g., 0). and $D$ are the data. 

When there are multiple parameters in play and the null hypothesis is expressed in terms of only one of them, this representation is valid only under the assumption that the prior for any of the other parameters under the null is the same as the conditional prior for those parameters under the alternative. 

Following the previous example $H_0: \pi = 0.5$. Under $H_1$ we use a vague prior by setting $\pi \sim Beta(1, 1)$.

Say we flipped the coin 20 times and we found that $\hat \pi = 0.75$.

```{r}
#| echo: false

curve(dbeta(x, 1, 1), 
      lwd = 2, 
      col = "darkgreen", 
      xlab = latex2exp::TeX("$\\pi$"),
      ylab = "Density",
      main = latex2exp::TeX("Prior distribution for $\\pi$"),
      cex.lab = 1.3,
      cex.main = 1.3,
      cex.axis = 1.3)
```

The ratio between the two black dots is the Bayes Factor.

```{r}
#| echo: false
par(mfrow = c(1,2))

ps <- seq(0, 1, 0.01)

# theta = 0.75
k <- 20
x <- 15
theta <- x/k
prior <- rep(1/length(ps), length(ps))
likelihood <- dbinom(x, k, ps)
posterior <- (prior * likelihood) / sum(prior * likelihood)

title <- latex2exp::TeX("$x = 15$, $k = 20$, $\\hat{\\pi} = 0.75$, $\\hat{\\pi_0} = 0.5$")
plot(ps, prior, ylim = c(0, 0.05), type = "l", col = "darkgreen", lwd = 3,
     main = title,
     xlab = latex2exp::TeX("\\pi"),
     ylab = "Density")
abline(v = 0.75, lty = "dashed", col = "grey")
lines(ps, posterior, col = "#619CFF", lwd = 3)
legend("topleft",
       legend = c("Prior","Posterior"),
       pch = 15,
       col = c("darkgreen","#619CFF"))
text(0.75+0.05, 0.015, latex2exp::TeX("$\\hat{\\pi}$"))
#segments(0.5, 0, 0.5, posterior[ps == 0.5])
points(0.5, posterior[ps == 0.5], cex = 2, pch = 19)
points(0.5, prior[ps == 0.5], cex = 2, pch = 19)
points(0.75, 0, cex = 1, pch = 19, col = "firebrick")

# theta = 0.5
k <- 20
x <- 10
theta <- x/k

prior <- rep(1/length(ps), length(ps))
likelihood <- dbinom(x, k, ps)
posterior <- (prior * likelihood) / sum(prior * likelihood)

title <- latex2exp::TeX("$x = 10$, $k = 20$, $\\hat{\\pi} = 0.5$, $\\hat{\\pi_0} = 0.5$")
plot(ps, prior, ylim = c(0, 0.05), type = "l", col = "darkgreen", lwd = 3,
     main = title,
     xlab = latex2exp::TeX("\\pi"),
     ylab = "Density")
abline(v = 0.5, lty = "dashed", col = "grey")
lines(ps, posterior, col = "#619CFF", lwd = 3)
legend("topleft",
       legend = c("Prior","Posterior"),
       pch = 15,
       col = c("darkgreen","#619CFF"))
text(0.5+0.05, 0.015, latex2exp::TeX("$\\hat{\\pi}$"))
points(0.5, posterior[ps == 0.5], cex = 2, pch = 19)
points(0.5, prior[ps == 0.5], cex = 2, pch = 19)
points(0.5, 0, cex = 1, pch = 19, col = "firebrick")
```

### @Verhagen2014-tx model^[see also @Ly2019-ow for an improvement]

The idea is using the posterior distribution of the original study as prior for a Bayesian hypothesis testing where:

- $H_0: \theta_{r} = 0$ meaning that there is no effect in the replication study
- $H_1: \theta_{r} \neq 0$ and in particular is distributed as $\delta \sim \mathcal{N}(\theta_{0}, \sigma^2_{0})$ where $\theta_{0}$ and $\sigma^2_{0}$ are the mean and standard error of the original study

If $H_0$ is more likely after seeing the data, there is evidence against the replication (i.e., $BF_{r0} > 1$) otherwise there is evidence for a successful replication ($BF_{r1} > 1$).

::: {.callout-warning}
**Disclaimer:** The actual implementation of @Verhagen2014-tx is different (they use the $t$ statistics). We implemented a similar model using a Bayesian linear model with `rstanarm`.
:::

Let's assume that the original study ($n = 30$) estimates a $\theta_{0} = 0.4$ and a standard error $\sigma^2/n$.

```{r}
#| echo: true
#| code-fold: false
# original study
n <- 30
yorig <- 0.4
se <- sqrt(1/n)
```

::: {.callout-note}
The assumption of @Verhagen2014-tx is that the original study performed a Bayesian analysis with a flat (unform) prior. Thus, the confidence interval is approximately the same as the Bayesian credible interval.
:::

For this reason, the posterior distribution of the original study can be approximated as:

```{r}
#| echo: false
curve(dnorm(x, yorig, se), 
      from = yorig - se*4,
      to = yorig + se*4,
      main = "Original Study Posterior Distribution",
      col = "firebrick3",
      lwd = 3,
      xlab = latex2exp::TeX("$\\delta$"),
      ylab = "Density")
```

Let's imagine that a new study tried to replicate the original one. They collected $n = 100$ participants with the same protocol and found an effect of $y_{rep} = 0.1$.

```{r}
nrep <- 100
yrep <- MASS::mvrnorm(nrep, mu = 0.1, Sigma = 1, empirical = TRUE)[, 1]
dat <- data.frame(y = yrep)
hist(yrep, main = "Replication Study (n1 = 100)", xlab = latex2exp::TeX("$\\theta_{r}$"))
abline(v = mean(yrep), lwd = 2, col = "firebrick")
```

We can analyze these data with an *intercept-only regression model* setting as prior the posterior distribution of the original study:

```{r}
#| echo: true

# setting the prior on the intercept parameter
prior <- rstanarm::normal(location = yorig,
                          scale = se)

# fitting the bayesian linear regression
fit <- stan_glm(y ~ 1, 
                data = dat, 
                prior_intercept = prior,
                refresh = FALSE)

summary(fit)
```

We can use the `bayestestR::bayesfactor_pointnull()` to calculate the BF using the Savage-Dickey density ratio.

```{r}
bf <- bayestestR::bayesfactor_pointnull(fit, null = 0)
print(bf)
plot(bf)
```

You can also use the `bf_replication()` function:

```{r}
#| echo: false
#| output: asis
filor::print_fun(funs$bf_replication)
```

```{r}
#| code-fold: false
#| eval: false
bf_replication(mu_original = yorig, se_original = se, replication = yrep)
```

A better custom plot:

```{r}
bfplot <- data.frame(
  prior = rnorm(1e5, yorig, se),
  posterior = rnorm(1e5, fit$coefficients, fit$ses)
)
 
ggplot() +
  stat_function(geom = "line", 
                aes(color = "Original Study (Prior)"),
                linewidth = 1,
                alpha = 0.3,
                fun = dnorm, args = list(mean = yorig, sd = se)) +
  stat_function(geom = "line",
                linewidth = 1,
                aes(color = "Replication Study (Posterior)"),
                fun = dnorm, args = list(mean = fit$coefficients, sd = fit$ses)) +
  xlim(c(-0.5, 1.2)) +
  geom_point(aes(x = c(0, 0), y = c(dnorm(0, yorig, sd = se),
                                    dnorm(0, fit$coefficients, sd = fit$ses))),
             size = 3) +
  xlab(latex2exp::TeX("\\delta")) +
  ylab("Density") +
  theme(legend.position = "bottom",
        legend.title = element_blank())
```

## Conclusions

We hope to have provided a useful overview of statistical approaches to different definitions of replication. The meta-analytic and $Q$ methods are more focused on accumulating evidence combining original and replication studies. The CI and PI methods are more interested in comparing the original and replication(s) study. The BF method constitutes a kind of compromise between these perspectives, by evaluating the weight of evidence in favor of the null --- from the point of view of the posterior distribution of the replication study. 

## References {.unnumbered}
