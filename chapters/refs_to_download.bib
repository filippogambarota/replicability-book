@ARTICLE{Shi2019-pj,
  title = {The trim-and-fill method for publication bias: practical guidelines
  and recommendations based on a large database of meta-analyses: practical
  guidelines and recommendations based on a large database of meta-analyses},
  author = {Shi, Linyu and Lin, Lifeng},
  journaltitle = {Medicine},
  publisher = {Ovid Technologies (Wolters Kluwer Health)},
  volume = {98},
  issue = {23},
  pages = {e15987},
  date = {2019-06},
  doi = {10.1097/MD.0000000000015987},
  pmc = {PMC6571372},
  pmid = {31169736},
  issn = {0025-7974,1536-5964},
  abstract = {Publication bias is a type of systematic error when synthesizing
  evidence that cannot represent the underlying truth. Clinical studies with
  favorable results are more likely published and thus exaggerate the
  synthesized evidence in meta-analyses. The trim-and-fill method is a popular
  tool to detect and adjust for publication bias. Simulation studies have been
  performed to assess this method, but they may not fully represent realistic
  settings about publication bias. Based on real-world meta-analyses, this
  article provides practical guidelines and recommendations for using the
  trim-and-fill method. We used a worked illustrative example to demonstrate the
  idea of the trim-and-fill method, and we reviewed three estimators (R0, L0,
  and Q0) for imputing missing studies. A resampling method was proposed to
  calculate P values for all 3 estimators. We also summarized available
  meta-analysis software programs for implementing the trim-and-fill method.
  Moreover, we applied the method to 29,932 meta-analyses from the Cochrane
  Database of Systematic Reviews, and empirically evaluated its overall
  performance. We carefully explored potential issues occurred in our analysis.
  The estimators L0 and Q0 detected at least one missing study in more
  meta-analyses than R0, while Q0 often imputed more missing studies than L0.
  After adding imputed missing studies, the significance of heterogeneity and
  overall effect sizes changed in many meta-analyses. All estimators generally
  converged fast. However, L0 and Q0 failed to converge in a few meta-analyses
  that contained studies with identical effect sizes. Also, P values produced by
  different estimators could yield different conclusions of publication bias
  significance. Outliers and the pre-specified direction of missing studies
  could have influential impact on the trim-and-fill results. Meta-analysts are
  recommended to perform the trim-and-fill method with great caution when using
  meta-analysis software programs. Some default settings (e.g., the choice of
  estimators and the direction of missing studies) in the programs may not be
  optimal for a certain meta-analysis; they should be determined on a
  case-by-case basis. Sensitivity analyses are encouraged to examine effects of
  different estimators and outlying studies. Also, the trim-and-fill estimator
  should be routinely reported in meta-analyses, because the results depend
  highly on it.},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6571372/},
  language = {en}
}

@ARTICLE{Viechtbauer2005-zt,
  title = {Bias and efficiency of meta-analytic variance estimators in the
  random-effects model},
  author = {Viechtbauer, Wolfgang},
  journaltitle = {Journal of educational and behavioral statistics: a quarterly
  publication sponsored by the American Educational Research Association and the
  American Statistical Association},
  publisher = {American Educational Research Association (AERA)},
  volume = {30},
  issue = {3},
  pages = {261-293},
  date = {2005-09},
  doi = {10.3102/10769986030003261},
  issn = {1076-9986,1935-1054},
  abstract = {The meta-analytic random effects model assumes that the
  variability in effect size estimates drawn from a set of studies can be
  decomposed into two parts: heterogeneity due to random population effects and
  sampling variance. In this context, the usual goal is to estimate the central
  tendency and the amount of heterogeneity in the population effect sizes. The
  amount of heterogeneity in a set of effect sizes has implications regarding
  the interpretation of the meta-analytic findings and often serves as an
  indicator for the presence of potential moderator variables. Five population
  heterogeneity estimators were compared in this article analytically and via
  Monte Carlo simulations with respect to their bias and efficiency.},
  url = {https://scholar.google.com/citations?view_op=view_citation&hl=en&user=J89RXJkAAAAJ&citation_for_view=J89RXJkAAAAJ:qjMakFHDy7sC},
  language = {en}
}

@ARTICLE{Rosenthal1979-yx,
  title = {The file drawer problem and tolerance for null results},
  author = {Rosenthal, Robert},
  journaltitle = {Psychological bulletin},
  publisher = {American Psychological Association (APA)},
  volume = {86},
  issue = {3},
  pages = {638-641},
  date = {1979-05},
  doi = {10.1037/0033-2909.86.3.638},
  issn = {0033-2909,1939-1455},
  url = {https://psycnet.apa.org/record/1979-27602-001},
  language = {en}
}

@ARTICLE{Orwin1983-vu,
  title = {A fail-SafeN for effect size in meta-analysis},
  author = {Orwin, Robert G},
  journaltitle = {Journal of educational statistics},
  publisher = {American Educational Research Association (AERA)},
  volume = {8},
  issue = {2},
  pages = {157-159},
  date = {1983-06},
  doi = {10.3102/10769986008002157},
  issn = {0362-9791,2328-0735},
  abstract = {Rosenthan’s (1979) concept of fail-safe N has thus far been
  applied to probability levels exclusively. This note introduces a fail-safe N
  for effect size.},
  url = {https://journals.sagepub.com/doi/abs/10.3102/10769986008002157},
  language = {en}
}

@ARTICLE{Rosenberg2005-ie,
  title = {The file-drawer problem revisited: a general weighted method for
  calculating fail-safe numbers in meta-analysis},
  author = {Rosenberg, Michael S},
  journaltitle = {Evolution; international journal of organic evolution},
  publisher = {Wiley},
  volume = {59},
  issue = {2},
  pages = {464-468},
  date = {2005-02},
  doi = {10.1111/j.0014-3820.2005.tb01004.x},
  pmid = {15807430},
  issn = {0014-3820,1558-5646},
  abstract = {Quantitative literature reviews such as meta-analysis are becoming
  common in evolutionary biology but may be strongly affected by publication
  biases. Using fail-safe numbers is a quick way to estimate whether publication
  bias is likely to be a problem for a specific study. However, previously
  suggested fail-safe calculations are unweighted and are not based on the
  framework in which most meta-analyses are performed. A general, weighted
  fail-safe calculation, grounded in the meta-analysis framework, applicable to
  both fixed- and random-effects models, is proposed. Recent meta-analyses
  published in Evolution are used for illustration.},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.0014-3820.2005.tb01004.x},
  language = {en}
}

@ARTICLE{Citkowicz2017-ox,
  title = {A parsimonious weight function for modeling publication bias},
  author = {Citkowicz, Martyna and Vevea, Jack L},
  journaltitle = {Psychological methods},
  publisher = {psycnet.apa.org},
  volume = {22},
  issue = {1},
  pages = {28-41},
  date = {2017-03},
  doi = {10.1037/met0000119},
  pmid = {28252998},
  issn = {1082-989X,1939-1463},
  abstract = {Quantitative research literature is often biased because studies
  that fail to find a significant effect (or that demonstrate effects in an
  undesired or unexpected direction) are less likely to be published. This
  phenomenon, termed publication bias, can cause problems when researchers
  attempt to synthesize results using meta-analytic methods. Various techniques
  exist that attempt to estimate and correct meta-analyses for publication bias.
  However, there is no single method that can (a) account for continuous
  moderators by including them within the model, (b) allow for substantial data
  heterogeneity, (c) produce an adjusted mean effect size, (d) include a formal
  test for publication bias, and (e) allow for correction when only a small
  number of effects is included in the analysis. This article describes a method
  that we believe helps fill that gap. The model uses the beta density as a
  weight function that represents the selection process and provides adjusted
  parameter estimates that account for publication bias. Use of the beta density
  allows us to represent selection using fewer parameters than similar models so
  that the proposed model is suitable for meta-analyses that include relatively
  few studies. We explain the model and its rationale, illustrate its use with a
  real data set, and describe the results of a simulation study that shows the
  model's utility. (PsycINFO Database Record},
  url = {https://psycnet.apa.org/journals/met/22/1/28/},
  language = {en}
}

@ARTICLE{McShane2016-bk,
  title = {Adjusting for publication bias in meta-analysis: An evaluation of
  selection methods and some cautionary notes: An evaluation of selection
  methods and some cautionary notes},
  author = {McShane, Blakeley B and Böckenholt, Ulf and Hansen, Karsten T},
  journaltitle = {Perspectives on psychological science: a journal of the
  Association for Psychological Science},
  publisher = {SAGE Publications},
  volume = {11},
  issue = {5},
  pages = {730-749},
  date = {2016-09},
  doi = {10.1177/1745691616662243},
  pmid = {27694467},
  issn = {1745-6916,1745-6924},
  abstract = {We review and evaluate selection methods, a prominent class of
  techniques first proposed by Hedges (1984) that assess and adjust for
  publication bias in meta-analysis, via an extensive simulation study. Our
  simulation covers both restrictive settings as well as more realistic settings
  and proceeds across multiple metrics that assess different aspects of model
  performance. This evaluation is timely in light of two recently proposed
  approaches, the so-called p-curve and p-uniform approaches, that can be viewed
  as alternative implementations of the original Hedges selection method
  approach. We find that the p-curve and p-uniform approaches perform reasonably
  well but not as well as the original Hedges approach in the restrictive
  setting for which all three were designed. We also find they perform poorly in
  more realistic settings, whereas variants of the Hedges approach perform well.
  We conclude by urging caution in the application of selection methods: Given
  the idealistic model assumptions underlying selection methods and the
  sensitivity of population average effect size estimates to them, we advocate
  that selection methods should be used less for obtaining a single estimate
  that purports to adjust for publication bias ex post and more for sensitivity
  analysis-that is, exploring the range of estimates that result from assuming
  different forms of and severity of publication bias.},
  url = {https://journals.sagepub.com/doi/abs/10.1177/1745691616662243},
  keywords = {effect size; meta-analysis; p-curve; p-uniform; selection methods},
  language = {en}
}

@ARTICLE{Bartos2022-im,
  title = {Adjusting for publication bias in JASP and R: Selection models,
  PET-PEESE, and robust Bayesian meta-analysis},
  author = {Bartoš, František and Maier, Maximilian and Quintana, Daniel S and
  Wagenmakers, Eric-Jan},
  journaltitle = {Advances in methods and practices in psychological science},
  publisher = {SAGE Publications},
  volume = {5},
  issue = {3},
  pages = {251524592211092},
  date = {2022-07},
  doi = {10.1177/25152459221109259},
  issn = {2515-2459,2515-2467},
  abstract = {Meta-analyses are essential for cumulative science, but their
  validity can be compromised by publication bias. To mitigate the impact of
  publication bias, one may apply publication-bias-adjustment techniques such as
  precision-effect test and precision-effect estimate with standard errors
  (PET-PEESE) and selection models. These methods, implemented in JASP and R,
  allow researchers without programming experience to conduct state-of-the-art
  publication-bias-adjusted meta-analysis. In this tutorial, we demonstrate how
  to conduct a publication-bias-adjusted meta-analysis in JASP and R and
  interpret the results. First, we explain two frequentist bias-correction
  methods: PET-PEESE and selection models. Second, we introduce robust Bayesian
  meta-analysis, a Bayesian approach that simultaneously considers both
  PET-PEESE and selection models. We illustrate the methodology on an example
  data set, provide an instructional video ( https://bit.ly/pubbias ) and an
  R-markdown script ( https://osf.io/uhaew/ ), and discuss the interpretation of
  the results. Finally, we include concrete guidance on reporting the
  meta-analytic results in an academic article.},
  url = {https://journals.sagepub.com/doi/abs/10.1177/25152459221109259},
  language = {en}
}

@ARTICLE{Maier2023-js,
  title = {Robust Bayesian meta-analysis: Addressing publication bias with
  model-averaging},
  author = {Maier, Maximilian and Bartoš, František and Wagenmakers, Eric-Jan},
  journaltitle = {Psychological methods},
  volume = {28},
  issue = {1},
  pages = {107-122},
  date = {2023-02},
  doi = {10.1037/met0000405},
  pmid = {35588075},
  issn = {1082-989X,1939-1463},
  abstract = {Meta-analysis is an important quantitative tool for cumulative
  science, but its application is frustrated by publication bias. In order to
  test and adjust for publication bias, we extend model-averaged Bayesian
  meta-analysis with selection models. The resulting robust Bayesian
  meta-analysis (RoBMA) methodology does not require all-or-none decisions about
  the presence of publication bias, can quantify evidence in favor of the
  absence of publication bias, and performs well under high heterogeneity. By
  model-averaging over a set of 12 models, RoBMA is relatively robust to model
  misspecification and simulations show that it outperforms existing methods. We
  demonstrate that RoBMA finds evidence for the absence of publication bias in
  Registered Replication Reports and reliably avoids false positives. We provide
  an implementation in R so that researchers can easily use the new methodology
  in practice. (PsycInfo Database Record (c) 2023 APA, all rights reserved).},
  url = {https://psycnet.apa.org/record/2022-62552-001},
  language = {en}
}

@ARTICLE{Guan2016-kn,
  title = {A Bayesian approach to mitigation of publication bias},
  author = {Guan, Maime and Vandekerckhove, Joachim},
  journaltitle = {Psychonomic bulletin \& review},
  publisher = {Springer Science and Business Media LLC},
  volume = {23},
  issue = {1},
  pages = {74-86},
  date = {2016-02},
  doi = {10.3758/s13423-015-0868-6},
  pmid = {26126776},
  issn = {1069-9384,1531-5320},
  abstract = {The reliability of published research findings in psychology has
  been a topic of rising concern. Publication bias, or treating positive
  findings differently from negative findings, is a contributing factor to this
  "crisis of confidence," in that it likely inflates the number of
  false-positive effects in the literature. We demonstrate a Bayesian model
  averaging approach that takes into account the possibility of publication bias
  and allows for a better estimate of true underlying effect size. Accounting
  for the possibility of bias leads to a more conservative interpretation of
  published studies as well as meta-analyses. We provide mathematical details of
  the method and examples.},
  url = {https://dx.doi.org/10.3758/s13423-015-0868-6},
  keywords = {Bayesian inference and parameter estimation; Bayesian statistics;
  Math modeling and model selection; Meta-analysis;Bayesian
  Statistics;replicability-book;extra},
  language = {en}
}

@ARTICLE{Schad2020-ht,
  title = {How to capitalize on a priori contrasts in linear (mixed) models: A
  tutorial},
  author = {Schad, Daniel J and Vasishth, Shravan and Hohenstein, Sven and
  Kliegl, Reinhold},
  journaltitle = {Journal of memory and language},
  volume = {110},
  pages = {104038},
  date = {2020-02-01},
  doi = {10.1016/j.jml.2019.104038},
  issn = {0749-596X},
  abstract = {Factorial experiments in research on memory, language, and in
  other areas are often analyzed using analysis of variance (ANOVA). However,
  for effects with more than one numerator degrees of freedom, e.g., for
  experimental factors with more than two levels, the ANOVA omnibus F-test is
  not informative about the source of a main effect or interaction. Because
  researchers typically have specific hypotheses about which condition means
  differ from each other, a priori contrasts (i.e., comparisons planned before
  the sample means are known) between specific conditions or combinations of
  conditions are the appropriate way to represent such hypotheses in the
  statistical model. Many researchers have pointed out that contrasts should be
  “tested instead of, rather than as a supplement to, the ordinary ‘omnibus’ F
  test” (Hays, 1973, p. 601). In this tutorial, we explain the mathematics
  underlying different kinds of contrasts (i.e., treatment, sum, repeated,
  polynomial, custom, nested, interaction contrasts), discuss their properties,
  and demonstrate how they are applied in the R System for Statistical Computing
  (R Core Team, 2018). In this context, we explain the generalized inverse which
  is needed to compute the coefficients for contrasts that test hypotheses that
  are not covered by the default set of contrasts. A detailed understanding of
  contrast coding is crucial for successful and correct specification in linear
  models (including linear mixed models). Contrasts defined a priori yield far
  more useful confirmatory tests of experimental hypotheses than standard
  omnibus F-tests. Reproducible code is available from https://osf.io/7ukf6/.},
  url = {https://www.sciencedirect.com/science/article/pii/S0749596X19300695},
  keywords = {Contrasts; Null hypothesis significance testing; Linear models; A
  priori hypotheses;Bayesian Statistics}
}

@BOOK{Harrer2021-go,
  title = {Doing meta-analysis with R: A hands-on guide},
  author = {Harrer, Mathias and Cuijpers, Pim and Furukawa, Toshi and Ebert,
  David},
  publisher = {CRC Press},
  location = {London, England},
  edition = {1st},
  date = {2021-09-13},
  pagetotal = {474},
  isbn = {9780367610074},
  language = {en}
}

@ARTICLE{Duval2000-ym,
  title = {Trim and fill: A simple funnel-plot-based method of testing and
  adjusting for publication bias in meta-analysis},
  author = {Duval, S and Tweedie, R},
  journaltitle = {Biometrics},
  publisher = {Wiley},
  volume = {56},
  issue = {2},
  pages = {455-463},
  date = {2000-06},
  doi = {10.1111/j.0006-341x.2000.00455.x},
  pmid = {10877304},
  issn = {0006-341X,1541-0420},
  abstract = {We study recently developed nonparametric methods for estimating
  the number of missing studies that might exist in a meta-analysis and the
  effect that these studies might have had on its outcome. These are simple
  rank-based data augmentation techniques, which formalize the use of funnel
  plots. We show that they provide effective and relatively powerful tests for
  evaluating the existence of such publication bias. After adjusting for missing
  studies, we find that the point estimate of the overall effect size is
  approximately correct and coverage of the effect size confidence intervals is
  substantially improved, in many cases recovering the nominal confidence levels
  entirely. We illustrate the trim and fill method on existing meta-analyses of
  studies in clinical trials and psychometrics.},
  url = {https://onlinelibrary.wiley.com/doi/10.1111/j.0006-341X.2000.00455.x},
  keywords = {Meta-analysis},
  language = {en}
}

@ARTICLE{Veroniki2016-nw,
  title = {Methods to estimate the between-study variance and its uncertainty in
  meta-analysis},
  author = {Veroniki, Areti Angeliki and Jackson, Dan and Viechtbauer, Wolfgang
  and Bender, Ralf and Bowden, Jack and Knapp, Guido and Kuss, Oliver and
  Higgins, Julian P T and Langan, Dean and Salanti, Georgia},
  journaltitle = {Research synthesis methods},
  publisher = {Wiley},
  volume = {7},
  issue = {1},
  pages = {55-79},
  date = {2016-03},
  doi = {10.1002/jrsm.1164},
  pmc = {PMC4950030},
  pmid = {26332144},
  issn = {1759-2879,1759-2887},
  abstract = {Meta-analyses are typically used to estimate the overall/mean of
  an outcome of interest. However, inference about between-study variability,
  which is typically modelled using a between-study variance parameter, is
  usually an additional aim. The DerSimonian and Laird method, currently widely
  used by default to estimate the between-study variance, has been long
  challenged. Our aim is to identify known methods for estimation of the
  between-study variance and its corresponding uncertainty, and to summarise the
  simulation and empirical evidence that compares them. We identified 16
  estimators for the between-study variance, seven methods to calculate
  confidence intervals, and several comparative studies. Simulation studies
  suggest that for both dichotomous and continuous data the estimator proposed
  by Paule and Mandel and for continuous data the restricted maximum likelihood
  estimator are better alternatives to estimate the between-study variance.
  Based on the scenarios and results presented in the published studies, we
  recommend the Q-profile method and the alternative approach based on a
  'generalised Cochran between-study variance statistic' to compute
  corresponding confidence intervals around the resulting estimates. Our
  recommendations are based on a qualitative evaluation of the existing
  literature and expert consensus. Evidence-based recommendations require an
  extensive simulation study where all methods would be compared under the same
  scenarios.},
  url = {https://onlinelibrary.wiley.com/doi/10.1002/jrsm.1164},
  file = {Veroniki et al. 2016 - Methods to estimate the between-study variance and its uncertainty in meta-analysis.pdf},
  keywords = {bias; confidence interval; coverage probability; heterogeneity;
  mean squared error},
  language = {en}
}

@MISC{Borenstein2009-mo,
  title = {Introduction to {Meta-Analysis}},
  author = {Borenstein, Michael and Hedges, Larry V and Higgins, Julian P T and
  Rothstein, Hannah R},
  date = {2009},
  doi = {10.1002/9780470743386},
  url = {http://dx.doi.org/10.1002/9780470743386}
}

@ARTICLE{Riley2011-hp,
  title = {Interpretation of random effects meta-analyses},
  author = {Riley, Richard D and Higgins, Julian P T and Deeks, Jonathan J},
  journaltitle = {BMJ},
  volume = {342},
  pages = {d549},
  date = {2011-02-10},
  doi = {10.1136/bmj.d549},
  pmid = {21310794},
  issn = {0959-8138,1756-1833},
  url = {http://dx.doi.org/10.1136/bmj.d549},
  language = {en}
}

@ARTICLE{IntHout2016-sz,
  title = {Plea for routinely presenting prediction intervals in meta-analysis},
  author = {IntHout, Joanna and Ioannidis, John P A and Rovers, Maroeska M and
  Goeman, Jelle J},
  journaltitle = {BMJ open},
  volume = {6},
  issue = {7},
  pages = {e010247},
  date = {2016-07-12},
  doi = {10.1136/bmjopen-2015-010247},
  pmc = {PMC4947751},
  pmid = {27406637},
  issn = {2044-6055},
  abstract = {OBJECTIVES: Evaluating the variation in the strength of the effect
  across studies is a key feature of meta-analyses. This variability is
  reflected by measures like τ(2) or I(2), but their clinical interpretation is
  not straightforward. A prediction interval is less complicated: it presents
  the expected range of true effects in similar studies. We aimed to show the
  advantages of having the prediction interval routinely reported in
  meta-analyses. DESIGN: We show how the prediction interval can help understand
  the uncertainty about whether an intervention works or not. To evaluate the
  implications of using this interval to interpret the results, we selected the
  first meta-analysis per intervention review of the Cochrane Database of
  Systematic Reviews Issues 2009-2013 with a dichotomous (n=2009) or continuous
  (n=1254) outcome, and generated 95\% prediction intervals for them. RESULTS:
  In 72.4\% of 479 statistically significant (random-effects p0), the 95\%
  prediction interval suggested that the intervention effect could be null or
  even be in the opposite direction. In 20.3\% of those 479 meta-analyses, the
  prediction interval showed that the effect could be completely opposite to the
  point estimate of the meta-analysis. We demonstrate also how the prediction
  interval can be used to calculate the probability that a new trial will show a
  negative effect and to improve the calculations of the power of a new trial.
  CONCLUSIONS: The prediction interval reflects the variation in treatment
  effects over different settings, including what effect is to be expected in
  future patients, such as the patients that a clinician is interested to treat.
  Prediction intervals should be routinely reported to allow more informative
  inferences in meta-analyses.},
  url = {http://dx.doi.org/10.1136/bmjopen-2015-010247},
  keywords = {Clinical trial; Cochrane Database of Systematic Reviews;
  Heterogeneity; Meta-analysis; Prediction interval; Random effects;Bayesian
  Statistics},
  language = {en}
}

@ARTICLE{Vevea2005-xc,
  title = {Publication bias in research synthesis: sensitivity analysis using a
  priori weight functions},
  author = {Vevea, Jack L and Woods, Carol M},
  journaltitle = {Psychological methods},
  publisher = {American Psychological Association (APA)},
  volume = {10},
  issue = {4},
  pages = {428-443},
  date = {2005-12},
  doi = {10.1037/1082-989X.10.4.428},
  pmid = {16392998},
  issn = {1082-989X,1939-1463},
  abstract = {Publication bias, sometimes known as the "file-drawer problem" or
  "funnel-plot asymmetry," is common in empirical research. The authors review
  the implications of publication bias for quantitative research synthesis
  (meta-analysis) and describe existing techniques for detecting and correcting
  it. A new approach is proposed that is suitable for application to
  meta-analytic data sets that are too small for the application of existing
  methods. The model estimates parameters relevant to fixed-effects,
  mixed-effects or random-effects meta-analysis contingent on a hypothetical
  pattern of bias that is fixed independently of the data. The authors
  illustrate this approach for sensitivity analysis using 3 data sets adapted
  from a commonly cited reference work on research synthesis (H. M. Cooper \& L.
  V. Hedges, 1994).},
  url = {https://psycnet.apa.org/record/2005-16136-006},
  urldate = {2024-02-03},
  language = {en}
}

@ARTICLE{Higgins2002-fh,
  title = {Quantifying heterogeneity in a meta-analysis},
  author = {Higgins, Julian P T and Thompson, Simon G},
  journaltitle = {Statistics in medicine},
  volume = {21},
  issue = {11},
  pages = {1539-1558},
  date = {2002-06-15},
  doi = {10.1002/sim.1186},
  pmid = {12111919},
  issn = {0277-6715},
  abstract = {The extent of heterogeneity in a meta-analysis partly determines
  the difficulty in drawing overall conclusions. This extent may be measured by
  estimating a between-study variance, but interpretation is then specific to a
  particular treatment effect metric. A test for the existence of heterogeneity
  exists, but depends on the number of studies in the meta-analysis. We develop
  measures of the impact of heterogeneity on a meta-analysis, from mathematical
  criteria, that are independent of the number of studies and the treatment
  effect metric. We derive and propose three suitable statistics: H is the
  square root of the chi2 heterogeneity statistic divided by its degrees of
  freedom; R is the ratio of the standard error of the underlying mean from a
  random effects meta-analysis to the standard error of a fixed effect
  meta-analytic estimate, and I2 is a transformation of (H) that describes the
  proportion of total variation in study estimates that is due to heterogeneity.
  We discuss interpretation, interval estimates and other properties of these
  measures and examine them in five example data sets showing different amounts
  of heterogeneity. We conclude that H and I2, which can usually be calculated
  for published meta-analyses, are particularly useful summaries of the impact
  of heterogeneity. One or both should be presented in published meta-analyses
  in preference to the test for heterogeneity.},
  url = {http://dx.doi.org/10.1002/sim.1186},
  language = {en}
}

@ARTICLE{Lopez-Lopez2014-it,
  title = {Estimation of the predictive power of the model in mixed-effects
  meta-regression: A simulation study},
  author = {López-López, José Antonio and Marín-Martínez, Fulgencio and
  Sánchez-Meca, Julio and Van den Noortgate, Wim and Viechtbauer, Wolfgang},
  journaltitle = {The British journal of mathematical and statistical psychology},
  publisher = {Wiley},
  volume = {67},
  issue = {1},
  pages = {30-48},
  date = {2014-02},
  doi = {10.1111/bmsp.12002},
  pmid = {23297709},
  issn = {0007-1102,2044-8317},
  abstract = {Several methods are available to estimate the total and residual
  amount of heterogeneity in meta-analysis, leading to different alternatives
  when estimating the predictive power in mixed-effects meta-regression models
  using the formula proposed by Raudenbush (1994, 2009). In this paper, a
  simulation study was conducted to compare the performance of seven estimators
  of these parameters under various realistic scenarios in psychology and
  related fields. Our results suggest that the number of studies (k) exerts the
  most important influence on the accuracy of the results, and that precise
  estimates of the heterogeneity variances and the model predictive power can
  only be expected with at least 20 and 40 studies, respectively. Increases in
  the average within-study sample size (N¯) also improved the results for all
  estimators. Some differences among the accuracy of the estimators were
  observed, especially under adverse (small k and N¯) conditions, while the
  results for the different methods tended to convergence for more optimal
  scenarios.},
  url = {https://bpspsychub.onlinelibrary.wiley.com/doi/abs/10.1111/bmsp.12002},
  language = {en}
}

@BOOK{Gelman2020-tg,
  title = {Regression and Other Stories},
  author = {Gelman, Andrew and Hill, Jennifer and Vehtari, Aki},
  publisher = {Cambridge University Press},
  date = {2020-07-23},
  doi = {10.1017/9781139161879},
  isbn = {9781139161879},
  abstract = {Most textbooks on regression focus on theory and the simplest of
  examples. Real statistical problems, however, are complex and subtle. This is
  not a book about the theory of regression. It is about using regression to
  solve real problems of comparison, estimation, prediction, and causal
  inference. Unlike other books, it focuses on practical issues such as sample
  size and missing data and a wide range of goals and techniques. It jumps right
  in to methods and computer code you can use immediately. Real examples, real
  stories from the authors' experience demonstrate what regression can do and
  its limitations, with practical advice for understanding assumptions and
  implementing methods for experiments and observational studies. They make a
  smooth transition to logistic regression and GLM. The emphasis is on
  computation in R and Stan rather than derivations, with code available online.
  Graphics and presentation aid understanding of the models and model fitting.},
  url = {https://www.cambridge.org/highereducation/books/regression-and-other-stories/DD20DD6C9057118581076E54E40C372C},
  urldate = {2023-02-14}
}

@ARTICLE{Hedges1989-ip,
  title = {An unbiased correction for sampling error in validity generalization
  studies},
  author = {Hedges, Larry V},
  journaltitle = {The Journal of applied psychology},
  publisher = {American Psychological Association (APA)},
  volume = {74},
  issue = {3},
  pages = {469-477},
  date = {1989-06},
  doi = {10.1037/0021-9010.74.3.469},
  issn = {0021-9010,1939-1854},
  url = {http://dx.doi.org/10.1037/0021-9010.74.3.469},
  language = {en}
}

@ARTICLE{Hedges2019-ry,
  title = {Statistical analyses for studying replication: Meta-analytic
  perspectives},
  author = {Hedges, Larry V and Schauer, Jacob M},
  journaltitle = {Psychological methods},
  publisher = {American Psychological Association (APA)},
  volume = {24},
  issue = {5},
  pages = {557-570},
  date = {2019-10},
  doi = {10.1037/met0000189},
  pmid = {30070547},
  issn = {1082-989X,1939-1463},
  abstract = {Formal empirical assessments of replication have recently become
  more prominent in several areas of science, including psychology. These
  assessments have used different statistical approaches to determine if a
  finding has been replicated. The purpose of this article is to provide several
  alternative conceptual frameworks that lead to different statistical analyses
  to test hypotheses about replication. All of these analyses are based on
  statistical methods used in meta-analysis. The differences among the methods
  described involve whether the burden of proof is placed on replication or
  nonreplication, whether replication is exact or allows for a small amount of
  "negligible heterogeneity," and whether the studies observed are assumed to be
  fixed (constituting the entire body of relevant evidence) or are a sample from
  a universe of possibly relevant studies. The statistical power of each of
  these tests is computed and shown to be low in many cases, raising issues of
  the interpretability of tests for replication. (PsycINFO Database Record (c)
  2019 APA, all rights reserved).},
  url = {http://dx.doi.org/10.1037/met0000189},
  file = {Hedges and Schauer 2019 - Statistical analyses for studying replication - Meta-analytic perspectives.pdf},
  keywords = {replicability-book;methods},
  language = {en}
}

