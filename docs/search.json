[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "The three Rs of trustworthy science",
    "section": "",
    "text": "Welcome\nThe aim of this book is to provide a critical and up-to-date overview of the replication problem in science from theoretical, philosophical, and statistical perspectives. Rather than being exhaustive, the book seeks to offer a general framework for understanding and teaching the replication issue in the context of the current reproducibility and replicability crisis.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#book-structure",
    "href": "index.html#book-structure",
    "title": "The three Rs of trustworthy science",
    "section": "Book structure",
    "text": "Book structure\nThe book is written in Quarto. All materials and source code GitHub. The main language used in the statistical part of the book is R. In addition, we wrote several functions used in the examples collected into the R folder. You are free to fork, share and reuse contents. If you have suggestions, additions or comments or to improve the content you can submit a pull request on Github or contact Filippo Gambarota (filippo.gambarota@unipd.it).",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "chapters/chapter1.html",
    "href": "chapters/chapter1.html",
    "title": "1  What is a replication",
    "section": "",
    "text": "Key Readings\nKey readings:\nOptional readings:",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>What is a replication</span>"
    ]
  },
  {
    "objectID": "chapters/chapter1.html#key-readings",
    "href": "chapters/chapter1.html#key-readings",
    "title": "1  What is a replication",
    "section": "",
    "text": "Machery (2020): What is a replication?\nNosek & Errington (2020): What is replication?\n\n\n\nRosenthal (1990): Replication in behavioral research. Especially for the concepts of precision and the idea of weighting replication studies according to qualitative and quantitative factors.\nNational Academies of Sciences, Engineering, and Medicine et al. (2019) National Academies report on Reproducibility and Replicability in Science",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>What is a replication</span>"
    ]
  },
  {
    "objectID": "chapters/chapter1.html#replication-robustness-and-reproducibility",
    "href": "chapters/chapter1.html#replication-robustness-and-reproducibility",
    "title": "1  What is a replication",
    "section": "1.1 Replication, Robustness and Reproducibility",
    "text": "1.1 Replication, Robustness and Reproducibility\nThis book is concerned with what we call the three “R”’s of trustworthy science: Replication, Robustness and Reproducibility. We begin by sketching a definition of each. This first Chapter then elaborates on the definition of replicability more specifically.\nIn 2019 the USA’s National Academies published a report on Reproducibility and Replicability in Science (National Academies of Sciences, Engineering, and Medicine et al., 2019), which we strongly recommend as a complementary reading to this document. They propose to define Replicability as “obtaining consistent results across studies aimed at answering the same scientific question, each of which has obtained its own data”. This will be our first “R”. The NAS definition is also endorsed by the American Statistical Association (Broman et al., 2017) and generally adopted in statistics (for example (Heller et al., 2014) add a couple of review papers).\nThe second “R” is Reproducibility, defined by NAS as “obtaining consistent results using the same input data, computational steps, methods, and code, and conditions of analysis” (National Academies of Sciences, Engineering, and Medicine et al., 2019). Here the experimental design, data and methodologies of analysis are all fixed. A reproducible research paper should provide enough information to obtain the results originally reported, starting from the same raw data. Raw data are data in a form as close as possible to what was generated by the original experimental source. Defining precisely what raw data is can be complex both conceptually and practically, but a digression on this would take us too far astray. Nowadays it is becoming more common to supplement a paper with analysis code, raw data (if possible) and details or materials (e.g., experimental stimuli, biological reagents, questionnaires) about the experimental setup. Reproducibility can be considered as the most fundamental pre-requisite of replication in science. Chapter 2 will provide a more detailed explanation of the concept with an overview of tools for doing reproducible science.\nOur third “R” also involves keeping the raw data fixed, and focuses on changing parts of the analysis method. This is different from the previous two, as it investigates the robustness of the results to the many judgment calls that typically need to be made in the implementation of a statistical analysis. Chapter 6 will present some methods for investigating robustness. For example, multiverse analysis (e.g., Steegen et al., 2016) is a way to systematically sample different plausible analytical strategies and see their impact on the final conclusions.\nUsage of the first two “R” terms is often inconsistent; and, it is reversed in computer science. (Barba, 2018; Goodman et al., 2016; Kenett & Shmueli, 2015) review these terminologies and their usage.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>What is a replication</span>"
    ]
  },
  {
    "objectID": "chapters/chapter1.html#defining-replication",
    "href": "chapters/chapter1.html#defining-replication",
    "title": "1  What is a replication",
    "section": "1.2 Defining Replication",
    "text": "1.2 Defining Replication\n\n1.2.1 Challenges\nWhen studying, planning or conducting studies about replication in science, it is essential to start from a clear definition and formalization of replication. First we need to define when an experiment or study constitutes a replication attempt of another.\nA gray area often arises where scientists attempt to replicate the results of a study using a similar but not identical scientific methodology. If they obtain a different result, should they conclude that the new study did not replicate the original? To answer this question, the replication needs to be evaluated not only in terms of its result; but, also with regard to the study design — for example, with regard to whether experimental factors, approaches and conditions are the same or different. In this first chapter we discuss the design of a replication study from both an empirical and a philosophical perspective.\nThe statistical analysis and the evaluation of whether replication was successful will be covered in Chapter 4, where we will discover that there can be several different definitions of what a successful replication is. Anderson & Kelley (2024) reviewed more than 20 different replication criteria and Heyard et al. (2024) identified more than 50 published statistical approaches to assess a replication experiment.\nWe begin by reviewing concepts that in our view are general enough to be applied to multiple research areas and translated into an empirically testable statistical framework. The bibliography for this chapter contains a selected list of theoretical papers on replication.\n\n\n1.2.2 Cronbach’s essential components of an experiment\nWe start by defining the essential components of an experiment as proposed by Cronbach & Shapiro (1982)1 Their framework is concisely referred to as UTOS because it consisted of these four components:\n\n\n\n\n\n\nUTOS Framework\n\n\n\n\nUnits: the elements of the population from which the study sample has been drawn; the term population is used here in its statistical sense: a real or hypothetical universe of units (not necessarily persons) to which the inferential conclusions of the study should apply;\nTreatments: the independent variables, for example experimental interventions, or population subgroups of interest;\nOutcomes: the dependent variables on which the study is focused\nSetting: the remainder of factors that can affect the experimental outcome and its relation to treatments. For example the factors defining the “environment” where a psychological experiment takes place (lab vs online experiment, cultural aspects, etc.)\n\n\n\nSome initial comments:\n\nU: this component positions us unequivocally in the territory of statistical inference, where we posit a population of reference consisting of distinct units, and we are interested in generalizing from an observed sample to the entire population. Naturally not all science aligns with this paradigm. Before reading further, try to think about important scientific studies you may have come across recently that do not fit with this paradigm.\nT & O: these components implicitly assume that the goal of the study was to elucidate how, in the population of interest, the outcomes relate to the independent variables, perhaps causally, if the experiment is controlled. Again, this is an important subset of science, but is by no means exhaustive. Clustering, discovery of latent dimensions in high dimensional data, segmentation of images, and a number of other exceptions come to mind. Add your own. Also, important science can be simply exploratory and descriptive, but replicability questions are relevant there as well.\nS: By changing settings “minimally” you get “essentially” the same experiments. For example you may change the temperature in the lab for an experiment where temperature is a negligible factor. On the other hand, if you change the temperature in a chemistry experiment where a certain reaction is very sensitive to temperature, you may get a completely different experiment and arguably a different study: skating on ice vs water, so to speak. So it takes a little sprinkle of common sense for this to be a useful definition. Be alert.\n\n\n\n1.2.3 Machery’s definition of a replication experiment\nMachery (2020) proposed a simple and flexible perspective on replication called the resampling account.\nConsider an original experiment in the UTOS framework, and imagine we can sharply specify fixed and random factors in the following sense: a random factor can be thought of as sampled from the population of interest, while a fixed factor is not sampled, but rather determined by design. For example, participants (Units) can be assumed to be random when the study samples a group of participants according to a well defined sampling scheme. In contrast, an experimental manipulation is generally fixed. Random sampling affords the option to use statistical inference to learn parameters of the population of interest (see also Yarkoni, 2020 for the idea of generalizability). Stricltly speaking, inference using a fixed factor is limited only to the tested condition. Defining fixed and random factors in real settings can be complex. Challenges and approaches are extensively debated in the literature (e.g., Clark & Linzer, 2015).\nReturning to replication, according to Machery, a replication experiment is an experiment created by sampling from the random factors (e.g., new group of participants), and keeping the non-random factors fixed (e.g., same type of drug or treatment). In this definition, population parameters of interest are identical between an original study and its replication. We will return to this point in Chapter 3.\nA replication experiment is meant to assess what Machery calls reliability. Machery (2020) uses the term reliability as in measurement theory (Tal, 2020) where a reliable instrument (in this case the experiment is seen as the instrument) produces consistent 2 results across different replications of the measurement. When replicating a whole experiment, say sampling a new sample of participants, we are similarly interested in evaluating whether the experimental setup is reliable in the sense of producing consistent results.\nAs an illustration of the measurement metaphor, imagine a treatment for depression that has been evaluated in an original study. The researchers developed a protocol, defined the concept of depression, administered the treatment and chose some depression self-report measures, finding a decreasing level of depression compared to the control group. Another group of researchers decided to replicate this original experiment. They adopted the same protocol, definitions and measures but collected another sample of participants from the same population. The results of the replication experiment, for example the magnitude of the effect of the treatment on the depression level as defined by the protocol, may or may not be close to the original. If they are close, this provides evidence in favor of the reliability of the original experiment, as defined above (Nosek & Errington, 2017). Here, we are not questioning the definition or the measures used in the study.\nTo recap the progress made so far, if you can frame a study as UTOS, and clearly separate the random and fixed factors, you know how to design a replication study!",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>What is a replication</span>"
    ]
  },
  {
    "objectID": "chapters/chapter1.html#validity-and-extensions",
    "href": "chapters/chapter1.html#validity-and-extensions",
    "title": "1  What is a replication",
    "section": "1.3 Validity and Extensions",
    "text": "1.3 Validity and Extensions\n\n1.3.1 Conceptual Replication\nDoes is make sense to compare experiments when fixed factors have changed? In many cases it does, and this type of analysis still falls under the purview of replication, broadly construed. Readers familiar with the replication literature may have noticed that we did not use concepts such as direct or conceptual replication yet. Briefly, a direct replication is usually defined as an experiment that tries to recreate the original experiment as closely as possible. While this objective can be achieved in some fields, it is typically empirically challenging, due to uncontrollable factors (Nosek & Errington, 2017, 2020). Examples include replicating effects that are strongly culturally dependent, or experiments that used a completely outdated technology for the experimental setup.\nIn contrast, conceptual replication is a broad term defining a replication experiment with similar aims and methodology but with important differences (e.g., Crandall & Sherman, 2016). In Machery’s terms, a conceptual replication changes some of the fixed factors. In the depression example above, a conceptual replication might be a second experiment using another theoretical definition or measure of depression. Here, the goal is still within the broad umbrella of understanding the effect of the treatment on depression, but the experiment is not a strict replication in the Machery sense, because the outcome measurement methodology has changed. For this reason Machery (2020) uses the term extension. Extensions may include a methodological change but also a more profound philosophical change, for example using a scale motivated by a different theoretical framework for defining depression. An extension of an experiment is not assessing the reliability, but rather the validity of the original experiment and in a broader sense the theory underlying the original experiment. Like reliability, the term validity is used with measurement theory in mind. The validity of a measurement involves correspondence with the true phenomenon being assessed (for example, the actual weight of an object being weighed).\nIn the depression example, if the researchers want to see whether the treatment is effective also using other measures of depression, they are extending the original results; and, thereby, testing the validity of the result about the treatment.\n\n\n1.3.2 Precision of Replication\nIn practice we can often design a variety of replication / extension experiments, with varying degrees of similarity with the original one. Rosenthal (1990) defines the important concept of precision as the degree of similarity between the replication and the original experiment. The most precise is a Machery replication. Generally, very precise experiments lack external validity (they are not able to extend) but they can directly speak to the reliability of the original experimental setup and results. Less precise experiments will provide weaker evidence regarding the original experiment, shifting the focus to our confidence in the underlying theory. For example, we may say that the treatment also reduces physiological indexes of depression or is also effective in a different population of patients.\n\n\n1.3.3 Replication vs Extension: different scientific value?\nWe have seen that a precise replication experiment is an attempt to ask the same inferential question in as simlar a way as possible. Variations in setting and fixed factors can generate experiments that probe the trustworthiness of the conclusions further. These slight variations can be successfully analyzed together with replicability in mind, for example in a meta-analysis (see Chapter 4), even though they do not constitute a strict replication by Machery’s exacting standards.\nAs settings are varied further and the replication becomes less precise, we eventually find ourselves having extended the experimental paradigm beyond its initial goals and populations. When is this line crossed? The answer to this question is dependent on the context; and, reasonable scientists may disagree about it. An important debate in the literature regards the value of a replication, as compared to an extension, of an experiment. Crandall & Sherman (2016) consider extensions to more valuable while Simons (2014) argues in favor of focusing on replications of experiments. Clearly both have a place in science. If the main aim is to test the reliability of the original experiment, one should reduce the heterogeneity (i.e., increase the precision) as much as possible. On the other hand, if the aim is to explore different instances of the same effect or theory, one should extend the experimental setup.\nIn the remainder of the discussion we will use precise replication to denote a replication experiment in the strict sense of Machery, or a sufficiently close approximation, while we use the word extension to describe replication settings where the precision is lower. The appropriate definition of a successful replication, and the proper choice of statistical methodlogy for analyzing data may vary greatly, depending on where the follow-up study falls on this precision continuum.\n\n\n1.3.4 Contextual dependency\nContextual dependency (Gollwitzer & Schwabe, 2022; Inbar, 2016; Van Bavel et al., 2016) complicates the Machery (2020) framework by considering specific weights assigned to each UTOS element. Without going into details, the impact of the different UTOS elements on the actual experiment could differ according to the type of research question. For example, consider a social psychology experiment originally conducted on Western American participants and later extended to European or Asian participants. If the psychological construct of interest is strongly affected by cultural variations, the impact of changing the ethnicity may be considerable. In contrast, a lab-based experiment about low-level perceptual effects may be more portable across culturally diverse populations. Our expectations about the replication/extension outcome need to be calibrated with respect to the specific research question at hand, and the impact of each experimental element.\n\n\n1.3.5 Investigators as a Factor\nAs a final note, one important but often underestimated problem concerns the investigators conducting the replication study or studies. They can legitimately be considered part of the setting in some cases; but, they can be seen as random or irrelevant in others. Rosenthal (1990) clearly described the problem of correlated replicators and proposed different weighting approaches, based on the design of the replication study, for replication/extension studies conducted from authors that are orthogonal in terms of background theory and methods. Authors sharing a theory or a strict network of co-authorships are likely to create experiments and data that are more similar, as compared to independent authors. A replication or extension study conducted by an independent, and possibly skeptical, researcher should have a greater impact on increasing or decreasing confidence in the original findings, as compared to a follow-up study conducted by the original author.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>What is a replication</span>"
    ]
  },
  {
    "objectID": "chapters/chapter1.html#key-questions",
    "href": "chapters/chapter1.html#key-questions",
    "title": "1  What is a replication",
    "section": "1.4 Key Questions",
    "text": "1.4 Key Questions\n\nIn what ways do replication, robustness, and reproducibility contribute to the trustworthiness of science?\nWhen should methodological differences between studies be viewed as a strength rather than a limitation in replication research?",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>What is a replication</span>"
    ]
  },
  {
    "objectID": "chapters/chapter1.html#references",
    "href": "chapters/chapter1.html#references",
    "title": "1  What is a replication",
    "section": "References",
    "text": "References\n\n\n\n\nAnderson, S. F., & Kelley, K. (2024). Sample size planning for replication studies: The devil is in the design. Psychological Methods, 29, 844–867. https://doi.org/10.1037/met0000520\n\n\nBarba, L. A. (2018). Terminologies for reproducible research. arXiv [Cs.DL], arXiv:1802.03311. http://arxiv.org/abs/1802.03311\n\n\nBroman, K., Cetinkaya-Rundel, M., Nussbaum, A., Paciorek, C., Peng, R., Turek, D., & Wickham, H. (2017, January 18). Recommendations to funding agencies for supporting reproducible research. https://www.amstat.org/asa/files/pdfs/POL-ReproducibleResearchRecommendations.pdf\n\n\nClark, T. S., & Linzer, D. A. (2015). Should i use fixed or random effects? Political Science Research and Methods, 3, 399–408. https://doi.org/10.1017/psrm.2014.32\n\n\nCrandall, C. S., & Sherman, J. W. (2016). On the scientific superiority of conceptual replications for scientific progress. Journal of Experimental Social Psychology, 66, 93–99. https://doi.org/10.1016/j.jesp.2015.10.002\n\n\nCronbach, L. J., & Shapiro, K. (1982). Designing evaluations of educational and social programs. https://eduq.info/xmlui/handle/11515/9106\n\n\nGollwitzer, M., & Schwabe, J. (2022). Context dependency as a predictor of replicability. Review of General Psychology: Journal of Division 1, of the American Psychological Association, 26, 241–249. https://doi.org/10.1177/10892680211015635\n\n\nGoodman, S. N., Fanelli, D., & Ioannidis, J. P. A. (2016). What does research reproducibility mean? Science Translational Medicine, 8, 341ps12. https://doi.org/10.1126/scitranslmed.aaf5027\n\n\nHeller, R., Bogomolov, M., & Benjamini, Y. (2014). Deciding whether follow-up studies have replicated findings in a preliminary large-scale omics study. Proceedings of the National Academy of Sciences of the United States of America, 111, 16262–16267. https://doi.org/10.1073/pnas.1314814111\n\n\nHeyard, R., Pawel, S., Frese, J., Voelkl, B., Würbel, H., McCann, S., Held, L., Wever, K. E., Hartmann, H., Townsin, L., & Zellers, S. (2024). A scoping review on metrics to quantify reproducibility: A multitude of questions leads to a multitude of metrics. https://scholar.google.com/citations?view_op=view_citation&hl=en&citation_for_view=JAb7P1QAAAAJ:ldfaerwXgEUC\n\n\nInbar, Y. (2016). Association between contextual dependence and replicability in psychology may be spurious. Proceedings of the National Academy of Sciences of the United States of America, 113, E4933–4. https://doi.org/10.1073/pnas.1608676113\n\n\nKenett, R. S., & Shmueli, G. (2015). Clarifying the terminology that describes scientific reproducibility. Nature Methods, 12, 699. https://doi.org/10.1038/nmeth.3489\n\n\nMachery, E. (2020). What is a replication? Philosophy of Science, 87, 545–567. https://doi.org/10.1086/709701\n\n\nNational Academies of Sciences, Engineering, and Medicine, Policy and Global Affairs, Committee on Science, Engineering, Medicine, and Public Policy, Board on Research Data and Information, Division on Engineering and Physical Sciences, Committee on Applied and Theoretical Statistics, Board on Mathematical Sciences and Analytics, Division on Earth and Life Studies, Nuclear and Radiation Studies Board, & Division of Behavioral and Social Sciences and Education. (2019). Reproducibility and replicability in science. National Academies Press. https://doi.org/10.17226/25303\n\n\nNosek, B. A., & Errington, T. M. (2017). Making sense of replications. eLife, 6, e23383. https://doi.org/10.7554/eLife.23383\n\n\nNosek, B. A., & Errington, T. M. (2020). What is replication? PLoS Biology, 18, e3000691. https://doi.org/10.1371/journal.pbio.3000691\n\n\nParmigiani, G. (2023). Defining replicability of prediction rules. Statistical Science: A Review Journal of the Institute of Mathematical Statistics, 38. https://doi.org/10.1214/23-sts891\n\n\nRosenthal, R. (1990). Replication in behavioral research. Journal of Social Behavior and Personality. https://search.proquest.com/openview/5ccc3a267139968c18c0d7ce1a1c09f6/1?pq-origsite=gscholar&cbl=1819046\n\n\nSimons, D. J. (2014). The value of direct replication. Perspectives on Psychological Science: A Journal of the Association for Psychological Science, 9, 76–80. https://doi.org/10.1177/1745691613514755\n\n\nSteegen, S., Tuerlinckx, F., Gelman, A., & Vanpaemel, W. (2016). Increasing transparency through a multiverse analysis. Perspectives on Psychological Science: A Journal of the Association for Psychological Science, 11, 702–712. https://doi.org/10.1177/1745691616658637\n\n\nTal, E. (2020). Measurement in science. Metaphysics Research Lab, Stanford University. https://plato.stanford.edu/archives/fall2020/entries/measurement-science/\n\n\nVan Bavel, J. J., Mende-Siedlecki, P., Brady, W. J., & Reinero, D. A. (2016). Contextual sensitivity in scientific reproducibility. Proceedings of the National Academy of Sciences of the United States of America, 113, 6454–6459. https://doi.org/10.1073/pnas.1521897113\n\n\nYarkoni, T. (2020). The generalizability crisis. The Behavioral and Brain Sciences, 45, e1. https://doi.org/10.1017/S0140525X20001685",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>What is a replication</span>"
    ]
  },
  {
    "objectID": "chapters/chapter1.html#footnotes",
    "href": "chapters/chapter1.html#footnotes",
    "title": "1  What is a replication",
    "section": "",
    "text": "This framework has been proposed for social science and psychology but can be easily applied to different fields. For example see Parmigiani (2023) for a more detailed taxonomy applied to biomedical studies.↩︎\ne.g. close in some quantiative metric if they are real numbers↩︎",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>What is a replication</span>"
    ]
  },
  {
    "objectID": "chapters/chapter2.html",
    "href": "chapters/chapter2.html",
    "title": "2  Replicability and reproducibility",
    "section": "",
    "text": "We are working on it! Stay tuned!",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Replicability and reproducibility</span>"
    ]
  },
  {
    "objectID": "chapters/chapter3.html",
    "href": "chapters/chapter3.html",
    "title": "3  Probability of Replication",
    "section": "",
    "text": "Key Readings\nY\nKey readings:\nOptional readings:",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Probability of Replication</span>"
    ]
  },
  {
    "objectID": "chapters/chapter3.html#key-readings",
    "href": "chapters/chapter3.html#key-readings",
    "title": "3  Probability of Replication",
    "section": "",
    "text": "Miller (2009) What is the probability of replicating a statistically significant effect?\n\n\nPawel & Held (2020) Probabilistic forecasting of replication studies\n\n\n\n\n\nUlrich & Miller (2020): Questionable research practices may have little effect on replicability. A more extended work based on the Miller (2009) model estimating the impact of questionable research practices",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Probability of Replication</span>"
    ]
  },
  {
    "objectID": "chapters/chapter3.html#formalization",
    "href": "chapters/chapter3.html#formalization",
    "title": "3  Probability of Replication",
    "section": "\n3.1 Formalization",
    "text": "3.1 Formalization\n\n3.1.1 The Miller (2009) definitions\nMiller (2009) proposed one of the earliest probabilistic frameworks for estimating the probability of replication. He defines two different senses of replication probabilities:\n\naggregate replication probability (ARP): the probability of researchers working on a certain research field finding a statistically significant result in a replication study given a significant original study.\nindividual replication probability (IRP): the probability of finding a significant effect on a replication study when replicating a specific original study.\n\nIn this chapter we will take inspiration from Miller’s definitions and provide a formal framework to answer key questions about replication probabilities.\n\n\n\n\n\n\nKey questions\n\n\n\n\nWhat is the probability of replicating an observed effect in a precise replication experiment?\nWhat is the probability of replicating experiments of a certain type within a research field?\nHow is the probability of replication affected by the experimental design (e.g., sample size, etc.) and by the effect size?\nIs the probability of replication affected by questionable research practices or publication bias?\n\n\n\n\n3.1.2 Notation\nWe begin by providing a consistent notation and terminology, and a formal framework for the statistical models and analysis examples throughout the book. The notation is based on the replication model by Pawel & Held (2020) and the meta-analytical notation by Hedges & Schauer (2021), Schauer & Hedges (2021), and Hedges & Schauer (2019).\nWe use \\(x\\)’s to denote predictors, treatments, and fixed factors, and \\(y\\)’s to denote outcomes. The subscript \\(i\\) denotes individuals within a study, while \\(r\\) indexes studies. We use \\(r=0\\) to denote the original study, and \\(r = 1, 2, \\ldots, R\\) to denote the \\(R \\geq 1\\) replication studies.\nWe use the greek letter \\(\\theta\\)’s to define the true effects for the original and replication study(s). These are population parameters. In particular \\(\\theta_0\\) is the effect in original study and \\(\\theta_r\\) is the effect in the \\(r\\)-th study. In principle, \\(\\theta\\)’s can be vector valued, but for simplicity we will focus on a scalar effect. We will also use other greek letters for parameters specific to individual settings.\nThe one-to-one replication design is used when a single replication study is replicating an original experiment. On the other hand, a replication project with \\(R\\) replication studies is called a one-to-many design.\nIn Chapter 1 we covered the Machery definition of replication, in which only the randomly drawn subjects differ between the original study and the replications. In a replication of this type, the population of reference remains the same, an therefore \\(\\theta_0 = \\theta_1 = \\ldots = \\theta_R\\). This will be the case with Miller’s original models, for example. Generally, in a highly precise replication, this will also be approximately correct.\nOn the other hand, if we consider a group of extension experiments, or even more broadly examine an entire field of research, we typically alter fixed factors, and thus consider many different, albeit related, populations. In this case, the \\(\\theta\\)’s will no longer be identical. Variation between \\(\\theta\\)’s is an important aspect of replicability analysis. We will return to it in several places. For now, keep in mind that this variation depends on the unverse of studies of interest. For example, a group of extension experiments for a specific treatment / outcome combination will generally have smaller variability than a collection of experiments considering the effect of the same treatment on a variety of outcomes.\n\n3.1.3 Two-Level Models\nOne way to think about studying the variation of the \\(\\theta\\)’s is to frame the analysis in a two-level model. Within a study, we imagine that subjects are drawn from a population, with each population associated with a specific study design / fixed factors. In turn, then, study designs themselves are seen as draws from a hypothetically vast collection of possible designs. This allows to describe the variation of \\(\\theta\\)’s probabilistically. In many examples we will use a Normal distribution to describe the resulting population of effect sizes. This normal distribution will have mean \\(\\mu_{\\theta}\\) and variance \\(\\tau^2\\).\nReplication is often framed in terms of tests of hypothesis. Parenthetically, several fields of study are making organized attempts at moving scientific reporting away from tests of hypotheses (Harrington et al., 2019). Evidential summaries from hypothesis tests, such as p-values, systematically confound sample size and effect size (Wasserstein & Lazar, 2016). Better reporting focuses on quantitative effect sizes. Nonetheless, replicability analysis requires explicit consideration of significance, as this remains a factor in determining publication.\nHere we will define the null hypothesis to be that “the effect is too small to be of scientific interest”. Formally, the null hypothesis is true when \\(|\\theta| &lt; h_0\\) where \\(h_0\\) is specified by the analyst and defines a so-called region of practical equivalence or ROPE (Kruschke, 2015; Kruschke & Liddell, 2018)1. In a precise replication, \\(\\theta\\) does not change across studies and thus the truth-value of the null hypothesis is invariant across studies. In contrast, if we consider a group of extension experiments or a whole field, the \\(\\theta\\)’s vary and the null hypothesis may hold in some studies and not others. We will use \\(\\pi\\) to denote the proportion of non-null effect in a population of studies.\nMiller (2009) notes that \\(\\pi\\) is an unknown quantity which is difficult to estimate from the literature. In Psychology, Wilson & Wixted (2018) tried to estimate \\(\\pi\\) based on the Collaboration & Open Science Collaboration (2015) large scale replication project, which found different rates for social and cognitive psychology. In a similar vein, Jager & Leek (2014) tried to estimate the false discovery rate across the top medical journals.\nThe more precise the replication, the smaller the value of \\(\\tau\\) will be. As \\(\\tau\\) goes to zero, all \\(\\theta\\)’s will concentrate around \\(\\mu_{\\theta}\\) and \\(\\pi\\) will approach either 0 or 1 depending on whether \\(\\mu_{\\theta}\\) is in the region of practical equivalence.\nFigure 3.1 depicts this simple generative model. The idea is that true effects in a given research area \\(\\theta_r\\) come from a normal distribution with mean \\(\\mu_\\theta\\) and standard deviation \\(\\tau\\). Within this research area, we have a certain proportion of true \\(\\pi\\) and false \\(1 - \\pi\\) effects. In the case of false effects, we can imagine a ROPE in which the effects are assumed to be equal to zero. Then the true effects \\(\\theta_r\\) can be the target of an original study \\(\\hat \\theta_0\\) and one or more replication studies \\(\\hat \\theta_r\\) (\\(r = (1, \\ldots, R)\\)). If the study is a precise replication, the heterogeneity is expected to be close to zero while in an extension experiment, we expect to see more heterogeneity. To simplify, the expected heterogeneity of a very precise replication is zero and all studies are essentially estimating the same original true effect \\(\\theta_0\\). In cases where the heterogeneity is different from zero (i.e., an extension) each replication study is associated with a different underlying true effect \\(\\theta_r\\). The degree of heterogeneity is related to the degree of precision, viz., to the number and relevance of changes to fixed factors. This model is similar to a random-effects meta-analysis (described in Chapter 4). The model can also be used to simulate replication studies as will be shown in Chapter 7.\n\n\n\n\n\n\nFigure 3.1: two level generating model\n\n\n\n3.1.4 Multi-level Model\nFigure 3.1 provides an example of a generating model for a specific research area in the literature, e.g., the distribution of true effects related to psychological treatments for depression. Some treatments in some conditions will have a higher/lower effect and in other conditions they will have a null effect. Extending the same reasoning to multiple effects within a broader literature, we can include another hierarchical layer in the model. Figure 3.2 depicts this extended model. For instance, we may be considering treatments for depression and anxiety. These research areas have specific average effects \\(\\mu_\\theta^1, \\mu_\\theta^2\\), heterogeneity \\(\\tau^1, \\tau^2\\) and proportions of true \\(\\pi^1, \\pi^2\\) and false \\(1 - \\pi^2, 1 - \\pi^2\\) effects. The combinatons of the two research areas define a broader literature, modeled as a mixture of these two distributions. Each component (i.e., anxiety and depression distributions) is associated with a weight \\(\\omega^1, \\omega^2\\), representing the importance (e.g., prevalence of studies) in the final mixture distribution. Finally, instead of considering \\(\\mu_\\theta\\) and \\(\\tau\\) as fixed, we can sample these values from two distributions generating a more realistic research area contanining a larger set of true effects. In terms of replication, we can apply the same logic of Figure 3.1 where the degree of precision determines the expected heterogeneity in the replication/extension study.\n\n\n\n\n\nFigure 3.2: Mixture generating model\n\n\n\n3.1.5 Simple Null Hypotheses\nMiller (2009) and many other works in the literature are concerned with simple null hypotheses. In our notation the null hypothesis is true when \\(|\\theta| = 0\\). The logic of section Section 3.1.3 can be adapted to this case, as illustrated in Figure Figure 3.3. Now the distribution of \\(\\theta\\) has two components: a continuous component describing the variation of the effect assuming the effect is not zero, and a discrete component at exactly zero. The probability of drawing a nonzero effect will again be \\(\\pi\\), which is also the area under the bell curve in the figure. Thus the point mass at zero is \\(1-\\pi\\). In terms of replication, we can apply the same logic of Figure 3.1 and Figure 3.2 where the degree of precision determines the expected heterogeneity in the replication/extension study.\n\n\n\n\n\nFigure 3.3: Point null generating model",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Probability of Replication</span>"
    ]
  },
  {
    "objectID": "chapters/chapter3.html#probability-of-replication-in-exact-replication-experiments",
    "href": "chapters/chapter3.html#probability-of-replication-in-exact-replication-experiments",
    "title": "3  Probability of Replication",
    "section": "\n3.2 Probability of Replication in Exact Replication Experiments",
    "text": "3.2 Probability of Replication in Exact Replication Experiments\n\n3.2.1 Two-Sample Design Review\nWe will often use a two-sample design to make the discussion more concrete. Assuming the homogeneity of variances among the two groups, the \\(t\\) statistic is calculated as follows:\n\\[\nt = \\frac{\\overline{y}_1 - \\overline{y}_2}{SE_{\\overline{y}_1 - \\overline{y}_2}}\n\\tag{3.1}\\]\nwhere subscripts denote the two groups. The standard error in the denominator is: \\[\n\\mbox{SE}_{\\overline{y}_1 - \\overline{y}_2} = s_p \\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}\n\\tag{3.2}\\]\nwhere \\(s_p\\) is the pooled standard deviation: \\[\ns_p = \\sqrt{\\frac{s^2_{y_1}(n_1 - 1) + s^2_{y_1}{(n_2 - 1)}}{n_1 + n_2 - 2}}\n\\tag{3.3}\\]\nIn the case of a simple null hypothesis, the p-value can be calculated from a \\(t\\) distribution with \\(\\nu = n_1 + n_2 - 2\\) degrees of freedom.\nMore generally, when the true effect size is not zero, the \\(t\\) statistic will follow a non-central Student’s \\(t\\) distribution with \\(\\nu\\) degrees of freedom and non-centrality parameter \\(\\lambda\\), calculated as follows: \\[\n\\lambda = \\delta \\sqrt{\\frac{n_1 n_2}{n_1 + n_2}}\n\\tag{3.4}\\]\nwhere \\(\\delta\\) is the true effect size — the true difference between means divided by the true group specific standard error.\nIf \\(F_{t_\\nu}\\) is the cumulative distribution function of the non-central Student’s \\(t\\) distribution, the statistical power is calculated as:\n\\[\n1 - \\beta = 1 - F_{t_\\nu} \\left(t_c, \\lambda \\right) + F_{t_\\nu} \\left(-t_{c}, \\lambda \\right)\n\\tag{3.5}\\]\nWith this setup, we can easily implement these equations in R, thus creating a flexible set of functions to reproduce and extend the examples from Miller (2009).\n\npower.t &lt;- function(d, n1, n2 = NULL, alpha = 0.05){\n    if(is.null(n2)) n2 &lt;- n1\n    df  &lt;- n1 + n2 - 2\n    ncp &lt;- d * sqrt((n1 * n2) / (n1 + n2))\n    tc  &lt;- abs(qt(alpha/2, df = df))\n    pt(-tc, df = df, ncp = ncp) + pt(tc, df = df, ncp = ncp, lower.tail = FALSE)\n}\n\n\npower.t(d = 0.5, n1 = 30)\n## [1] 0.4778965\npower.t(d = 0.5, n1 = 100)\n## [1] 0.9404272\npower.t(d = 0.1, n1 = 200)\n## [1] 0.1694809\n\nWhen the null hypothesis is a set (section Section 3.1.3) the p-value is calculated conditional on the least favorable case, which occurs when \\(\\theta = h_0\\) or \\(\\theta = - h_0\\), so the noncentral \\(t\\) is used to compute both the p-value and the power.\nAnother possibility is to use the full ROPE for the statistical test within the so-called equivalence testing framework (Lakens et al., 2018; see also Goeman et al., 2010 for the three-sided hypothesis testing).\n\n3.2.2 The Calculus of Replication Probabilities\nConsider the case of strict replication, so the null hypothesis has the same truth-value in both studies.\nTo prepare for Miller (2009), we look at replication probability by focusing on replicating significance. Given that the original study was significant (that is that \\(|t_0| &gt; t_{c_0}\\)), what is the probability that the second study will also be significant and the effect will be in the same direction as the original study? Formally we express significance in the original study as \\(|t_0| &gt; t_{c_0}\\), significance in the replication study as \\(|t_1| &gt; t_{c_1}\\), and concordance of the direction of the effect as \\(t_0 \\cdot t_1 &gt; 0\\).\n\\[\np_{rep} = \\Pr(|t_1| &gt; t_{c_1} \\cap t_0 \\cdot t_1 &gt;0 \\boldsymbol{\\mid} |t_0| &gt; t_{c_0})\n\\tag{3.6}\\]\nUsing the laws of conditional and total probability in turn, We can express this as\n\\[\n\\begin{split}\np_{rep} &= \\Pr(|t_1| \\geq t_{c_1} \\boldsymbol{\\mid} |t_0| \\geq t_{c_0}) \\\\[10pt]\n&= \\frac{\\Pr(|t_1| \\geq t_{c_1} \\cap t_0 \\cdot t_1 &gt;0 \\cap |t_0| \\geq t_{c_0})}{\\Pr(|t_0| \\geq t_{c_0})} \\\\[10pt]\n\\end{split}\n\\tag{3.7}\\]\nConsiderering separately the numerator and the denominator, and using the law of total probability, we have:\n\\[\n\\begin{split}\n\\mbox{numerator} =& \\Pr(H_1) \\cdot \\Pr(|t_1| \\geq t_{c1} \\cap t_0 \\cdot t_1 &gt;0 \\cap |t_0| \\geq t_{c_0} \\mid H_1) + \\\\\n& \\Pr(H_0) \\cdot \\Pr(|t_1| \\geq t_{c1} \\cap t_0 \\cdot t_1 &gt;0 \\cap |t_0| \\geq t_{c0} \\mid H_0) \\\\[10pt]\n\\mbox{denominator} =& \\Pr(H_1) \\cdot \\Pr(|t_0| \\geq t_{c_0} \\mid H_1) + \\\\\n& \\Pr(H_0) \\cdot \\Pr(|t_0| \\geq t_{c_1} \\mid H_0).\n\\end{split}\n\\tag{3.8}\\]\nThese expressions are a bit cumbersome, but they allow us to express the probability of replication as a useful function of power and significance level.\n\\[\n\\Pr (|t_0| \\geq t_{c_0} | H_0 ) = \\alpha_0, \\quad \\mbox{and} \\quad \\Pr (|t_0| \\geq t_{c_0} \\cap t_0 \\cdot t_1 &gt;0 | H_0 ) = \\alpha_1 / 2\n\\tag{3.9}\\]\nwhere \\(\\alpha_r\\) is the the study-specific significance level. The division by two is because half of the false positive rejections are expected to occur in the discordant direction under the null. Also\n\\[\n\\Pr(|t_0| \\geq t_{c_0} | H_1) = 1-\\beta_0 \\quad \\mbox{and} \\quad \\Pr(|t_1| \\geq t_{c_1} \\cap t_0 \\cdot t_1 &gt;0 | H_1) \\approx 1-\\beta_1\n\\tag{3.10}\\]\nwhere \\(1-\\beta_r\\) is the study specific power. Power is simple to compute when the alternative is also a single point, but it is more complex when, as in section Section 3.1.3, we consider a distribution of non-null effects. For now, think about \\(1-\\beta_r\\) as an average power. We will return on this point when we discuss hierarchical models.\nThe approximation sign \\(\\approx\\) refers to the fact that we are ignoring the probability that the replication study will generate a significant and discordant effect if the true effect is not null and is indeed in the direction identified in the original study. This probability will be small when the sample size is large or the effect size is large, but can be nontrivial in small studies with small effects.\nRewriting and using independence of the two studies:\n\\[\n\\begin{split}\np_{rep} & \\approx\n\\frac{\\Pr(H_1) \\cdot (1-\\beta_0)(1-\\beta_1) + (1- \\Pr(H_1)) \\cdot \\alpha_0 \\alpha_1 / 2}{\\Pr(H_1) \\cdot (1-\\beta_0) + (1-\\Pr(H_1)) \\cdot \\alpha_0} \\\\[10pt]\n\\end{split}\n\\tag{3.11}\\]\nThe key quantity that remains to be explained is \\(\\Pr(H_1)\\), the probability that the effect is not null. The interpretation of this probability depends on the context as well as the analysts’ approach. In section Section 3.2.3 we consider Miller’s version, where \\(\\Pr(H_1)\\) is essentially \\(\\pi\\) from Section Section 3.1.3. Later on, we will discuss an alternative Bayesian interpretation, where \\(\\Pr(H_1)\\) is based on expert knowledge existing prior to the original experiment.\nWe take two quick digressions before going back to Miller. First, if \\(R&gt;1\\) we can extend this reasoning to more general expressions. For example if we want to estimate the probability of having successful replications in \\(R\\) experiments we can consider \\[\n\\Pr \\left( \\bigcap_{r=1}^R |t_r| &gt; t_{c_r} \\boldsymbol{\\mid} |t_0| &gt; t_{c_0} \\right).\n\\tag{3.12}\\]\nSecond, if you are not familiar with the logic behind Bayes’s rule, we will walk you through it in the remainder of this section. The probability of the event A happening given (\\(|\\)) that B happened is:\n\\[\n\\Pr(A|B) = \\frac{\\Pr(A \\cap B)}{\\Pr(B)}\n\\tag{3.13}\\]\nWhere \\(\\Pr(A \\cap B)\\) is the joint probability that can be calculated as:\n\\[\n\\Pr(A \\cap B) = \\Pr(B|A) \\cdot \\Pr(A)\n\\tag{3.14}\\]\nThen the full equation also known as Bayes rule is:\n\\[\n\\Pr(A|B) = \\frac{\\Pr(B|A) \\cdot \\Pr(A)}{\\Pr(B)}\n\\tag{3.15}\\]\n\n3.2.3 Miller’s Aggregate Replication Probability (ARP)\n\nTo recap, Miller (2009) is about replication with the highest precision, because he considers no differences in populations between the experiments. He focuses on tests of hypotheses, and considers replication of experiments with significant testing results, as we did above. He also assumes that power and significance level are the same in the original and replication experiment. And, he evaluates power at a single point at a time, effectively assuming that the statistical problem is testing a simple null hypothesis against a simple alternative.\nWith regards to \\(\\Pr( H_1)\\) he considers a scheme similar to that of section Section 3.1.3. A study design is drawn randomly from the aggregate describing a scientific field of interest, and then replicated with high precision. In what proportion of cases does this exercise lead to a replication? For this calculation \\(\\Pr( H_1)\\) is the same as \\(\\pi\\) in Figure 3.1. Miller calls \\(\\pi\\) the strength of a research area. This parameter has also been considered by other authors when evaluating the probability of replication within a research field (Ioannidis, 2005; Wilson & Wixted, 2018).\nIn Miller’s setting, the only relevant parameters in the replication model are the type-2 error rate \\(\\beta\\) (or equivalently the statistical power \\(1 - \\beta\\)), the type-1 error rate \\(\\alpha\\) and the proportion of true hypotheses \\(\\pi\\) in a certain research field.\nUsing formula Equation 3.11, the probability of replicating an effect in a given research area (ARP), conditional on a significant original experiment, is \\[\np_{ARP} \\approx \\frac\n{\\pi \\cdot (1 - \\beta)^2 + (1 - \\pi) \\cdot \\alpha^2 / 2}\n{\\pi \\cdot (1 - \\beta)   + (1 - \\pi) \\cdot \\alpha}\n\\tag{3.16}\\]\n\nThe denominator is the probability of a significant result in the original study, broken down into the probability of a false positive \\((1 - \\pi) \\alpha\\) plus the probability of a true positive \\(\\pi (1 - \\beta)\\). Now, let’s consider the numerator. This is the probability that both the original and replication studies are significant and also concordant in the sign of their effects.\nA good way to understand the process is by formalizing inference using a contingency table as done in Table 3.1.\n\n\nTable 3.1: Inferential process formalized using a contingency table\n\n\n\n\n\n\n\n\nDecision on \\(H_0\\)\n\n\n\n\n\n\n\n\n\n\n\nFalse\n\n\nTrue\n\n\n\n\n\n\n\n\n\\(\\boldsymbol{H_0}\\)\n\n\nFalse\n\n\nTrue Positive (\\(1 - \\beta\\))\n\n\nFalse Negative (\\(\\beta\\))\n\n\n\\(\\pi\\)\n\n\n\n\n\\(\\boldsymbol{H_0}\\)\n\n\nTrue\n\n\nFalse Positive (\\(\\alpha\\))\n\n\nTrue Negative (\\(1 - \\alpha\\))\n\n\n\\(1 - \\pi\\)\n\n\n\n\n\n\n\nFor example, the probability that something is significant is the sum of true positives (i.e., the statistical power) and false positives (i.e., type-1 errors) weighted by the prevalence \\(\\pi\\). Thus the numerator \\(\\Pr(|t_1| &gt; t_{c1} \\cap |t_0| &gt; t_{c0})\\) can be written as the sum of the following two quantities:\n\nOriginal study being significant \\((1 - \\beta) \\cdot \\pi + \\alpha \\cdot (1 - \\pi)\\)\n\nReplication study being significant \\((1 - \\beta) + \\frac{\\alpha}{2}\\)\n\n\nWe have \\(\\frac{\\alpha}{2}\\) in the second expression because we assume that replication success happens when the effect is in the same direction, i.e., we are ignoring false positives with opposite sign.\nAn interesting use of Equation 3.11 and Equation 3.16 is that we can explore the probability of replication in different scenarios by manipulating the parameters defined above. Ulrich & Miller (2020) improved and extended Miller (2009), by estimating the impact of questionable research practices on the probability of replication. They found that the most important parameter negatively impacting replication rates is \\(\\pi\\), with questionable research practices having only a minor impact.\n\n3.2.4 R Code for the Aggregate replication probability (ARP)\nWe implemented the Miller (2009) equations in R (see also ?arp):\n\narp &lt;- function(R, pi, power, alpha) {\n\n    if(any(R &lt; 2)) stop(\"number of studies should be equal or greater than 2\")\n\n    if(length(power) != 1 & length(power) != R){\n      stop(\"length of power vector should be 1 or R\")\n    }\n\n    if(length(power) == 1) power &lt;- rep(power, R)\n\n    num &lt;- pi * prod(power) + (1 - pi) * alpha * (alpha/2)^(R - 1)\n    den &lt;- pi * power[1] + (1 - pi) * alpha\n    num/den\n}\n\nr is the number of replication studies, pi is \\(\\pi\\), power is \\(1 - \\beta\\) and alpha is \\(\\alpha\\).\nThe Figure 3.4 depicts how the ARP changes as we vary the theory strength \\(\\pi\\) and the power (assumed to be the same for the original and replication experiment).\n\n\n\n\n\n\n\nFigure 3.4\n\n\n\n\nThe main takeaways from Figure 3.4 are:\n\nfor medium (and probably plausible) levels of theory strength \\(\\pi\\), the replication probabilities are relatively low — even when the power is high\nwhen \\(R &gt; 1\\), the replication probabilities are very low — in all conditions\n\n3.2.5 Miller’s Individual replication probability (IRP)\n\nIn contrast to the ARP, which considers a randomly drawn original study from a field, the individual replication probability (IRP) focuses on a specific original study. Miller’s approach is ask what is the probability of a significant and concordant test result in the replication study, supposing that the estimated effect in the original study is correct. This reduces to calculating the power of the replication study, conditional upon the observed effect, that is: \\(p_{IRP} = 1 - \\beta(\\hat\\theta_0)\\)\nUsing the Equations from Section 3.2.1, we can reproduce and extend the Miller (2009) figures. Basically, we estimate the IRP assuming that the power (thus the effect size and sample size) of the replication study is the same as that of the original study. The key function is p2d, which returns the power, given the effect size, for the point estimate; and, the lower/upper limit of the effect size confidence interval2.\n\np2d &lt;- function(p, n, alpha = 0.05){\n    df &lt;- n*2 - 2\n    to &lt;- abs(qt(p/2, df))\n    se &lt;- sqrt(1/n + 1/n)\n    cit &lt;- effectsize:::.get_ncp_t(to, df)\n    lb.t &lt;- cit[1]\n    ub.t &lt;- cit[2]\n    d &lt;- to * se\n    lb &lt;- lb.t * se\n    ub &lt;- ub.t * se\n    # power\n    d.power &lt;- power.t(d = d, n1 = n, alpha = alpha)\n    lb.power &lt;- power.t(d = lb, n1 = n, alpha = alpha)\n    ub.power &lt;- power.t(d = ub, n1 = n, alpha = alpha)\n    list(t = to, d = d,\n         se = se, df = df,\n         ci.lb = lb, ci.ub = ub,\n         d.power = d.power,\n         lb.power = lb.power,\n         ub.power = ub.power,\n         p = p)\n}\n\n\n\n\n\n\n\n\nFigure 3.5: fig-miller-irp\n\n\n\n\nThe most important takeaway from Figure 3.5 is that the statistical significance of the original experiment alone \\(p_0 \\leq \\alpha\\) is not enough to reliably estimate the probability of replicating the effect. Only when the p-value is very small is the range of expected power relatively narrow. For significant but higher p-values there is a lot of uncertainty vis-á-vis the IRP. In this context, Perugini et al. (2014) have recommended against the use of the point estimate of an effect size to plan a new study (e.g., a replication). A more conservative approach would be to use another value (e.g., the lower bound) of the \\((1 - \\alpha) \\cdot 100\\) confidence interval. This conservative approach can mitigate the likely overestimation of the original study, thus providing a more realistic estimation of the probability of replication.\n\n3.2.6 Bayesian Individual Replication Probability\nMiller’s IRP has two main limitations. First, the effect size estimated in the original study is subject to sampling variability. This is not considered by Miller. In reality, the effect size could be larger or smaller. Second, in scientific areas where finding large effects is more challenging, the true effect size is more likely to be smaller, relative to those estimated for research areas in which large effects abound. So, considering a study in isolation is a limitation. Even at the individual level, we should expect a higher probability of replication for research areas where there are more true effects to be found. Some of these limitations can be mitigated by Bayesian replication probabilities. If you are not familiar with the logic behind Bayesian statistics, Lindley (1972) provides an excellent introduction.\nFor a simple example, consider specifying \\(\\Pr(H_1)\\) for the individual experiment at hand to reflect knowledge exisitng up to the point of the original experiment, but not including the original experiment itself. In the depression treatment example, this may have to to with the strength of the hypothesis, the plausibility of treatment examined, prior experiments in animals if the original experiment is in humans etc. Then one can apply the calculus of Section Section 3.2.2 (and specifically Equation 3.16) to evaluate the individual replication probability. Consideration of the sampling variability in the effect size may be incorporated in the specific way one calculates the average power. Consideration of the strength of the field of study can be embedded in the prior, either informally or through the use of hierarchical models, which we discuss below.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Probability of Replication</span>"
    ]
  },
  {
    "objectID": "chapters/chapter3.html#selection-and-regression-to-the-mean",
    "href": "chapters/chapter3.html#selection-and-regression-to-the-mean",
    "title": "3  Probability of Replication",
    "section": "\n3.3 Selection and Regression to the Mean",
    "text": "3.3 Selection and Regression to the Mean\nThe original effect size estimation could be biased (usually inflated) for several reasons (e.g., publication bias) affecting the actual probability of replication. For example, let’s assume that original studies are published only when they are significant. In addition, let’s assume we are considering a research area with medium strength and with average medium-small effect sizes (a very likely scenario). Finally, let’s assume we are working with sample sizes that are relatively low. Such a research area will tend to be low-powered; and, significant effects will either be inflated (in the best case) or false positives (in the worst case).\n\nWhen conducting replication studies of these kinds of original studies, we may find significant but substantially smaller estimated effects (or even non significant effects.) For example, the large scale project Collaboration & Open Science Collaboration (2015) reported that — regardless of significance level — replicated effects were substantially smaller than original effects. Regression to the mean is a good explanation of this phenomenon, since selection of extreme results will produce on average lower estimations (i.e., shrinkage) in later studies. In addition, if the replication study design (i.e., sample size) is based on the original effect size estimation, the inflation can greatly reduce the power. This is related to the concept of the type-M (M for magnitude) error (Gelman & Carlin, 2014), which quantifies the exent of inflation given the true effect and the sample size.\nIn Code 3.1, there is a small simulation showing how to estimate the type-M error for a two-sample design.\n\n\n\nCode 3.1: Type-M error simulation for a two-samples design\n\nd &lt;- 0.3\nn &lt;- 50\nnsim &lt;- 1e4\n\ndi &lt;- pi &lt;- rep(NA, nsim)\n\nfor(i in 1:nsim){\n    g0 &lt;- rnorm(n, 0, 1)\n    g1 &lt;- rnorm(n, d, 1)\n    tt &lt;- t.test(g1, g0)\n    di[i] &lt;- tt$statistic * tt$stderr\n    pi[i] &lt;- tt$p.value\n}\n\nsignificant &lt;- pi &lt;= 0.05\nmean(abs(di[significant])) / d\n\n\n\n\n[1] 1.738048\n\n\nThe type-M error increases as the sample size and/or the true effect size decreases. The Figure 3.6 shows the type-M error for different conditions.\n\nCodetypeM &lt;- function(d, n, B = 1e3){\n    di &lt;- pi &lt;- rep(NA, B)\n    for(i in 1:nsim){\n        g0 &lt;- rnorm(n, 0, 1)\n        g1 &lt;- rnorm(n, d, 1)\n        tt &lt;- t.test(g1, g0)\n        di[i] &lt;- tt$statistic * tt$stderr\n        pi[i] &lt;- tt$p.value\n    }\n    significant &lt;- pi &lt;= 0.05\n    mean(abs(di[significant])) / d\n}\n\nsim &lt;- expand.grid(\n    n = seq(10, 100, 10),\n    d = c(0.1, 0.3, 0.5, 1),\n    M = NA\n)\n\n\nfor(i in 1:nrow(sim)){\n    sim$M[i] &lt;- typeM(sim$d[i], sim$n[i])\n}\n\n\n\n\n\n\n\n\n\nFigure 3.6: fig-typem\n\n\n\n\nFigure 3.7 shows the impact of regression to the mean. We simulated two scenarios with a selection for extreme (i.e., significant) values, where the true average effect is zero. In the top panel, we included some heterogeneity — there is a distribution of true effects where the average size is zero. The selection occurs in a hypothetical original study with a sample size of 25. Then, we simulated two other studies with sample sizes 100 and 500, respectively, sampling from the same distribution and with the same true effect. The crucial lesson of Figure 3.7 is that selection of extreme studies will result in replication studies with lower power and smaller estimated effects. This is especially true if the replication study is based on the estimated effect of the original, low-powered study.\n\n\n\n\n\n\n\nFigure 3.7: Regression to the mean. Triangles are significant studies at \\(\\alpha = 0.05\\) and dots are the non-significant ones. The highlighted effect (in red) is a randomly selected effect that was significant with \\(n = 25\\) participants (original study \\(\\theta_0\\)). We simulated \\(k = 50\\) true effects \\(\\theta_k\\) (in blue) from a Gaussian distribution with mean \\(\\mu_{\\theta = 0}\\) and \\(\\tau^2 = 0.05\\). Then we simulated the first experiment as a comparison of two independent groups, with effect size \\(\\theta_k\\). Then we used the same \\(\\theta_k\\) to simulate another experiment with \\(n = 100\\) and \\(n = 500\\) participants. Clearly, as \\(n\\) increases, the \\(\\theta_k\\) estimation is more precise, with a shrinkage towards the average effect \\(\\mu_\\theta\\).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Probability of Replication</span>"
    ]
  },
  {
    "objectID": "chapters/chapter3.html#probability-of-replication-in-extension-experiments",
    "href": "chapters/chapter3.html#probability-of-replication-in-extension-experiments",
    "title": "3  Probability of Replication",
    "section": "\n3.4 Probability of Replication in Extension Experiments",
    "text": "3.4 Probability of Replication in Extension Experiments\n\n3.4.1 Hierarchical Models\nPawel & Held (2020) generalize the methodology of Miller (2009). Miller’s model neglected the possibility of heterogeneity among true effects, assuming that the power (and thus the effect size) is the same when \\(H_0\\) is false. A more sophisticated way to predict the probability of estimating a certain finding is to use the information of the original study and replication study (one-to-one replication) within a hierarchical bayesian model. Here are two main features of this more general modelling approach:\n\nincludes the uncertainty of both studies and eventually the between-study heterogeneity \\(\\tau\\). This allows the replication study to be truly different from the original study (e.g., it allows us to take the effects of publication bias into account).\nincludes a shrinkage factor, balancing the strength of the original study and the between-study heterogeneity, which allows us to account for regression to the mean and effect size inflation of the original study.\n\nWe adapted the Pawel & Held (2020) model, creating a more general version that can be easily adapted to simulating and analyzing data from replication studies as well as predicting the probability of replication given the model parameters. The generalized version of the model is presented in Figure 3.8. (The actual model will be implemented in Chapter 4.)\nIn the simple case of a two-group study, where \\(x_{ir}\\) denotes whether individual \\(i\\) is in group one, the distribution of the data in study \\(r\\) is as follows:\n\\[\ny_{ir}|x_{ir} \\sim \\mathcal{N}(\\mu_r + x_{ir}\\theta_r, \\sigma_r)\n\\]\nIn this case, \\(\\theta_r\\) can be intepreted as the mean difference between the two groups.\nAt the effect size level, for example, we may have:\n\\[\n\\theta_r \\sim \\mathcal{N}(\\mu_\\theta, \\tau)\n\\]\n\nFigure 3.8 depicts the hierarchical model. \\(\\mu_\\theta\\) is the average true effect and \\(\\tau\\) is the heterogeneity thus the standard deviation of the true effects. This is very similar to the generating model presented in Figure 3.1 and can be considered as a standard two-level model (or a random-effects meta-analysis). Both \\(\\mu_\\theta\\) and \\(\\tau\\) can be associated with a prior distribution. In this case \\(\\boldsymbol{\\zeta_\\theta}\\) and \\(\\boldsymbol{\\zeta_\\tau}\\) are vectors of parameters of the prior distributions. A common choice would be a normal distribution for \\(\\mu_\\theta\\) and an half-cauchy distribution for \\(\\tau\\) (Williams et al., 2018). At the study level, \\(\\theta_r\\) is the study-specific slope (i.e., the difference between the groups), \\(\\mu_t\\) is the intercept (i.e., expected value for the reference group). \\(y_{nr}\\) and \\(x_{nr}\\) are the response values and the predictors (respectively) for the individual \\(n\\) of the study \\(r\\).\n\n\n\n\n\n\n\n\nFigure 3.8: Hierarchical baysian model for estimating the replication outcome.\n\n\n\n\nThe model can be easily implemented in R using the probabilistic programming language STAN (Stan Development Team, 2021) or, under some conditions, using the analytical form as used in Pawel & Held (2020).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Probability of Replication</span>"
    ]
  },
  {
    "objectID": "chapters/chapter3.html#key-questions-1",
    "href": "chapters/chapter3.html#key-questions-1",
    "title": "3  Probability of Replication",
    "section": "\n3.5 Key Questions",
    "text": "3.5 Key Questions\n\nHow do questionable research practices and publication bias influence our expectations about replication?\nWhat is the difference between individual replication probability (IRP) and aggregate replication probability (ARP)?\nIs there a different scientific value in replications vs exensions?",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Probability of Replication</span>"
    ]
  },
  {
    "objectID": "chapters/chapter3.html#references",
    "href": "chapters/chapter3.html#references",
    "title": "3  Probability of Replication",
    "section": "References",
    "text": "References\n\n\n\n\nCollaboration, O. S., & Open Science Collaboration. (2015). Estimating the reproducibility of psychological science. In Science (Vol. 349). https://doi.org/10.1126/science.aac4716\n\n\nGelman, A., & Carlin, J. (2014). Beyond power calculations. In Perspectives on Psychological Science (Vol. 9, pp. 641–651). https://doi.org/10.1177/1745691614551642\n\n\nGoeman, J. J., Solari, A., & Stijnen, T. (2010). Three-sided hypothesis testing: Simultaneous testing of superiority, equivalence and inferiority. Statistics in Medicine, 29, 2117–2125. https://doi.org/10.1002/sim.4002\n\n\nHarrington, D., D’Agostino, R. B., Sr, Gatsonis, C., Hogan, J. W., Hunter, D. J., Normand, S.-L. T., Drazen, J. M., & Hamel, M. B. (2019). New guidelines for statistical reporting in the journal. The New England Journal of Medicine, 381, 285–286. https://doi.org/10.1056/NEJMe1906559\n\n\nHedges, L. V., & Schauer, J. M. (2019). Statistical analyses for studying replication: Meta-analytic perspectives. Psychological Methods, 24, 557–570. https://doi.org/10.1037/met0000189\n\n\nHedges, L. V., & Schauer, J. M. (2021). The design of replication studies. Journal of the Royal Statistical Society. Series A, (Statistics in Society), 184, 868–886. https://doi.org/10.1111/rssa.12688\n\n\nIoannidis, J. P. A. (2005). Why most published research findings are false. PLoS Medicine, 2, e124. https://doi.org/10.1371/journal.pmed.0020124\n\n\nJager, L. R., & Leek, J. T. (2014). An estimate of the science-wise false discovery rate and application to the top medical literature. Biostatistics, 15, 1–12. https://academic.oup.com/biostatistics/article-abstract/15/1/1/244509\n\n\nKruschke, J. K. (2015). Bayesian approaches to testing a point (“null”) hypothesis. In Doing bayesian data analysis (pp. 335–358). Elsevier. https://doi.org/10.1016/b978-0-12-405888-0.00012-x\n\n\nKruschke, J. K., & Liddell, T. M. (2018). The bayesian new statistics: Hypothesis testing, estimation, meta-analysis, and power analysis from a bayesian perspective. Psychonomic Bulletin & Review, 25, 178–206. https://doi.org/10.3758/s13423-016-1221-4\n\n\nLakens, D., Scheel, A. M., & Isager, P. M. (2018). Equivalence testing for psychological research: A tutorial. Advances in Methods and Practices in Psychological Science, 1, 259–269. https://doi.org/10.1177/2515245918770963\n\n\nLindley, D. V. (1972). Bayesian statistics a review. In Bayesian statistics (pp. 1–74). https://doi.org/10.1137/1.9781611970654.ch1\n\n\nMiller, J. (2009). What is the probability of replicating a statistically significant effect? Psychonomic Bulletin & Review, 16, 617–640. https://doi.org/10.3758/PBR.16.4.617\n\n\nPawel, S., & Held, L. (2020). Probabilistic forecasting of replication studies. PloS One, 15, e0231416. https://doi.org/10.1371/journal.pone.0231416\n\n\nPerugini, M., Gallucci, M., & Costantini, G. (2014). Safeguard power as a protection against imprecise power estimates. Perspectives on Psychological Science: A Journal of the Association for Psychological Science, 9, 319–332. https://doi.org/10.1177/1745691614528519\n\n\nSchauer, J. M., & Hedges, L. V. (2021). Reconsidering statistical methods for assessing replication. Psychological Methods, 26, 127–139. https://doi.org/10.1037/met0000302\n\n\nStan Development Team. (2021). Stan: A c++ library for probability and sampling, version 2.26.0. https://mc-stan.org/\n\n\nSteiger, J. H. (2004). Beyond the f test: Effect size confidence intervals and tests of close fit in the analysis of variance and contrast analysis. Psychological Methods, 9, 164–182. https://doi.org/10.1037/1082-989X.9.2.164\n\n\nUlrich, R., & Miller, J. (2020). Questionable research practices may have little effect on replicability. eLife, 9, e58237. https://doi.org/10.7554/eLife.58237\n\n\nWasserstein, R. L., & Lazar, N. A. (2016). The ASA statement on p-values: Context, process, and purpose. The American Statistician, 70, 129–133. https://doi.org/10.1080/00031305.2016.1154108\n\n\nWilliams, D. R., Rast, P., & Bürkner, P.-. C. (2018). Bayesian meta-analysis with weakly informative prior distributions. PsyArXiv. https://doi.org/10.31234/osf.io/7tbrm\n\n\nWilson, B. M., & Wixted, J. T. (2018). The prior odds of testing a true effect in cognitive and social psychology. Advances in Methods and Practices in Psychological Science, 1, 186–197. https://doi.org/10.1177/2515245918767122",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Probability of Replication</span>"
    ]
  },
  {
    "objectID": "chapters/chapter3.html#footnotes",
    "href": "chapters/chapter3.html#footnotes",
    "title": "3  Probability of Replication",
    "section": "",
    "text": "To note that the term ROPE refer to the Bayesian version of the equivalence region in the frequentist equivalence testing approach (Lakens et al., 2018). Despite using the Bayesian term, we are just referring to an area where the effect is neglegible and equivalent to a point-null value (e.g, zero).↩︎\nTo calculate the confidence interval of the effect size we used the so-called pivot method (Steiger, 2004) implemented into the effectsize:::.get_ncp_t() function↩︎",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Probability of Replication</span>"
    ]
  },
  {
    "objectID": "chapters/chapter4.html",
    "href": "chapters/chapter4.html",
    "title": "4  Statistical methods for replication assessment",
    "section": "",
    "text": "Suggested Readings",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Statistical methods for replication assessment</span>"
    ]
  },
  {
    "objectID": "chapters/chapter4.html#suggested-readings",
    "href": "chapters/chapter4.html#suggested-readings",
    "title": "4  Statistical methods for replication assessment",
    "section": "",
    "text": "Heyard et al. (2024)\nHedges & Schauer (2019c)",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Statistical methods for replication assessment</span>"
    ]
  },
  {
    "objectID": "chapters/chapter4.html#how-to-assess-the-replication-outcome",
    "href": "chapters/chapter4.html#how-to-assess-the-replication-outcome",
    "title": "4  Statistical methods for replication assessment",
    "section": "\n4.1 How to assess the replication outcome?",
    "text": "4.1 How to assess the replication outcome?\nIn earlier chapters, we introduced the concept of replication from both statistical and theoretical perspectives. We now focus on analyzing the outcomes of replication experiments. Just as there are many definitions of replication, there are also multiple ways of assessing the outcomes of replication studies.\nMuch of the discussion will focus on replication metrics, which is a burgeoning area of statistics. Heyard et al. (2024) published an extensive systematic review, supported by an online catalog. They also maintain an up-to-date database with more than 50 replication measures organized and classified according to a common scheme.\nThe database is available at the following link http://rachelheyard.com/reproducibility_metrics/. Here, we have selected methods from the database, according to the following criteria:\n\nwe have included methods not only for answering the question “Has a study been replicated?”, but also methods which instead combine evidence and estimate the size of the effect\nwe have selected methods that are not linked to a specific research area or topic. (Some methods are specific for a given area of research or type of experiment such as replicating fMRI studies at the voxel level.)\nwe have included both bayesian and frequentist approaches\nwhere methods are very similar, we have focused on the most recent or general version\n\nIn general, choosing a single “measure of replicability” is probably not the best approach Typically, large-scale replication projects (e.g., Collaboration & Open Science Collaboration, 2015; Errington et al., 2021; The Brazilian Reproducibility Initiative et al., 2025) evaluate results across different metrics.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Statistical methods for replication assessment</span>"
    ]
  },
  {
    "objectID": "chapters/chapter4.html#replication-studies-as-meta-analysis",
    "href": "chapters/chapter4.html#replication-studies-as-meta-analysis",
    "title": "4  Statistical methods for replication assessment",
    "section": "\n4.2 Replication studies as meta-analysis",
    "text": "4.2 Replication studies as meta-analysis\nHedges & Schauer (2019c) point out that replication studies can be seen — from a statistical point of view — as meta-analyses. Typically, there is a single initial study and one or more attempts to replicate its initial findings. For the simple one-to-one replication design we have a meta-analysis with two studies while in the one-to-many design we have a meta-analysis with \\(R\\) studies. When it comes to evaluating the results of replication studies, we can choose between several methods (see Heyard et al., 2024). Some of these methods will explictly involve meta-analyses, which pool evidence from original and replication studies. While we consider meta-analytic thinking to be crucial for understanding replication, not all methods are (strictly speaking) meta-analyses in the usual sense.\nSpecifically, the methodology of the initial and/or replication studies is important when it comes to evaluating a single result. However, at the aggregate level, we evaluate how a certain focal parameter (e.g., the difference between two groups or conditions) varies across replications with an eye toward determining whether there is evidence — whatever the criterion — for a successful replication. In this sense, whatever model happens to be used in the original study, we can (without loss of generality) fruitfully model the situation from the aggregative perspective.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Statistical methods for replication assessment</span>"
    ]
  },
  {
    "objectID": "chapters/chapter4.html#simulating-data",
    "href": "chapters/chapter4.html#simulating-data",
    "title": "4  Statistical methods for replication assessment",
    "section": "\n4.3 Simulating data",
    "text": "4.3 Simulating data\nWhile real-world examples are important, simulating simplified examples (“toy models”) is useful for understanding replication methods from a statistical point of view. Indeed, in general, simulating data is an important technique for teaching and understanding statistical methods (Hardin et al., 2015). Moreover, in practice, Monte Carlo simulations are necessary for estimating statistical properties (e.g., statistical power or type-1 error rate) of complex models.\nFor our simulated examples, we adpot the following methodological assumptions:\n\nprimary studies always compare two independent groups on a certain response variable\nwithin a study, the two groups are assumed to come from two normal distributions with unit variance. For one group \\(y_0\\) the mean is zero and for the other group \\(y_1\\) the mean is the value representing the effect size \\(\\theta_r\\) for that specific study.\nthe sample size can vary between the two groups\n\nFor example we can simulate a single study:\n\ntheta &lt;- 0.3\nn0 &lt;- 30\nn1 &lt;- 30\n\ny0 &lt;- rnorm(n0, 0, 1)\ny1 &lt;- rnorm(n1, theta, 1)\n\nmean(y1) - mean(y0) # this is the effect size\n## [1] 0.02489806\nvar(y1)/n1 + var(y0)/n0 # this is the sampling variability\n## [1] 0.09229475\n\n\ndd &lt;- data.frame(y = c(y0, y1),\n                 x = factor(rep(0:1, c(n0, n1))))\n\nggplot(dd, aes(x = x, y = y)) +\n    geom_boxplot(aes(fill = x))\n\n\n\n\n\n\n\nNow we can simply iterate the process for \\(R\\) studies to create a series of replications with a common true parameter \\(\\mu_{\\theta}\\). We can also generate a set of different sample sizes by sampling from a probability distribution (e.g., Poisson). Then, if we want to include variability in the true effects (as would be expected in the case of extensions of an experinent), we can sample \\(R\\) effects from, say, a normal distribution with variance \\(\\tau^2\\).\n\nR &lt;- 10\nmu &lt;- 0.3 # average true effect\ntau2 &lt;- 0.1 # heterogeneity\ndeltai &lt;- rnorm(R, mu, sqrt(tau2)) # true effects for R studies\n\nyi &lt;- vi &lt;- rep(NA, R)\nn0 &lt;- n1 &lt;- 10 + rpois(R, 40 - 10)\n\nfor(i in 1:R){\n    y0 &lt;- rnorm(n0[i], 0, 1)\n    y1 &lt;- rnorm(n1[i], deltai[i], 1)\n    yi[i] &lt;- mean(y1) - mean(y0)\n    vi[i] &lt;- var(y1)/n1[i] + var(y0)/n0[i]\n}\n\nsim &lt;- data.frame(id = 1:R, yi, vi, n0, n1)\nsim\n\n   id          yi         vi n0 n1\n1   1  0.85048016 0.04778103 36 36\n2   2  0.30921378 0.04848062 35 35\n3   3 -0.08156708 0.06168057 39 39\n4   4 -0.29994880 0.04077748 44 44\n5   5  0.32756262 0.05728339 33 33\n6   6  0.79298676 0.04796314 39 39\n7   7  0.67216941 0.04605116 34 34\n8   8 -0.37006253 0.04260985 40 40\n9   9  1.06199067 0.04762220 45 45\n10 10  0.82797567 0.06092187 31 31\n\n\nHere, we put everything into a single function, which can be used to simulate different scenarios.\nsim_study &lt;- function(theta, n0, n1 = NULL, id = NULL){\n    if(is.null(n1)) n1 &lt;- n0\n    y0 &lt;- rnorm(n0, 0, 1)\n    y1 &lt;- rnorm(n1, theta, 1)\n    sim &lt;- data.frame(\n        yi = mean(y1) - mean(y0),\n        vi = var(y1)/n1 + var(y0)/n0\n    )\n    sim$sei &lt;- sqrt(sim$vi)\n    sim$n0 &lt;- n0\n    sim$n1 &lt;- n1\n    if(!is.null(id)){\n        sim &lt;- cbind(id = id, sim)\n    }\n    class(sim) &lt;- c(\"rep3data\", class(sim))\n    return(sim)\n}\nsim_studies &lt;- function(R, mu = 0, tau2 = 0, n0, n1 = NULL){\n    # check input parameters consistency\n    if(length(mu) == 1) mu &lt;- rep(mu, R)\n    if(length(n0) == 1) n0 &lt;- rep(n0, R)\n    if(length(n1) == 1) n1 &lt;- rep(n1, R)\n    if(is.null(n1)) n1 &lt;- n0\n\n    deltai &lt;- rnorm(R, 0, sqrt(tau2))\n    mu &lt;- mu + deltai\n\n    sim &lt;- mapply(sim_study, mu, n0, n1, SIMPLIFY = FALSE)\n    sim &lt;- do.call(rbind, sim)\n    sim &lt;- cbind(id = 1:R, sim)\n    class(sim) &lt;- c(\"rep3data\", class(sim))\n    return(sim)\n}\n\nsim_studies(R = 10, mu = 0.5, tau2 = 0, n0 = 30)\n\n   id        yi         vi       sei n0 n1\n1   1 0.7637734 0.05646744 0.2376288 30 30\n2   2 0.4986094 0.06305786 0.2511132 30 30\n3   3 0.2429051 0.06739538 0.2596062 30 30\n4   4 0.4922899 0.07759289 0.2785550 30 30\n5   5 0.3290003 0.05849493 0.2418573 30 30\n6   6 0.6820730 0.06914787 0.2629598 30 30\n7   7 0.2040946 0.05499395 0.2345079 30 30\n8   8 0.5796687 0.05165193 0.2272706 30 30\n9   9 0.1460033 0.05915946 0.2432272 30 30\n10 10 0.6659032 0.08595197 0.2931757 30 30",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Statistical methods for replication assessment</span>"
    ]
  },
  {
    "objectID": "chapters/chapter4.html#methods",
    "href": "chapters/chapter4.html#methods",
    "title": "4  Statistical methods for replication assessment",
    "section": "\n4.4 Methods",
    "text": "4.4 Methods\nThe database available at the following link http://rachelheyard.com/reproducibility_metrics/ can be consulted for a more complete overview. Among all the available methods, we have selected the following metrics1, each of which will be illustrated below, with suitable examples.\n\nPrediction interval: is the replication effect inside the original 95% prediction interval?\nProportion of population effects agreeing in direction with the original (vote counting)\nConfidence interval (CI): original effect in replication 95% CI\nConfidence interval (CI): replication effect in original 95% CI\nPrediction interval (CI): replication effect in original 95% PI\nReplication Bayes Factor (BF)\nSmall Telescopes\nConsistency of original with replications, \\(P_{\\mbox{orig}}\\)\n\nProportion of population effects agreeing in direction with the original, \\(\\hat{P}_{&gt;0}\\)\n\nMeta-analysis and \\(Q\\) statistics",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Statistical methods for replication assessment</span>"
    ]
  },
  {
    "objectID": "chapters/chapter4.html#vote-counting-based-on-significance-or-direction",
    "href": "chapters/chapter4.html#vote-counting-based-on-significance-or-direction",
    "title": "4  Statistical methods for replication assessment",
    "section": "\n4.5 Vote Counting based on significance or direction",
    "text": "4.5 Vote Counting based on significance or direction\nThe simplest method is called vote counting (Hedges & Olkin, 1980; Valentine et al., 2011). A replication attempt \\(\\theta_{r}\\) is considered to be successful if the result is is statistically significant i.e., \\(p_{\\theta_{r}} \\leq \\alpha\\) and has the same direction as the original study \\(\\theta_{0}\\). Similarly, we can count the number of replications with the same sign as the original study. This method has the following pros and cons.\n\n\nEasy to understand, communicate and compute\n\n\n\n\nDoes not consider the size of the effect\nDepends on the power of \\(\\theta_{rep}\\)\n\n\n\nHere is an example which illustrates the vote counting method First, we simulate a replication:\n\n## original study\nn_orig &lt;- 30\ntheta_orig &lt;- theta_from_z(2, n_orig)\n\norig &lt;- data.frame(\n  yi = theta_orig,\n  vi = 4/(n_orig*2)\n)\n\norig$sei &lt;- sqrt(orig$vi)\norig &lt;- summary_es(orig)\n\norig\n\n         yi         vi       sei zi       pval      ci.lb    ci.ub\n1 0.5163978 0.06666667 0.2581989  2 0.04550026 0.01033725 1.022458\n\n## replications\n\nR &lt;- 10\nreps &lt;- sim_studies(R = R, \n                    mu = theta_orig, \n                    tau2 = 0, \n                    n_orig)\n\nreps &lt;- summary_es(reps)\n\nhead(reps)\n\n  id         yi         vi       sei n0 n1         zi         pval       ci.lb\n1  1  0.5417437 0.06415704 0.2532924 30 30  2.1388074 0.0324512712  0.04529967\n2  2  0.8204319 0.05502213 0.2345680 30 30  3.4976299 0.0004694121  0.36068715\n3  3 -0.1758015 0.08193435 0.2862418 30 30 -0.6141713 0.5391021068 -0.73682505\n4  4  0.6274413 0.05432717 0.2330819 30 30  2.6919351 0.0071038776  0.17060921\n5  5  0.4938945 0.06952231 0.2636708 30 30  1.8731478 0.0610479723 -0.02289088\n6  6  0.2908247 0.08020594 0.2832065 30 30  1.0268996 0.3044677335 -0.26424992\n      ci.ub\n1 1.0381876\n2 1.2801767\n3 0.3852221\n4 1.0842734\n5 1.0106798\n6 0.8458993\n\n\nThen, we compute the proportions of replication studies that are statistically significant:\n\nmean(reps$pval &lt;= 0.05)\n\n[1] 0.5\n\n\nAnd, the proportions of replication studies with the same sign as the original:\n\nmean(sign(orig$yi) == sign(reps$yi))\n\n[1] 0.9\n\n\nAt this stage, we could also perform some statistical tests in order to complement the vote counting approach. See Bushman & Wang (2009) and Hedges & Olkin (1980) for vote-counting methods in meta-analysis.\nHere is an extreme example:\nImagine an original experiment with \\(n_{0} = 30\\) and \\(\\hat \\theta_{0} = 0.5\\) that is statistically significant \\(p \\approx 0.045\\). Now, consider a direct replication study (thus assume \\(\\tau^2 = 0\\)) with \\(n_{1} = 350\\) which found \\(\\hat \\theta_{1} = 0.15\\), which is statistically significant at \\(p\\approx 0.047\\).\n\n\n\n\n\n\n\n\nThe most problematic aspect of using only the information about the sign of the effect (or the p value) is that we lose information about the size of the effect and the precision.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Statistical methods for replication assessment</span>"
    ]
  },
  {
    "objectID": "chapters/chapter4.html#confidenceprediction-interval-methods",
    "href": "chapters/chapter4.html#confidenceprediction-interval-methods",
    "title": "4  Statistical methods for replication assessment",
    "section": "\n4.6 Confidence/prediction interval methods",
    "text": "4.6 Confidence/prediction interval methods\nAn advantage of confidence/prediction interval methods is that they take into account the size of the effect and the precision. The confidence interval represents the sampling uncertainty around the estimated value. It is interpreted as the proportion of confidence intervals (under hypothetical repetition of the same sampling procedure) which contain the true value.\nThe confidence interval around an estimated effect \\(\\theta_r\\) can be written as follows:\n\\[\n95\\%\\;\\mbox{CI} = \\hat\\theta_r \\pm t_{\\alpha}\\sqrt{\\sigma^2_{r}}\n\\]\nHere, \\(t_{\\alpha}\\) is the critical test statistic for significance level \\(\\alpha\\). This version of the confidence interval is symmetric around the estimated value and its width is a function of the estimation precision.\n\nalpha &lt;- 0.05\nn &lt;- c(10, 50, 100, 500)\ntheta &lt;- rep(0.3, length(n))\nse &lt;- sqrt(1/n + 1/n)\ntc &lt;- abs(qnorm(alpha/2))\nlb &lt;- theta - se * tc\nub &lt;- theta + se * tc\n\nplot(n, theta, ylim = c(min(lb), max(ub)), type = \"b\", pch = 19,\n     xlab = \"Sample Size\",\n     ylab = latex2exp::TeX(\"$\\\\theta\\\\;_{r}$\"))\nlines(n, ub, lty = \"dashed\")\nlines(n, lb, lty = \"dashed\")\n\n\n\n\n\n\n\n\n4.6.1 Replication effect within the original CI\nWe can check whether a replication effect falls within the original study’s confidence interval. That is, whether the following inequalities are satisfied.\n\\[\n\\theta_{0} - t_{\\alpha} \\sqrt{\\sigma^2_{0}} &lt; \\theta_{r} &lt; \\theta_{0} + t_{\\alpha} \\sqrt{\\sigma^2_{0}}\n\\]\nThe pros and cons of this approach include:\n\n\nTakes into account the size of the effect and the precision of \\(\\theta_{0}\\)\n\n\n\n\n\nAssumes the original study provides a reliable estimate\nHas no extension for many-to-one designs\nOriginal studies with limited precision will tend to lead to higher success rates (i.e., original studies with wide intervals will be easier to replicate)\n\n\n\nz_orig &lt;- 2\nn_orig &lt;- 30\ntheta_orig &lt;- theta_from_z(z_orig, n_orig)\nn_rep &lt;- 350\ntheta_rep &lt;- 0.15\nse_orig &lt;- sqrt(4 / (2 * n_orig))\nci_orig &lt;- theta_orig + qnorm(c(0.025, 0.975)) * se_orig\ncurve(dnorm(x, theta_orig, se_orig), \n      -1, 2, \n      ylab = \"Density\", \n      xlab = latex2exp::TeX(\"$\\\\theta$\"))\nabline(v = ci_orig, lty = \"dashed\")\npoints(theta_orig, 0, pch = 19, cex = 2)\npoints(theta_rep, 0, pch = 19, cex = 2, col = \"firebrick\")\nlegend(\"topleft\", \n       legend = c(\"Original\", \"Replication\"), \n       fill = c(\"black\", \"firebrick\"))\n\n\n\n\n\n\n\nHere is an illustration of the last con of this method (i.e., that original studies with wide intervals will be easier to replicate).\n\n\n\n\n\n\n\n\n\n4.6.2 Original study within replication CI\nA similar approach is checking whether the original effect size is contained within the replication confidence interval, i.e., whether the following inequalities are satisfied.\n\\[\n\\theta_{r} - t_{\\alpha} \\sqrt{\\sigma_{r}^2} &lt; \\theta_{0} &lt; \\theta_{r} + t_{\\alpha} \\sqrt{\\sigma_{r}^2}\n\\]\nThis method has the same pros and cons as the previous approach. One advantage of this method is that replication studies are usually more precise (because they tend to have higher sample sizes). As a result, the parameter and the %CI tends to be more reliable.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Statistical methods for replication assessment</span>"
    ]
  },
  {
    "objectID": "chapters/chapter4.html#prediction-interval-pi",
    "href": "chapters/chapter4.html#prediction-interval-pi",
    "title": "4  Statistical methods for replication assessment",
    "section": "\n4.7 Prediction interval PI",
    "text": "4.7 Prediction interval PI\nThe CI methods described so far only take into account uncertainty from either the original or the replication study. The prediction interval PI allows us to account for uncertainty regarding a future observation (in this case, a future study).\nIf we compute the difference between the original and the replication study, the sampling distribution of the difference represents the range of values that are expected in the absence of other sources of variability. In other words, assuming that \\(\\tau^2 = 0\\).\nIf the original and replication studies come from the same population, the sampling distribution of the difference is centered on zero with a certain standard error \\(\\theta_{0} - \\theta_{r} \\sim \\mathcal{N}\\left(0, \\sqrt{\\sigma^2_{\\hat \\theta_{0} - \\hat \\theta_{r}}} \\right)\\) (the subscript \\(0\\) is to indicate that the effect is expected to be sampled from the same population as \\(\\theta_{0}\\))\n\\[\n\\hat \\theta_{0} \\pm t_{\\alpha} \\sqrt{\\sigma^2_{\\theta_{0} - \\theta_{r}}}\n\\]\nThe new standard error is the sum (assuming that the two studies are independent) of the two standard errors. Thus, we are taking into account the estimation uncertainty of the original and replication studies.\n\nset.seed(2025)\n\no1 &lt;- rnorm(50, 0.5, 1) # group 1\no2 &lt;- rnorm(50, 0, 1) # group 2\nod &lt;- mean(o1) - mean(o2) # effect size\nse_o &lt;- sqrt(var(o1)/50 + var(o2)/50) # standard error of the difference\n\nn_r &lt;- 100 # sample size replication\n\nse_o_r &lt;- sqrt(se_o^2 + (var(o1)/100 + var(o2)/100))\n\nod + qnorm(c(0.025, 0.975)) * se_o_r\n\n[1] 0.325520 1.298378\n\n\n\nCodepar(mar = c(4, 4, 0.1, 0.1))  \ncurve(dnorm(x, od, se_o_r), od - se_o_r*4, od + se_o_r*4, lwd = 2, xlab = latex2exp::TeX(\"$\\\\theta$\"), ylab = \"Density\")\nabline(v = od + qnorm(c(0.025, 0.975)) * se_o_r, lty = \"dashed\")\n\n\n\n\n\n\n\nThe pros and cons of this method include:\n\n\nTakes into account uncertainty in both studies\nAllows us to plan a replication using the standard deviation of the original study and the expected sample size of the replication study\n\n\n\n\nLow precision in original studies leads to wider PI. For a replication study, it is difficult to fall outside the PI\nMainly useful for one-to-one replication designs",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Statistical methods for replication assessment</span>"
    ]
  },
  {
    "objectID": "chapters/chapter4.html#mathur-vanderweele--mathur2020-nw-p_orig",
    "href": "chapters/chapter4.html#mathur-vanderweele--mathur2020-nw-p_orig",
    "title": "4  Statistical methods for replication assessment",
    "section": "\n4.8 Mathur & VanderWeele (2020) \\(p_{orig}\\)\n",
    "text": "4.8 Mathur & VanderWeele (2020) \\(p_{orig}\\)\n\nMathur & VanderWeele (2020) proposed a new method based on the prediction interval to calculate a p value \\(p_{orig}\\) representing the probability that \\(\\theta_{0}\\) is consistent with the replications. This method is suited for many-to-one replication designs. Formally:\n\\[\np_{orig} = 2 \\left[ 1 - \\Phi \\left( \\frac{|\\hat \\theta_{0} - \\hat \\mu_{\\theta_{R}}|}{\\sqrt{\\hat \\tau^2 + \\sigma^2_{\\theta_{0}} + \\hat{SE}^2_{\\hat \\mu_{\\theta_{R}}}}} \\right) \\right]\n\\]\n\n\n\\(\\hat \\mu_{\\theta_{R}}\\) is the pooled (i.e., meta-analytic) estimation of the \\(k\\) replications\n\n\\(\\tau^2\\) is the variance of effects across replications (i.e., heterogeneity)\n\n\\(p_{orig}\\) is interpreted as the probability that \\(\\theta_{0}\\) is at least as extreme as what is observed. A low \\(p_{orig}\\) suggests that the original study is inconsistent with replications. This method as three key pros.\n\n\nSuited for many-to-one designs\nTakes into account all sources of uncertainty\nProvides a p-value (to gauge consistency of the original study with replications)\n\n\nThe code for this approach is implemented in the Replicate and MetaUtility R packages:\n\ntau2 &lt;- 0.05\ntheta_rep &lt;- 0.2\ntheta_orig &lt;- 0.7\n\nn_orig &lt;- 30\nn_rep &lt;- 100\nk &lt;- 20\n\nreplications &lt;- sim_studies(k, theta_rep, tau2, n_rep, n_rep)\noriginal &lt;- sim_studies(1, theta_orig, 0, n_orig, n_orig)\n\nfit_rep &lt;- metafor::rma(yi, vi, data = replications) # random-effects meta-analysis\n\nReplicate::p_orig(original$yi, original$vi, fit_rep$b[[1]], fit_rep$tau2, fit_rep$se^2)\n\n[1] 0.06692786\n\n\n\n4.8.1 Simulation\n\n# standard errors assuming same n and variance 1\nse_orig &lt;- sqrt(4/(n_orig * 2))\nse_rep &lt;- sqrt(4/(n_rep * 2))\nse_theta_rep &lt;- sqrt(1/((1/(se_rep^2 + tau2)) * k)) # standard error of the random-effects estimate\n\nsep &lt;- sqrt(tau2 + se_orig^2 + se_theta_rep^2) # z of p-orig denominator\n\ncurve(dnorm(x, theta_rep, sep), theta_rep - 4*sep, theta_rep + 4*sep, ylab = \"Density\", xlab = latex2exp::TeX(\"\\\\theta\"))\npoints(theta_orig, 0.02, pch = 19, cex = 2)\nabline(v = qnorm(c(0.025, 0.975), theta_rep, sep), lty = \"dashed\", col = \"firebrick\")\n\n\n\n\n\n\n\n\n4.8.2 Mathur & VanderWeele (2020) \\(\\hat P_{&gt; 0}\\)\n\nAnother related metric is the \\(\\hat P_{&gt; 0}\\), which represents the proportion of replications having the same direction as the original effect. Note: before computing the proportions, they adjust the estimated \\(\\theta_{r}\\) (\\(r = 1, \\dots, R\\)) with a shrinkage factor (to account for regression to the mean):\n\\[\n\\tilde{\\theta}_{r} = (\\theta_{r} - \\mu_{\\theta_{r}}) \\sqrt{\\frac{\\hat \\tau^2}{\\hat \\tau^2 + \\sigma^2_{r}}}\n\\]\nHere \\(\\sigma^2_r\\) is the sampling variance for the replication effect size \\(r\\). This method is similar to vote counting; but, we are adjusting the effects with a shrinkage factor based on \\(\\tau^2\\).\n\n# compute calibrated estimation for the replications\n# use restricted maximum likelihood to estimate tau2 under the hood\ntheta_sh &lt;- MetaUtility::calib_ests(replications$yi, replications$sei, method = \"REML\")\nmean(theta_sh &gt; 0)\n\n[1] 0.85\n\n\nThe authors suggest a bootstrapping approach for inference on \\(\\hat P_{&gt; 0}\\)\n\nnboot &lt;- 1e3\ntheta_boot &lt;- matrix(0, nrow = nboot, ncol = k)\n\nfor(i in 1:nboot){\n  idx &lt;- sample(1:nrow(replications), nrow(replications), replace = TRUE)\n  replications_boot &lt;- replications[idx, ]\n  theta_cal &lt;- MetaUtility::calib_ests(replications_boot$yi, \n                                       replications_boot$sei, \n                                       method = \"REML\")\n  theta_boot[i, ] &lt;- theta_cal\n}\n\n# calculate\np_greater_boot &lt;- apply(theta_boot, 1, function(x) mean(x &gt; 0))\n\n\n\n\n\n\n\n\n\n\n4.8.3 Mathur & VanderWeele (2020) \\(\\hat P_{\\gtrless q*}\\)\n\nInstead of using 0 as threshold, we can use a practically meaningful effect size, considered to be low but different from 0. \\(\\hat P_{\\gtrless q*}\\) is the proportion of (calibrated) replications greater or lower than the \\(q*\\) value. This framework is similar to equivalence and minimum effect size testing (Lakens et al., 2018).\n\nq &lt;- 0.2 # minimum non zero effect\n\nfit &lt;- metafor::rma(yi, vi, data = replications)\n\n# see ?MetaUtility::prop_stronger\nMetaUtility::prop_stronger(q = q,\n                           M = fit$b[[1]],\n                           t2 = fit$tau2,\n                           tail = \"above\",\n                           estimate.method = \"calibrated\",\n                           ci.method = \"calibrated\",\n                           dat = replications,\n                           yi.name = \"yi\",\n                           vi.name = \"vi\")\n\n   est        se lo  hi   bt.mn shapiro.pval\n1 0.35 0.1756669  0 0.6 0.35375    0.8137044",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Statistical methods for replication assessment</span>"
    ]
  },
  {
    "objectID": "chapters/chapter4.html#meta-analytic-methods",
    "href": "chapters/chapter4.html#meta-analytic-methods",
    "title": "4  Statistical methods for replication assessment",
    "section": "\n4.9 Meta-analytic methods",
    "text": "4.9 Meta-analytic methods\nAnother approach is to combine the original and replication results using a meta-analysis model. This is applicable to both one-to-one and many-to-one designs. In this case, we can test whether the pooled estimate is different from 0 (or another practically relevant value). The meta-analytic approach has the following pros and cons.\n\n\nUses all the available information, especially when fitting a random-effects model\nTakes into account the precision of individual studies (e.g., via inverse-variance weighting)\n\n\n\n\nDoes not take into account publication bias\nFor one-to-one designs, only a fixed-effects model can be used\n\n\n\n4.9.1 Fixed-effects Model\n\n# fixed-effects\nfit_fixed &lt;- rma(yi, vi, method = \"FE\")\nsummary(fit_fixed)\n\n\nFixed-Effects Model (k = 20)\n\n  logLik  deviance       AIC       BIC      AICc   \n 20.7415   -0.0000  -39.4829  -38.4872  -39.2607   \n\nI^2 (total heterogeneity / total variability):   0.00%\nH^2 (total variability / sampling variability):  0.00\n\nTest for Heterogeneity:\nQ(df = 19) = 0.0000, p-val = 1.0000\n\nModel Results:\n\nestimate      se    zval    pval   ci.lb   ci.ub      \n  0.2000  0.0316  6.3246  &lt;.0001  0.1380  0.2620  *** \n\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n4.9.2 Random-Effects model\n\n# fixed-effects\nfit_random &lt;- rma(yi, vi, method = \"REML\")\nsummary(fit_random)\n\n\nRandom-Effects Model (k = 20; tau^2 estimator: REML)\n\n  logLik  deviance       AIC       BIC      AICc   \n 19.7044  -39.4088  -35.4088  -33.5199  -34.6588   \n\ntau^2 (estimated amount of total heterogeneity): 0 (SE = 0.0065)\ntau (square root of estimated tau^2 value):      0\nI^2 (total heterogeneity / total variability):   0.00%\nH^2 (total variability / sampling variability):  1.00\n\nTest for Heterogeneity:\nQ(df = 19) = 0.0000, p-val = 1.0000\n\nModel Results:\n\nestimate      se    zval    pval   ci.lb   ci.ub      \n  0.2000  0.0316  6.3246  &lt;.0001  0.1380  0.2620  *** \n\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n4.9.3 Pooling replications\nThe previous approach can also incorporate the combination/pooling of replications into a single effect (then comparing the original study and the combined/pooled replication study).\nThis is similar to using the CI or PI approaches; but, the replication effect is likely to be more precise, since we are pooling multiple replication studies.\n\n4.9.4 The Q Statistic\nAn interesting proposal involves the use of the Q statistic (Hedges & Schauer, 2019a, 2019b, 2019c, 2021; Schauer, 2022; Schauer & Hedges, 2020; Schauer & Hedges, 2021), which is commonly applied in meta-analysis to assess the presence of heterogeneity. Formally:\n\\[\nQ = \\sum_{r = 0}^{R} \\frac{(\\theta_r - \\bar \\theta_R)^2}{\\sigma^2_r}\n\\]\nwhere \\(\\bar \\theta_R\\) is the inverse-variance weighted average (e.g., from a fixed-effect model). The \\(Q\\) statistic is a weighted sum of squares. Under the null hypothesis where all studies are equal \\(\\theta_0 = \\theta_1, ... = \\theta_R\\), the \\(Q\\) statistic has a \\(\\chi^2\\) distribution with \\(R - 1\\) degrees of freedom. Under the alternative hypothesis the distribution is a non-central \\(\\chi^2\\) with non centrality parameter \\(\\lambda\\). The expected value of \\(Q\\) is \\(E(Q) = \\nu + \\lambda\\), where \\(\\nu\\) are the degrees of freedom.\nHedges & Schauer proposed the use of \\(Q\\) to evaluate the consistency of a series of replications:\n\nIn case of a replication, \\(\\lambda = 0\\) because \\(\\theta_0 = \\theta_1, ... = \\theta_R\\).\nIn case of an extension, \\(\\lambda &lt; \\lambda_0\\) where \\(\\lambda_0\\) is the maximum value considered equivalent to zero. In other words, \\(\\lambda_0\\) is the threshold above which the variability is too high to be considered negligible for a replication (in the sense of Machery, 2020).\n\nThis approach is testing the consistency (i.e., homogeneity) of replications. A successful replication should minimize the heterogeneity and the presence of a significant \\(Q\\) statistic would suggest a failed replication attempt. Hedges & Schauer (2019a) and Mathur & VanderWeele (2019) discuss this approach in detail. Their discussion addresses various issues surrounding the proper use of this method, including:\n\nvariations of the apporoach, which cover different replication setups (burden of proof on replicating vs non-replicating, many-to-one and one-to-one, etc.)\nthe choice (and interpretation) of the \\(\\lambda_0\\) parameter\nevaluating the power and statistical properties under different replication scenarios\n\nWhen evaluating a replication, we can use the Qrep() function which calculates the p-value based on the Q sampling distribution.\nQrep &lt;- function(yi, vi, lambda0 = 0, alpha = 0.05){\n  fit &lt;- metafor::rma(yi, vi)\n  k &lt;- fit$k\n  Q &lt;- fit$QE\n  df &lt;- k - 1\n  Qp &lt;- pchisq(Q, df = df, ncp = lambda0, lower.tail = FALSE)\n  pval &lt;- ifelse(Qp &lt; 0.001, \"p &lt; 0.001\", sprintf(\"p = %.3f\", Qp))\n  lambda &lt;- ifelse((Q - df) &lt; 0, 0, (Q - df))\n  res &lt;- list(Q = Q, lambda = lambda, pval = Qp, df = df, k = k, alpha = alpha, lambda0 = lambda0)\n  H0 &lt;- ifelse(lambda0 != 0, paste(\"H0: lambda &lt;\", lambda0), \"H0: lambda = 0\")\n  title &lt;- ifelse(lambda0 != 0, \"Q test for Approximate Replication\", \"Q test for Exact Replication\")\n  cli::cli_rule()\n  cat(cli::col_blue(cli::style_bold(title)), \"\\n\\n\")\n  cat(sprintf(\"Q = %.3f (df = %s), lambda = %.3f, %s\", res$Q, res$df, lambda, pval), \"\\n\")\n  cat(H0, \"\\n\")\n  cli::cli_rule()\n  class(res) &lt;- \"Qrep\"\n  invisible(res)\n}\n\n\nQ test for Exact Replication \n\nQ = 509.539 (df = 99), lambda = 410.539, p &lt; 0.001 \nH0: lambda = 0 \n\n\n\nQres &lt;- Qrep(dat$yi, dat$vi)\n\nQ test for Exact Replication \n\nQ = 509.539 (df = 99), lambda = 410.539, p &lt; 0.001 \nH0: lambda = 0 \n\n\n\nplot.Qrep(Qres)\n\n\n\n\n\n\n\n\n4.9.4.1 Q Statistic for an extension study\nIn the case of an extension of an intial experiment, we will need to set \\(\\lambda_0\\) to a meaningful value. The critical \\(Q\\) is no longer evaluated with a central \\(\\chi^2\\) but is now a non-central \\(\\chi^2\\) with non-centrality parameter \\(\\lambda_0\\).\nHedges & Schauer (2019c) provide different strategies for choosing \\(\\lambda_0\\). They found that (under some mild assumptions), \\(\\lambda = (R - 1) \\frac{\\tau^2}{\\tilde{v}}\\) seems to be a suitable choice.\nSimilarly to a meta-analysis, in a many-to-one replication design we have mainly two sources of variability: the true heterogeneity \\(\\tau^2\\) and the within-studies veriability \\(\\sigma_i^2\\). The sum of these two quantities is the total variability and the proportion of total variability due to heterogeneity is a relative index of (in)consistency among effects. This proportion is called \\(I^2\\) (Higgins & Thompson, 2002) in the meta-analysis literature is calculated as: \\[\nI^2 = 100\\% \\times \\frac{\\hat{\\tau}^2}{\\hat{\\tau}^2 + \\tilde{v}}\n\\]\nWhere \\(\\tilde{v}\\) can be considered as a typical within-study variance value.\n\\[\n\\tilde{v} = \\frac{(k-1) \\sum w_i}{(\\sum w_i)^2 - \\sum w_i^2},\n\\]\nHaving introduced the \\(I^2\\) statistic, we can derive a \\(\\lambda_0\\) based on \\(I^2\\). Schmidt & Hunter (2014) proposed that when \\(\\tilde{v}\\) is at least 75% of the total variance \\(\\tilde{v} + \\tau^2\\), \\(\\tau^2\\) can be considered to be negligible. This corresponds to a \\(I^2 = 25%\\) and a ratio \\(\\frac{\\tau^2}{\\tilde{v}} = 1/3\\). Hence, \\(\\lambda_0 = \\frac{(R - 1)}{3}\\) can be considered to be a negligible amount of heterogeneity.\n\nk &lt;- 100\ndat &lt;- sim_studies(k, 0.5, 0, 50, 50)\nQrep(dat$yi, dat$vi, lambda0 = (k - 1)/3)\n\nQ test for Approximate Replication \n\nQ = 93.881 (df = 99), lambda = 0.000, p = 0.989 \nH0: lambda &lt; 33",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Statistical methods for replication assessment</span>"
    ]
  },
  {
    "objectID": "chapters/chapter4.html#small-telescopes-simonsohn2015-kg",
    "href": "chapters/chapter4.html#small-telescopes-simonsohn2015-kg",
    "title": "4  Statistical methods for replication assessment",
    "section": "\n4.10 Small Telescopes (Simonsohn, 2015)\n",
    "text": "4.10 Small Telescopes (Simonsohn, 2015)\n\nThe idea is simple but quite powerful and insightful. An original study \\(\\theta_{0}\\) is conducted estimating a certain effect size with a certain sample size. This study has a certain statistical power level assuming a plausible effect size. Then we estimate the effect size that would have a certain low power level (e.g., 33%, \\(d_{33\\%}\\)). Then we run the replication study \\(\\theta_r\\) testing if the replication effect size is lower (one-sided test) than \\(d_{33\\%}\\). If upper bound of the confidence interval of the replication study is lower this means that the original study is probably seriously underpowered suggesting evidence of non-replication.\nAs a practical example, let’s assume that an original study found an effect of \\(\\theta_{0} = 0.7\\) on a two-sample design with \\(n = 20\\) per group. We define a threshold as the effect size that is associated with a certain low statistical power level e.g., \\(33\\%\\) given the sample size i.e. \\(d_{33\\%} = 0.5\\). The replication study found an effect of \\(\\theta_{r} = 0.2\\) with \\(n = 100\\) subjects.\nIf the \\(\\theta_{r}\\) is statistically significantly lower than \\(d_{33\\%} = 0.5\\) we conclude that the effect is probably so small that could not have been detected by the original study.\nWe can use the custom small_telescope() function on simulated data:\nsmall_telescope &lt;- function(or_d,\n                            or_se,\n                            rep_d,\n                            rep_se,\n                            small,\n                            ci = 0.95){\n  # quantile for the ci\n  qs &lt;- c((1 - ci)/2, 1 - (1 - ci)/2)\n\n  # original confidence interval\n  or_ci &lt;- or_d + qnorm(qs) * or_se\n\n  # replication confidence interval\n  rep_ci &lt;- rep_d + qnorm(qs) * rep_se\n\n  # small power\n  is_replicated &lt;- rep_ci[2] &gt; small\n\n  msg_original &lt;- sprintf(\"Original Study: d = %.3f %s CI = [%.3f, %.3f]\",\n                          or_d, ci, or_ci[1], or_ci[2])\n\n  msg_replicated &lt;- sprintf(\"Replication Study: d = %.3f %s CI = [%.3f, %.3f]\",\n                            rep_d, ci, rep_ci[1], rep_ci[2])\n\n\n  if(is_replicated){\n    msg_res &lt;- sprintf(\"The replicated effect is not smaller than the small effect (%.3f), (probably) replication!\", small)\n    msg_res &lt;- cli::col_green(msg_res)\n  }else{\n    msg_res &lt;- sprintf(\"The replicated effect is smaller than the small effect (%.3f), no replication!\", small)\n    msg_res &lt;- cli::col_red(msg_res)\n  }\n\n  out &lt;- data.frame(id = c(\"original\", \"replication\"),\n                    d = c(or_d, rep_d),\n                    lower = c(or_ci[1], rep_ci[1]),\n                    upper = c(or_ci[2], rep_ci[2]),\n                    small = small\n  )\n\n  # nice message\n  cat(\n    msg_original,\n    msg_replicated,\n    cli::rule(),\n    msg_res,\n    sep = \"\\n\"\n  )\n\n  invisible(out)\n\n}\n\nset.seed(2025)\n\nd &lt;- 0.2 # real effect\n\n# original study\nor_n &lt;- 20\nor_d &lt;- 0.7\nor_se &lt;- sqrt(1/20 + 1/20)\nd_small &lt;- pwr::pwr.t.test(or_n, power = 0.33)$d\n\n# replication\nrep_n &lt;- 100 # sample size of replication study\ng0 &lt;- rnorm(rep_n, 0, 1)\ng1 &lt;- rnorm(rep_n, d, 1)\n\nrep_d &lt;- mean(g1) - mean(g0)\nrep_se &lt;- sqrt(var(g1)/rep_n + var(g0)/rep_n)\n\nHere we are using the pwr::pwr.t.test() to compute the effect size \\(\\theta_{small}\\) (in code d) associated with 33% power.\n\nsmall_telescope(or_d, or_se, rep_d, rep_se, d_small, ci = 0.95)\n\nOriginal Study: d = 0.700 0.95 CI = [0.080, 1.320]\nReplication Study: d = 0.214 0.95 CI = [-0.061, 0.490]\n────────────────────────────────────────────────────────────────────────────────\nThe replicated effect is smaller than the small effect (0.493), no replication!\n\n\nAnd a plot:",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Statistical methods for replication assessment</span>"
    ]
  },
  {
    "objectID": "chapters/chapter4.html#replication-bayes-factor",
    "href": "chapters/chapter4.html#replication-bayes-factor",
    "title": "4  Statistical methods for replication assessment",
    "section": "\n4.11 Replication Bayes factor",
    "text": "4.11 Replication Bayes factor\nVerhagen & Wagenmakers (2014) proposed a method to estimate the evidence of a replication study. The core topics to understand the method are:\n\nBayesian hypothesis testing using the Bayes Factor (see, Rouder et al., 2009)\n\nBayes Factor using the Savage-Dickey density ratio (SDR, Wagenmakers et al., 2010)\n\n\n\n4.11.1 Bayesian inference\nBayesian inference is a statistical procedure where prior beliefs about a phenomenon are combined, using Bayes’s theorem, with evidence from data to obtain posterior beliefs.\nIf the researcher can express their prior beliefs in probabilistic terms, then evidence from the experiment is taken into account (via Bayesian conditionalization), thus increasing or decreasing the plausibility of prior beliefs.\nLet’s consider a simple example. We need to evaluate the fairness of a coin. The parameter is the probability \\(\\pi\\) of success (i.e., heads). We have our prior belief about the coin (e.g., fair but with some uncertainty). We toss the coin \\(k\\) times, and we observe \\(x\\) heads. How should we update our prior beliefs? We apply Bayes’s Theorem, which, in this case, is:\n\\[\np(\\pi|D) = \\frac{p(D|\\pi) \\; p(\\pi)}{p(D)}\n\\]\nHere, \\(\\pi\\) is our parameter and \\(D\\) is our data. \\(p(\\pi|D)\\) is the posterior distribution, which is the normalized product of \\(p(D|\\pi)\\) — which is called the likelihood of the hypothesis \\(\\pi\\), relative to the data \\(D\\) — and the prior \\(p(\\pi)\\). \\(p(D)\\) is the prior probability of the data (aka marginal likelihood) and is necessary for the posterior to be a (proper) probability distribution.\nWe can “read” the formula as: The probability of the parameter given the data is the product of the probability of the data given the parameter and the prior probability of the parameter.\nLet’s express our prior belief in probabilistic terms:\n\n\n\n\n\n\n\n\nNow we collect data and we observe \\(x = 40\\) tails out of \\(k = 50\\) trials thus \\(\\hat{\\pi} = 0.8\\) and compute the likelihood:\n\n\n\n\n\n\n\n\nFinally we combine, using the Bayes rule, prior and likelihood to obtain the posterior distribution:\n\n\n\n\n\n\n\n\nThe Bayes Factor (BF) — also called the likelihood ratio, for obvious reasons — is a measure of the relative support that the evidence provides for two competing hypotheses, \\(H_0\\) and \\(H_1\\) (~ \\(\\pi\\) in our previous example). It plays a key role in the following odds form of Bayes’s theorem.\n\\[\n\\frac{p(H_0|D)}{p(H_1|D)} = \\frac{p(D|H_0)}{p(D|H_1)} \\times \\frac{p(H_0)}{p(H_1)}\n\\]\nThe ratio of the priors \\(\\frac{p(H_0)}{p(H_1)}\\) is called the prior odds of the hypotheses; and, the ratio of the poosteriors \\(\\frac{p(H_0| D)}{p(H_1 | D)}\\) is called the posterior odds of the hypotheses. Thus, the above (odds form) of Bayes’s Theorem can be paraphrased as follows\n\\[\n\\text{posterior odds} = \\text{Bayes Factor} \\times \\text{prior odds}\n\\] In this way, it can be seen that the Bayes Factor captures the relative empirical support that the evidence \\(D\\) provides in favor of \\(H_0\\) over \\(H_1\\). If the Bayes Factor is greater than 1, then \\(D\\) favors \\(H_0\\) over \\(H_1\\). If the BF is less than one, then \\(D\\) favors \\(H_1\\) over \\(H_0\\). If the BF is 1, then \\(D\\) is neutral regarding \\(H_0\\) vs \\(H_1\\) (Royall, 1997). For this reason, the BF is also sometimes called the weight of evidence in favor of \\(H_0\\) (vs \\(H_1\\)) (Good, 1950).\n\n4.11.2 Calculating the Bayes Factor using the SDR\nThe Savage-Dickey density ratio (SDR) is a convenient shortcut to calculate the Bayes Factor (Marin & Robert, 2010; see Wagenmakers et al., 2010) in a situation where the null hypothesis (\\(H_0\\)) is a single parameter value (e.g., \\(H_0: \\theta = 0\\)). With SDR the Bayes Factor can be calculated as the ratio between the prior and posterior density distribution under the alternative hypothesis \\(H_1\\).\n\\[\nBF_{01} = \\frac{p(D|H_0)}{p(D|H_1)} \\approx \\frac{p(\\pi = x|D, H_1)}{p(\\pi = x | H_1)}\n\\]\nWhere \\(\\pi\\) is the parameter of interest and \\(x\\) is the null value under \\(H_0\\) (e.g., 0). and \\(D\\) are the data.\nWhen there are multiple parameters in play and the null hypothesis is expressed in terms of only one of them, this representation is valid only under the assumption that the prior for any of the other parameters under the null is the same as the conditional prior for those parameters under the alternative.\nFollowing the previous example \\(H_0: \\pi = 0.5\\). Under \\(H_1\\) we use a vague prior by setting \\(\\pi \\sim Beta(1, 1)\\).\nSay we flipped the coin 20 times and we found that \\(\\hat \\pi = 0.75\\).\n\n\n\n\n\n\n\n\nThe ratio between the two black dots is the Bayes Factor.\n\n\n\n\n\n\n\n\n\n4.11.3 Verhagen & Wagenmakers (2014) model2\n\nThe idea is using the posterior distribution of the original study as prior for a Bayesian hypothesis testing where:\n\n\n\\(H_0: \\theta_{r} = 0\\) meaning that there is no effect in the replication study\n\n\\(H_1: \\theta_{r} \\neq 0\\) and in particular is distributed as \\(\\delta \\sim \\mathcal{N}(\\theta_{0}, \\sigma^2_{0})\\) where \\(\\theta_{0}\\) and \\(\\sigma^2_{0}\\) are the mean and standard error of the original study\n\nIf \\(H_0\\) is more likely after seeing the data, there is evidence against the replication (i.e., \\(BF_{r0} &gt; 1\\)) otherwise there is evidence for a successful replication (\\(BF_{r1} &gt; 1\\)).\n\n\n\n\n\n\nWarning\n\n\n\nDisclaimer: The actual implementation of Verhagen & Wagenmakers (2014) is different (they use the \\(t\\) statistics). We implemented a similar model using a Bayesian linear model with rstanarm.\n\n\nLet’s assume that the original study (\\(n = 30\\)) estimates a \\(\\theta_{0} = 0.4\\) and a standard error \\(\\sigma^2/n\\).\n\n# original study\nn &lt;- 30\nyorig &lt;- 0.4\nse &lt;- sqrt(1/n)\n\n\n\n\n\n\n\nNote\n\n\n\nThe assumption of Verhagen & Wagenmakers (2014) is that the original study performed a Bayesian analysis with a flat (unform) prior. Thus, the confidence interval is approximately the same as the Bayesian credible interval.\n\n\nFor this reason, the posterior distribution of the original study can be approximated as:\n\n\n\n\n\n\n\n\nLet’s imagine that a new study tried to replicate the original one. They collected \\(n = 100\\) participants with the same protocol and found an effect of \\(y_{rep} = 0.1\\).\n\nnrep &lt;- 100\nyrep &lt;- MASS::mvrnorm(nrep, mu = 0.1, Sigma = 1, empirical = TRUE)[, 1]\ndat &lt;- data.frame(y = yrep)\nhist(yrep, main = \"Replication Study (n1 = 100)\", xlab = latex2exp::TeX(\"$\\\\theta_{r}$\"))\nabline(v = mean(yrep), lwd = 2, col = \"firebrick\")\n\n\n\n\n\n\n\nWe can analyze these data with an intercept-only regression model setting as prior the posterior distribution of the original study:\n\n# setting the prior on the intercept parameter\nprior &lt;- rstanarm::normal(location = yorig,\n                          scale = se)\n\n# fitting the bayesian linear regression\nfit &lt;- stan_glm(y ~ 1, \n                data = dat, \n                prior_intercept = prior,\n                refresh = FALSE)\n\nsummary(fit)\n\n\nModel Info:\n function:     stan_glm\n family:       gaussian [identity]\n formula:      y ~ 1\n algorithm:    sampling\n sample:       4000 (posterior sample size)\n priors:       see help('prior_summary')\n observations: 100\n predictors:   1\n\nEstimates:\n              mean   sd   10%   50%   90%\n(Intercept) 0.2    0.1  0.1   0.2   0.3  \nsigma       1.0    0.1  0.9   1.0   1.1  \n\nFit Diagnostics:\n           mean   sd   10%   50%   90%\nmean_PPD 0.2    0.1  0.0   0.2   0.3  \n\nThe mean_ppd is the sample average posterior predictive distribution of the outcome variable (for details see help('summary.stanreg')).\n\nMCMC diagnostics\n              mcse Rhat n_eff\n(Intercept)   0.0  1.0  2609 \nsigma         0.0  1.0  2608 \nmean_PPD      0.0  1.0  3185 \nlog-posterior 0.0  1.0  1693 \n\nFor each parameter, mcse is Monte Carlo standard error, n_eff is a crude measure of effective sample size, and Rhat is the potential scale reduction factor on split chains (at convergence Rhat=1).\n\n\nWe can use the bayestestR::bayesfactor_pointnull() to calculate the BF using the Savage-Dickey density ratio.\n\nbf &lt;- bayestestR::bayesfactor_pointnull(fit, null = 0)\nprint(bf)\n\nBayes Factor (Savage-Dickey density ratio)\n\nParameter   |    BF\n-------------------\n(Intercept) | 0.274\n\n* Evidence Against The Null: 0\n\nplot(bf)\n\n\n\n\n\n\n\nYou can also use the bf_replication() function:\nbf_replication &lt;- function(mu_original,\n                           se_original,\n                           replication){\n\n  # prior based on the original study\n  prior &lt;- rstanarm::normal(location = mu_original, scale = se_original)\n\n  # to dataframe\n  replication &lt;- data.frame(y = replication)\n\n  fit &lt;- rstanarm::stan_glm(y ~ 1,\n                            data = replication,\n                            prior_intercept = prior,\n                            refresh = 0) # avoid printing\n\n  bf &lt;- bayestestR::bayesfactor_pointnull(fit, null = 0, verbose = FALSE)\n\n  title &lt;- \"Bayes Factor Replication Rate\"\n  posterior &lt;- \"Posterior Distribution ~ Mean: %.3f, SE: %.3f\"\n  replication &lt;- \"Evidence for replication: %3f (log %.3f)\"\n  non_replication &lt;- \"Evidence for non replication: %3f (log %.3f)\"\n\n  if(bf$log_BF &gt; 0){\n    replication &lt;- cli::col_green(sprintf(replication, exp(bf$log_BF), bf$log_BF))\n    non_replication &lt;- sprintf(non_replication, 1/exp(bf$log_BF), -bf$log_BF)\n  }else{\n    replication &lt;- sprintf(replication, exp(bf$log_BF), bf$log_BF)\n    non_replication &lt;- cli::col_red(sprintf(non_replication, 1/exp(bf$log_BF), -bf$log_BF))\n  }\n\n  outlist &lt;- list(\n    fit = fit,\n    bf = bf\n  )\n\n  cat(\n    cli::col_blue(title),\n    cli::rule(),\n    sprintf(posterior, fit$coefficients, fit$ses),\n    \"\\n\",\n    replication,\n    non_replication,\n    sep = \"\\n\"\n  )\n\n  invisible(outlist)\n\n}\n\nbf_replication(mu_original = yorig, se_original = se, replication = yrep)\n\nA better custom plot:\n\nbfplot &lt;- data.frame(\n  prior = rnorm(1e5, yorig, se),\n  posterior = rnorm(1e5, fit$coefficients, fit$ses)\n)\n \nggplot() +\n  stat_function(geom = \"line\", \n                aes(color = \"Original Study (Prior)\"),\n                linewidth = 1,\n                alpha = 0.3,\n                fun = dnorm, args = list(mean = yorig, sd = se)) +\n  stat_function(geom = \"line\",\n                linewidth = 1,\n                aes(color = \"Replication Study (Posterior)\"),\n                fun = dnorm, args = list(mean = fit$coefficients, sd = fit$ses)) +\n  xlim(c(-0.5, 1.2)) +\n  geom_point(aes(x = c(0, 0), y = c(dnorm(0, yorig, sd = se),\n                                    dnorm(0, fit$coefficients, sd = fit$ses))),\n             size = 3) +\n  xlab(latex2exp::TeX(\"\\\\delta\")) +\n  ylab(\"Density\") +\n  theme(legend.position = \"bottom\",\n        legend.title = element_blank())",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Statistical methods for replication assessment</span>"
    ]
  },
  {
    "objectID": "chapters/chapter4.html#conclusions",
    "href": "chapters/chapter4.html#conclusions",
    "title": "4  Statistical methods for replication assessment",
    "section": "\n4.12 Conclusions",
    "text": "4.12 Conclusions\nWe hope to have provided a useful overview of statistical approaches to different definitions of replication. The meta-analytic and \\(Q\\) methods are more focused on accumulating evidence combining original and replication studies. The CI and PI methods are more interested in comparing the original and replication(s) study. The BF method constitutes a kind of compromise between these perspectives, by evaluating the weight of evidence in favor of the null — from the point of view of the posterior distribution of the replication study.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Statistical methods for replication assessment</span>"
    ]
  },
  {
    "objectID": "chapters/chapter4.html#key-questions",
    "href": "chapters/chapter4.html#key-questions",
    "title": "4  Statistical methods for replication assessment",
    "section": "\n4.13 Key Questions",
    "text": "4.13 Key Questions\n\nHow might combining evidence from original and replication studies provide a richer understanding than comparing them separately?\nWhy might different definitions of replication lead to different conclusions about the success of a replication attempt?",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Statistical methods for replication assessment</span>"
    ]
  },
  {
    "objectID": "chapters/chapter4.html#references",
    "href": "chapters/chapter4.html#references",
    "title": "4  Statistical methods for replication assessment",
    "section": "References",
    "text": "References\n\n\n\n\nBushman, B. J., & Wang, M. C. (2009). Vote-counting procedures in meta-analysis. In The handbook of research synthesis and meta-analysis (pp. 207–220). Russell Sage Foundation.\n\n\nCollaboration, O. S., & Open Science Collaboration. (2015). Estimating the reproducibility of psychological science. In Science (Vol. 349). https://doi.org/10.1126/science.aac4716\n\n\nErrington, T. M., Mathur, M., Soderberg, C. K., Denis, A., Perfito, N., Iorns, E., & Nosek, B. A. (2021). Investigating the replicability of preclinical cancer biology. eLife, 10. https://doi.org/10.7554/eLife.71601\n\n\nGood, I. J. (1950). Probability and the weighing of evidence. Charles Griffin & Co., Ltd., London; Hafner Publishing Co., New York.\n\n\nHardin, J., Hoerl, R., Horton, N. J., Nolan, D., Baumer, B., Hall-Holt, O., Murrell, P., Peng, R., Roback, P., Temple Lang, D., & Ward, M. D. (2015). Data science in statistics curricula: Preparing students to “think with data.” The American Statistician, 69, 343–353. https://doi.org/10.1080/00031305.2015.1077729\n\n\nHedges, L. V., & Olkin, I. (1980). Vote-counting methods in research synthesis. Psychological Bulletin, 88, 359–369. https://doi.org/10.1037/0033-2909.88.2.359\n\n\nHedges, L. V., & Schauer, J. M. (2019a). Consistency of effects is important in replication: Rejoinder to mathur and VanderWeele (2019). Psychological Methods, 24, 576–577. https://doi.org/10.1037/met0000237\n\n\nHedges, L. V., & Schauer, J. M. (2019b). More than one replication study is needed for unambiguous tests of replication. Journal of Educational and Behavioral Statistics: A Quarterly Publication Sponsored by the American Educational Research Association and the American Statistical Association, 44, 543–570. https://doi.org/10.3102/1076998619852953\n\n\nHedges, L. V., & Schauer, J. M. (2019c). Statistical analyses for studying replication: Meta-analytic perspectives. Psychological Methods, 24, 557–570. https://doi.org/10.1037/met0000189\n\n\nHedges, L. V., & Schauer, J. M. (2021). The design of replication studies. Journal of the Royal Statistical Society. Series A, (Statistics in Society), 184, 868–886. https://doi.org/10.1111/rssa.12688\n\n\nHeyard, R., Pawel, S., Frese, J., Voelkl, B., Würbel, H., McCann, S., Held, L., Wever, K. E., Hartmann, H., Townsin, L., & Zellers, S. (2024). A scoping review on metrics to quantify reproducibility: A multitude of questions leads to a multitude of metrics. https://scholar.google.com/citations?view_op=view_citation&hl=en&citation_for_view=JAb7P1QAAAAJ:ldfaerwXgEUC\n\n\nHiggins, J. P. T., & Thompson, S. G. (2002). Quantifying heterogeneity in a meta-analysis. Statistics in Medicine, 21, 1539–1558. https://doi.org/10.1002/sim.1186\n\n\nLakens, D., Scheel, A. M., & Isager, P. M. (2018). Equivalence testing for psychological research: A tutorial. Advances in Methods and Practices in Psychological Science, 1, 259–269. https://doi.org/10.1177/2515245918770963\n\n\nLy, A., Etz, A., Marsman, M., & Wagenmakers, E.-J. (2019). Replication bayes factors from evidence updating. Behavior Research Methods, 51, 2498–2508. https://doi.org/10.3758/s13428-018-1092-x\n\n\nMachery, E. (2020). What is a replication? Philosophy of Science, 87, 545–567. https://doi.org/10.1086/709701\n\n\nMarin, J.-M., & Robert, C. P. (2010). On resolving the savage–dickey paradox. Electronic Journal of Statistics, 4, 643–654. https://doi.org/10.1214/10-EJS564\n\n\nMathur, M. B., & VanderWeele, T. J. (2019). Challenges and suggestions for defining replication \"success\" when effects may be heterogeneous: Comment on hedges and schauer (2019). Psychological Methods, 24, 571–575. https://doi.org/10.1037/met0000223\n\n\nMathur, M. B., & VanderWeele, T. J. (2020). New statistical metrics for multisite replication projects. Journal of the Royal Statistical Society. Series A, (Statistics in Society), 183, 1145–1166. https://doi.org/10.1111/rssa.12572\n\n\nRouder, J. N., Speckman, P. L., Sun, D., Morey, R. D., & Iverson, G. (2009). Bayesian t tests for accepting and rejecting the null hypothesis. Psychonomic Bulletin & Review, 16, 225–237. https://doi.org/10.3758/PBR.16.2.225\n\n\nRoyall, R. M. (1997). Statistical evidence: A likelihood paradigm (Vol. 71). Chapman & Hall, London.\n\n\nSchauer, J. M. (2022). Replicability and meta-analysis. In W. O’Donohue, A. Masuda, & S. Lilienfeld (Eds.), Avoiding questionable research practices in applied psychology (pp. 301–342). Springer International Publishing. https://doi.org/10.1007/978-3-031-04968-2_14\n\n\nSchauer, J. M., & Hedges, L. V. (2020). Assessing heterogeneity and power in replications of psychological experiments. Psychological Bulletin, 146, 701–719. https://doi.org/10.1037/bul0000232\n\n\nSchauer, J. M., & Hedges, L. V. (2021). Reconsidering statistical methods for assessing replication. Psychological Methods, 26, 127–139. https://doi.org/10.1037/met0000302\n\n\nSchmidt, F. L., & Hunter, J. E. (2014). Methods of meta-analysis: Correcting error and bias in research findings (3rd ed.). SAGE Publications. https://doi.org/10.4135/9781483398105\n\n\nSimonsohn, U. (2015). Small telescopes: Detectability and the evaluation of replication results: Detectability and the evaluation of replication results. Psychological Science, 26, 559–569. https://doi.org/10.1177/0956797614567341\n\n\nThe Brazilian Reproducibility Initiative, Amaral, O. B., Carneiro, C. F. D., Neves, K., Sampaio, A. P. W., Gomes, B. V., Abreu, M. B. de, Tan, P. B., Mota, G. P. S., Goulart, R. N., Fernandes, N. R. de S., Linhares, J. H., Ibelli, A. M. G., Sebollela, A., Andricopulo, A. D., Carvalho, A. A. V. de, Silva, A. P. e., Souza, A. S. O., Lima, A. C. C. de, … Altei, W. F. (2025). Estimating the replicability of brazilian biomedical science. bioRxiv, 2025.04.02.645026. https://doi.org/10.1101/2025.04.02.645026\n\n\nValentine, J. C., Biglan, A., Boruch, R. F., Castro, F. G., Collins, L. M., Flay, B. R., Kellam, S., Mościcki, E. K., & Schinke, S. P. (2011). Replication in prevention science. Prevention Science: The Official Journal of the Society for Prevention Research, 12, 103–117. https://doi.org/10.1007/s11121-011-0217-6\n\n\nVerhagen, J., & Wagenmakers, E.-J. (2014). Bayesian tests to quantify the result of a replication attempt. Journal of Experimental Psychology. General, 143, 1457–1475. https://doi.org/10.1037/a0036731\n\n\nWagenmakers, E.-J., Lodewyckx, T., Kuriyal, H., & Grasman, R. (2010). Bayesian hypothesis testing for psychologists: A tutorial on the savage–dickey method. Cognitive Psychology, 60, 158–189. https://doi.org/10.1016/j.cogpsych.2009.12.001",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Statistical methods for replication assessment</span>"
    ]
  },
  {
    "objectID": "chapters/chapter4.html#footnotes",
    "href": "chapters/chapter4.html#footnotes",
    "title": "4  Statistical methods for replication assessment",
    "section": "",
    "text": "The name of the method corresponds to the first column of the online table↩︎\nsee also Ly et al. (2019) for an improvement↩︎",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Statistical methods for replication assessment</span>"
    ]
  },
  {
    "objectID": "chapters/chapter5.html",
    "href": "chapters/chapter5.html",
    "title": "5  Meta-analysis and publication bias",
    "section": "",
    "text": "We are working on it! Stay tuned!",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Meta-analysis and publication bias</span>"
    ]
  },
  {
    "objectID": "chapters/chapter6.html",
    "href": "chapters/chapter6.html",
    "title": "6  Improving study robustness",
    "section": "",
    "text": "We are working on it! Stay tuned!",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Improving study robustness</span>"
    ]
  },
  {
    "objectID": "chapters/chapter7.html",
    "href": "chapters/chapter7.html",
    "title": "7  Planning a replication study",
    "section": "",
    "text": "We are working on it! Stay tuned!",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Planning a replication study</span>"
    ]
  }
]